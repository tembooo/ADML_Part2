{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## R-LSTM Text Classification (Final, No Optuna)\n",
        "\n",
        "Trains a Residual LSTM (R-LSTM) text classifier on df_file.csv with five classes. It preprocesses the data, builds a frequency vocabulary, encodes texts as fixed-length sequences, trains with strong regularization and early stopping, evaluates on a held-out test set, and saves plots and a PyTorch checkpoint.\n",
        "\n",
        "Data and labels\n",
        "\n",
        "File: df_file.csv\n",
        "\n",
        "Columns: Text (string), Label (int)\n",
        "\n",
        "Example mapping (update if needed): 0: Politics, 1: Sport, 2: Technology, 3: Entertainment, 4: Business\n",
        "\n",
        "Pipeline\n",
        "\n",
        "Lowercase text, drop empty rows.\n",
        "\n",
        "Build vocabulary of size 5000 with <PAD>=0 and <UNK>=1.\n",
        "\n",
        "Convert each document to a sequence of token IDs of length 50 (truncate/right-pad).\n",
        "\n",
        "Stratified split: 80% train, 10% validation, 10% test.\n",
        "\n",
        "Train R-LSTM with Adam, label smoothing, ReduceLROnPlateau, gradient clipping, and early stopping.\n",
        "\n",
        "Evaluate, print classification report, and save artifacts.\n",
        "\n",
        "Model: Residual LSTM\n",
        "\n",
        "Stacked LSTM layers. Each layer adds a linear projection of its input to the LSTM output (residual).\n",
        "\n",
        "Optional bidirectionality per layer.\n",
        "\n",
        "Uses the last time step as the sequence representation, followed by a linear classifier.\n",
        "\n",
        "Training configuration (from best trial)\n",
        "\n",
        "embedding_dim=128, hidden_size=128, num_layers=2, bidirectional=True\n",
        "\n",
        "dropout≈0.336, batch_size=24, learning_rate≈3.11e-3, weight_decay≈9.93e-4\n",
        "\n",
        "label_smoothing≈0.14, max_epochs=150, patience=20\n",
        "\n",
        "Optimizer: Adam. Scheduler: ReduceLROnPlateau (factor 0.5). Gradient clipping: 5.0.\n",
        "\n",
        "Reproducibility: global seeds, cuDNN deterministic where available.\n",
        "\n",
        "Outputs saved\n",
        "\n",
        "rlstm_training_curves.svg (loss and accuracy vs. epochs)\n",
        "\n",
        "rlstm_confusion_matrix.svg (test confusion matrix)\n",
        "\n",
        "rlstm_per_class_accuracy.svg (per-class test accuracy)\n",
        "\n",
        "rlstm_lr_schedule.svg (learning rate per epoch)\n",
        "\n",
        "rlstm_pytorch_model.pt (model state dict, vocab, metadata)\n",
        "\n",
        "How to run\n",
        "\n",
        "Place df_file.csv next to the notebook.\n",
        "\n",
        "Install: pip install torch pandas scikit-learn matplotlib seaborn\n",
        "\n",
        "Run cells in order. The script auto-detects CUDA, MPS, or CPU.\n",
        "\n",
        "Tips and troubleshooting\n",
        "\n",
        "On Windows, if DataLoader workers fail, set num_workers=0.\n",
        "\n",
        "For GPU OOM, reduce batch_size or hidden_size.\n",
        "\n",
        "Ensure labels are consecutive integers starting at 0 and match class_names.\n",
        "\n",
        "If seaborn is missing, install it or adapt the confusion matrix to pure matplotlib."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRzryL6UTqtL",
        "outputId": "f0548f72-a191-42a3-9aca-b349a47a278b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA GPU: Tesla T4\n",
            "Device: cuda\n",
            "\n",
            "\n",
            "======================================================================\n",
            "LOADING DATASET\n",
            "======================================================================\n",
            "Dataset shape: (2225, 2)\n",
            "Columns: ['Text', 'Label']\n",
            "Number of classes: 5\n",
            "\n",
            "Label distribution:\n",
            "Label\n",
            "0    417\n",
            "1    511\n",
            "2    401\n",
            "3    386\n",
            "4    510\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class mapping:\n",
            "  0: Politics (417 samples)\n",
            "  1: Sport (511 samples)\n",
            "  2: Technology (401 samples)\n",
            "  3: Entertainment (386 samples)\n",
            "  4: Business (510 samples)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "PREPROCESSING\n",
            "======================================================================\n",
            "[Step] Lowercasing and dropping empty rows ...\n",
            "[Done] Dataset shape after preprocessing: (2225, 2)\n",
            "\n",
            "======================================================================\n",
            "VOCABULARY\n",
            "======================================================================\n",
            "[Step] Counting token frequencies ...\n",
            "[Info] Total unique tokens: 60616\n",
            "[Step] Building vocab size=5000 with <PAD>=0, <UNK>=1 ...\n",
            "[Done] Vocab size: 5000 | Coverage: 8.25%\n",
            "\n",
            "======================================================================\n",
            "SEQUENCE ENCODING\n",
            "======================================================================\n",
            "[Info] Sequence length: 50\n",
            "[Step] Converting texts to index sequences ...\n",
            "[Done] X_sequences shape: (2225, 50)\n",
            "[Done] y_labels shape:  (2225,)\n",
            "\n",
            "======================================================================\n",
            "DATA SPLIT\n",
            "======================================================================\n",
            "[Info] Training set:   1780 samples (80.0%)\n",
            "[Info] Validation set: 222 samples (10.0%)\n",
            "[Info] Test set:       223 samples (10.0%)\n",
            "\n",
            "======================================================================\n",
            "FINAL TRAINING WITH BEST HYPERPARAMETERS\n",
            "======================================================================\n",
            "[Final Config]\n",
            "  Vocab size:       5000\n",
            "  Embedding dim:    128\n",
            "  Hidden size:      128\n",
            "  Num layers:       2\n",
            "  Bidirectional:    True\n",
            "  Dropout:          0.3363570033\n",
            "  Output size:      5\n",
            "  Batch size:       24\n",
            "  Learning rate:    0.00311292936\n",
            "  Weight decay:     0.00099272647\n",
            "  Label smoothing:  0.1405728207\n",
            "  Max epochs:       150\n",
            "  Sequence length:  50\n",
            "\n",
            "Model architecture:\n",
            "ResLSTM(\n",
            "  (embedding): Embedding(5000, 128, padding_idx=0)\n",
            "  (lstm_layers): ModuleList(\n",
            "    (0): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
            "    (1): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (proj_layers): ModuleList(\n",
            "    (0): Linear(in_features=128, out_features=256, bias=False)\n",
            "    (1): Linear(in_features=256, out_features=256, bias=False)\n",
            "  )\n",
            "  (drop_layers): ModuleList(\n",
            "    (0-1): 2 x Dropout(p=0.3363570033, inplace=False)\n",
            "  )\n",
            "  (final_dropout): Dropout(p=0.3363570033, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 1,399,045\n",
            "Trainable parameters: 1,399,045\n",
            "[Final] [Train] epochs=150, batch_size=24, lr=0.003113, patience=20\n",
            "[Final] Epoch   1/150 | Train Loss: 1.7223 | Train Acc: 0.2264 | Val Loss: 1.6341 | Val Acc: 0.2027 | LR: 0.003113\n",
            "[Final] Epoch   2/150 | Train Loss: 1.5900 | Train Acc: 0.2944 | Val Loss: 1.6208 | Val Acc: 0.2793 | LR: 0.003113\n",
            "[Final] Epoch   3/150 | Train Loss: 1.5182 | Train Acc: 0.3517 | Val Loss: 1.5870 | Val Acc: 0.3694 | LR: 0.003113\n",
            "[Final] Epoch   4/150 | Train Loss: 1.3859 | Train Acc: 0.4815 | Val Loss: 1.4552 | Val Acc: 0.4414 | LR: 0.003113\n",
            "[Final] Epoch   5/150 | Train Loss: 1.2725 | Train Acc: 0.5376 | Val Loss: 1.4727 | Val Acc: 0.3874 | LR: 0.003113\n",
            "[Final] Epoch   6/150 | Train Loss: 1.1484 | Train Acc: 0.6275 | Val Loss: 1.4321 | Val Acc: 0.5045 | LR: 0.003113\n",
            "[Final] Epoch   7/150 | Train Loss: 1.0717 | Train Acc: 0.6826 | Val Loss: 1.2875 | Val Acc: 0.5631 | LR: 0.003113\n",
            "[Final] Epoch   8/150 | Train Loss: 0.9198 | Train Acc: 0.7860 | Val Loss: 1.2081 | Val Acc: 0.6622 | LR: 0.003113\n",
            "[Final] Epoch   9/150 | Train Loss: 0.8362 | Train Acc: 0.8303 | Val Loss: 1.1860 | Val Acc: 0.6216 | LR: 0.003113\n",
            "[Final] Epoch  10/150 | Train Loss: 0.7952 | Train Acc: 0.8584 | Val Loss: 1.0267 | Val Acc: 0.7297 | LR: 0.003113\n",
            "[Final] Epoch  11/150 | Train Loss: 0.7189 | Train Acc: 0.9073 | Val Loss: 0.9228 | Val Acc: 0.7748 | LR: 0.003113\n",
            "[Final] Epoch  12/150 | Train Loss: 0.6616 | Train Acc: 0.9489 | Val Loss: 0.8503 | Val Acc: 0.8243 | LR: 0.003113\n",
            "[Final] Epoch  13/150 | Train Loss: 0.6108 | Train Acc: 0.9652 | Val Loss: 0.9627 | Val Acc: 0.7703 | LR: 0.003113\n",
            "[Final] Epoch  14/150 | Train Loss: 0.6439 | Train Acc: 0.9483 | Val Loss: 0.8654 | Val Acc: 0.8559 | LR: 0.003113\n",
            "[Final] Epoch  15/150 | Train Loss: 0.6202 | Train Acc: 0.9579 | Val Loss: 0.8416 | Val Acc: 0.8378 | LR: 0.003113\n",
            "[Final] Epoch  16/150 | Train Loss: 0.6192 | Train Acc: 0.9534 | Val Loss: 0.8584 | Val Acc: 0.8468 | LR: 0.003113\n",
            "[Final] Epoch  17/150 | Train Loss: 0.5804 | Train Acc: 0.9725 | Val Loss: 0.7896 | Val Acc: 0.8739 | LR: 0.003113\n",
            "[Final] Epoch  18/150 | Train Loss: 0.5745 | Train Acc: 0.9775 | Val Loss: 0.7987 | Val Acc: 0.8739 | LR: 0.003113\n",
            "[Final] Epoch  19/150 | Train Loss: 0.5876 | Train Acc: 0.9702 | Val Loss: 0.9197 | Val Acc: 0.8063 | LR: 0.003113\n",
            "[Final] Epoch  20/150 | Train Loss: 0.6076 | Train Acc: 0.9657 | Val Loss: 0.8795 | Val Acc: 0.8378 | LR: 0.003113\n",
            "[Final] Epoch  21/150 | Train Loss: 0.5950 | Train Acc: 0.9685 | Val Loss: 0.8457 | Val Acc: 0.8333 | LR: 0.003113\n",
            "[Final] Epoch  22/150 | Train Loss: 0.5703 | Train Acc: 0.9792 | Val Loss: 0.8610 | Val Acc: 0.8378 | LR: 0.003113\n",
            "[Final] Epoch  23/150 | Train Loss: 0.5789 | Train Acc: 0.9730 | Val Loss: 0.8274 | Val Acc: 0.8559 | LR: 0.001556 (LR reduced from 0.003113 to 0.001556)\n",
            "[Final] Epoch  24/150 | Train Loss: 0.5433 | Train Acc: 0.9916 | Val Loss: 0.8385 | Val Acc: 0.8514 | LR: 0.001556\n",
            "[Final] Epoch  25/150 | Train Loss: 0.5293 | Train Acc: 0.9961 | Val Loss: 0.8210 | Val Acc: 0.8559 | LR: 0.001556\n",
            "[Final] Epoch  26/150 | Train Loss: 0.5301 | Train Acc: 0.9966 | Val Loss: 0.8417 | Val Acc: 0.8378 | LR: 0.001556\n",
            "[Final] Epoch  27/150 | Train Loss: 0.5281 | Train Acc: 0.9961 | Val Loss: 0.7705 | Val Acc: 0.8874 | LR: 0.001556\n",
            "[Final] Epoch  28/150 | Train Loss: 0.5305 | Train Acc: 0.9949 | Val Loss: 0.7697 | Val Acc: 0.8829 | LR: 0.001556\n",
            "[Final] Epoch  29/150 | Train Loss: 0.5249 | Train Acc: 0.9978 | Val Loss: 0.8172 | Val Acc: 0.8694 | LR: 0.001556\n",
            "[Final] Epoch  30/150 | Train Loss: 0.5327 | Train Acc: 0.9933 | Val Loss: 0.8909 | Val Acc: 0.8333 | LR: 0.001556\n",
            "[Final] Epoch  31/150 | Train Loss: 0.5516 | Train Acc: 0.9860 | Val Loss: 0.8665 | Val Acc: 0.8378 | LR: 0.001556\n",
            "[Final] Epoch  32/150 | Train Loss: 0.5456 | Train Acc: 0.9882 | Val Loss: 0.8191 | Val Acc: 0.8649 | LR: 0.001556\n",
            "[Final] Epoch  33/150 | Train Loss: 0.5359 | Train Acc: 0.9916 | Val Loss: 0.8513 | Val Acc: 0.8288 | LR: 0.001556\n",
            "[Final] Epoch  34/150 | Train Loss: 0.5397 | Train Acc: 0.9899 | Val Loss: 0.7939 | Val Acc: 0.8739 | LR: 0.000778 (LR reduced from 0.001556 to 0.000778)\n",
            "[Final] Epoch  35/150 | Train Loss: 0.5258 | Train Acc: 0.9966 | Val Loss: 0.8119 | Val Acc: 0.8514 | LR: 0.000778\n",
            "[Final] Epoch  36/150 | Train Loss: 0.5191 | Train Acc: 0.9989 | Val Loss: 0.8096 | Val Acc: 0.8559 | LR: 0.000778\n",
            "[Final] Epoch  37/150 | Train Loss: 0.5167 | Train Acc: 0.9994 | Val Loss: 0.8195 | Val Acc: 0.8649 | LR: 0.000778\n",
            "[Final] Epoch  38/150 | Train Loss: 0.5155 | Train Acc: 1.0000 | Val Loss: 0.8572 | Val Acc: 0.8514 | LR: 0.000778\n",
            "[Final] Epoch  39/150 | Train Loss: 0.5168 | Train Acc: 0.9994 | Val Loss: 0.8182 | Val Acc: 0.8649 | LR: 0.000778\n",
            "[Final] Epoch  40/150 | Train Loss: 0.5149 | Train Acc: 1.0000 | Val Loss: 0.8387 | Val Acc: 0.8559 | LR: 0.000389 (LR reduced from 0.000778 to 0.000389)\n",
            "[Final] Epoch  41/150 | Train Loss: 0.5146 | Train Acc: 1.0000 | Val Loss: 0.8338 | Val Acc: 0.8604 | LR: 0.000389\n",
            "[Final] Epoch  42/150 | Train Loss: 0.5145 | Train Acc: 1.0000 | Val Loss: 0.8435 | Val Acc: 0.8559 | LR: 0.000389\n",
            "[Final] Epoch  43/150 | Train Loss: 0.5146 | Train Acc: 1.0000 | Val Loss: 0.8251 | Val Acc: 0.8559 | LR: 0.000389\n",
            "[Final] Epoch  44/150 | Train Loss: 0.5149 | Train Acc: 1.0000 | Val Loss: 0.8623 | Val Acc: 0.8468 | LR: 0.000389\n",
            "[Final] Epoch  45/150 | Train Loss: 0.5145 | Train Acc: 1.0000 | Val Loss: 0.8627 | Val Acc: 0.8378 | LR: 0.000389\n",
            "[Final] Epoch  46/150 | Train Loss: 0.5148 | Train Acc: 1.0000 | Val Loss: 0.8717 | Val Acc: 0.8333 | LR: 0.000195 (LR reduced from 0.000389 to 0.000195)\n",
            "[Final] Epoch  47/150 | Train Loss: 0.5142 | Train Acc: 1.0000 | Val Loss: 0.8746 | Val Acc: 0.8378 | LR: 0.000195\n",
            "[Final] Epoch  48/150 | Train Loss: 0.5140 | Train Acc: 1.0000 | Val Loss: 0.8847 | Val Acc: 0.8333 | LR: 0.000195\n",
            "[Final] [EarlyStopping] triggered at epoch 48\n",
            "[Final] [Info] Loaded best model weights\n",
            "\n",
            "======================================================================\n",
            "MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Final Accuracy Scores:\n",
            "  Training Accuracy:   0.9983 (99.83%)\n",
            "  Validation Accuracy: 0.8829 (88.29%)\n",
            "  Test Accuracy:       0.8430 (84.30%)\n",
            "\n",
            "======================================================================\n",
            "DETAILED CLASSIFICATION REPORT (Test Set)\n",
            "======================================================================\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Politics       0.74      0.95      0.83        42\n",
            "        Sport       0.94      0.88      0.91        51\n",
            "   Technology       0.81      0.88      0.84        40\n",
            "Entertainment       0.88      0.74      0.81        39\n",
            "     Business       0.87      0.76      0.81        51\n",
            "\n",
            "     accuracy                           0.84       223\n",
            "    macro avg       0.85      0.84      0.84       223\n",
            " weighted avg       0.85      0.84      0.84       223\n",
            "\n",
            "\n",
            "======================================================================\n",
            "PLOTTING & SAVING FIGURES (SVG)\n",
            "======================================================================\n",
            "[Plot] Training/Validation curves -> rlstm_training_curves.svg\n",
            "[Plot] Confusion Matrix -> rlstm_confusion_matrix.svg\n",
            "[Plot] Per-class accuracy -> rlstm_per_class_accuracy.svg\n",
            "[Plot] Learning Rate schedule -> rlstm_lr_schedule.svg\n",
            "[Output] Saved SVGs: rlstm_training_curves.svg, rlstm_confusion_matrix.svg, rlstm_per_class_accuracy.svg, rlstm_lr_schedule.svg\n",
            "\n",
            "======================================================================\n",
            "SAMPLE PREDICTIONS\n",
            "======================================================================\n",
            "Text preview: <UNK> anger at <UNK> stars sale <UNK> director of rugby <UNK> <UNK> has <UNK> <UNK> <UNK> <UNK> sale host bath...\n",
            "True Label: Sport\n",
            "Predicted:  Sport\n",
            "Confidence: 78.98%\n",
            "Result: Correct\n",
            "----------------------------------------------------------------------\n",
            "Text preview: <UNK> <UNK> deal for <UNK> a <UNK> aid made from a southern african <UNK> is set to be developed by...\n",
            "True Label: Business\n",
            "Predicted:  Business\n",
            "Confidence: 78.70%\n",
            "Result: Correct\n",
            "----------------------------------------------------------------------\n",
            "Text preview: broadband <UNK> ahead in the us more and more americans are joining the <UNK> fast <UNK> according to official figures....\n",
            "True Label: Technology\n",
            "Predicted:  Business\n",
            "Confidence: 53.33%\n",
            "Result: Incorrect\n",
            "----------------------------------------------------------------------\n",
            "Text preview: ireland <UNK> england ireland <UNK> england to their third straight six nations defeat with a <UNK> victory at lansdowne <UNK>...\n",
            "True Label: Sport\n",
            "Predicted:  Sport\n",
            "Confidence: 88.80%\n",
            "Result: Correct\n",
            "----------------------------------------------------------------------\n",
            "Text preview: little britain two top comic list little britain stars matt <UNK> and david <UNK> have been named the most powerful...\n",
            "True Label: Entertainment\n",
            "Predicted:  Entertainment\n",
            "Confidence: 86.36%\n",
            "Result: Correct\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Saving model and metadata ...\n",
            "Saved model as: rlstm_pytorch_model.pt\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "Final Test Accuracy: 84.30%\n",
            "Device used: cuda\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "PyTorch Residual LSTM (R-LSTM) Text Classification — Final (No Optuna)\n",
        "Dataset: df_file.csv with columns ['Text', 'Label'] and 5 classes\n",
        "\n",
        "Pipeline:\n",
        "- Preprocess (lowercase, drop empties)\n",
        "- Build vocabulary (size=5000) -> indexify -> pad to seq_len=50\n",
        "- Model: Residual LSTM stack with per-layer linear projections (for residuals)\n",
        "- Train with best hyperparameters (from your top trial)\n",
        "- Early stopping on validation loss\n",
        "- Save:\n",
        "    * rlstm_training_curves.svg\n",
        "    * rlstm_confusion_matrix.svg\n",
        "    * rlstm_per_class_accuracy.svg\n",
        "    * rlstm_lr_schedule.svg\n",
        "    * rlstm_pytorch_model.pt\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib; matplotlib.use(\"Agg\")  # headless back-end for servers/CI\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# -----------------------------\n",
        "# Global config & reproducibility\n",
        "# -----------------------------\n",
        "GLOBAL_SEED   = 42\n",
        "SEQ_LEN       = 50\n",
        "VOCAB_SIZE    = 5000\n",
        "PRINT_LINE    = \"=\" * 70\n",
        "\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "torch.manual_seed(GLOBAL_SEED)\n",
        "\n",
        "# (Optional) make cuDNN deterministic; slightly slower but reproducible on GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def hr(msg: str):\n",
        "    print(\"\\n\" + PRINT_LINE)\n",
        "    print(msg)\n",
        "    print(PRINT_LINE)\n",
        "\n",
        "# -----------------------------\n",
        "# Best hyperparameters (from your top Optuna trial)\n",
        "# -----------------------------\n",
        "BEST_HP = {\n",
        "    \"embedding_dim\":   128,\n",
        "    \"hidden_size\":     128,\n",
        "    \"num_layers\":      2,\n",
        "    \"bidirectional\":   True,\n",
        "    \"dropout\":         0.3363570033,      # ~0.34\n",
        "    \"batch_size\":      24,\n",
        "    \"learning_rate\":   0.00311292936,     # ~3.11e-3\n",
        "    \"weight_decay\":    0.00099272647,     # ~9.93e-4\n",
        "    \"label_smoothing\": 0.1405728207,      # ~0.14\n",
        "    \"max_epochs\":      150,\n",
        "    \"patience\":        20,\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Device selection\n",
        "# -----------------------------\n",
        "def get_device():\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        print(\"Using Mac GPU (MPS)\")\n",
        "        return torch.device(\"mps\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        return torch.device(\"cuda\")\n",
        "    print(\"Using CPU\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "print(f\"Device: {device}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load data\n",
        "# -----------------------------\n",
        "hr(\"LOADING DATASET\")\n",
        "# Expecting df_file.csv with columns ['Text', 'Label']\n",
        "df = pd.read_csv(\"df_file.csv\")\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"Number of classes: {df['Label'].nunique()}\")\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['Label'].value_counts().sort_index())\n",
        "\n",
        "# Change this mapping if your labels differ\n",
        "class_names = {0: 'Politics', 1: 'Sport', 2: 'Technology', 3: 'Entertainment', 4: 'Business'}\n",
        "print(\"\\nClass mapping:\")\n",
        "for label, name in class_names.items():\n",
        "    print(f\"  {label}: {name} ({len(df[df['Label'] == label])} samples)\")\n",
        "print(PRINT_LINE)\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocess\n",
        "# -----------------------------\n",
        "hr(\"PREPROCESSING\")\n",
        "print(\"[Step] Lowercasing and dropping empty rows ...\")\n",
        "df['Text'] = df['Text'].astype(str).str.lower()\n",
        "df = df[df['Text'].str.len() > 0].reset_index(drop=True)\n",
        "print(f\"[Done] Dataset shape after preprocessing: {df.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Vocabulary\n",
        "# -----------------------------\n",
        "hr(\"VOCABULARY\")\n",
        "print(\"[Step] Counting token frequencies ...\")\n",
        "all_words = []\n",
        "for text in df['Text'].values:\n",
        "    all_words.extend(text.split())\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "print(f\"[Info] Total unique tokens: {len(word_counts)}\")\n",
        "\n",
        "print(f\"[Step] Building vocab size={VOCAB_SIZE} with <PAD>=0, <UNK>=1 ...\")\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "for w, _ in word_counts.most_common(VOCAB_SIZE - 2):\n",
        "    vocab[w] = len(vocab)\n",
        "coverage = (len(vocab) / max(1, len(word_counts))) * 100\n",
        "print(f\"[Done] Vocab size: {len(vocab)} | Coverage: {coverage:.2f}%\")\n",
        "\n",
        "# (Optional) inverse vocab for readable previews\n",
        "inverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "# -----------------------------\n",
        "# Sequences\n",
        "# -----------------------------\n",
        "hr(\"SEQUENCE ENCODING\")\n",
        "print(f\"[Info] Sequence length: {SEQ_LEN}\")\n",
        "print(\"[Step] Converting texts to index sequences ...\")\n",
        "\n",
        "X_sequences = []\n",
        "for text in df['Text'].values:\n",
        "    words = text.split()[:SEQ_LEN]  # truncate to SEQ_LEN\n",
        "    seq = [vocab.get(w, vocab['<UNK>']) for w in words]\n",
        "    if len(seq) < SEQ_LEN:          # right-pad with <PAD>\n",
        "        seq += [vocab['<PAD>']] * (SEQ_LEN - len(seq))\n",
        "    X_sequences.append(seq)\n",
        "\n",
        "X_sequences = np.array(X_sequences, dtype=np.int64)\n",
        "y_labels = df['Label'].astype(int).values\n",
        "\n",
        "print(f\"[Done] X_sequences shape: {X_sequences.shape}\")\n",
        "print(f\"[Done] y_labels shape:  {y_labels.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Split\n",
        "# -----------------------------\n",
        "hr(\"DATA SPLIT\")\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_sequences, y_labels, test_size=0.2, random_state=GLOBAL_SEED, stratify=y_labels\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=GLOBAL_SEED, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"[Info] Training set:   {X_train.shape[0]} samples ({X_train.shape[0]/len(X_sequences)*100:.1f}%)\")\n",
        "print(f\"[Info] Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X_sequences)*100:.1f}%)\")\n",
        "print(f\"[Info] Test set:       {X_test.shape[0]} samples ({X_test.shape[0]/len(X_sequences)*100:.1f}%)\")\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset class\n",
        "# -----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"Simple tensorized dataset of (padded_int_sequence, label).\"\"\"\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = torch.LongTensor(sequences)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "val_dataset   = TextDataset(X_val,   y_val)\n",
        "test_dataset  = TextDataset(X_test,  y_test)\n",
        "\n",
        "# -----------------------------\n",
        "# R-LSTM model (Residual LSTM)\n",
        "# -----------------------------\n",
        "class ResLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual LSTM stack:\n",
        "      For each layer i:\n",
        "        y_i = LSTM_i(x_i)\n",
        "        y_i = y_i + Proj_i(x_i)        # residual projection to match dims\n",
        "        x_{i+1} = Dropout(y_i)\n",
        "    After the last layer, we take the last time-step features and classify.\n",
        "    Supports bidirectionality at each layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size,\n",
        "                 num_layers=2, dropout=0.5, bidirectional=False, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.hidden_size   = hidden_size\n",
        "        self.num_layers    = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_dirs      = 2 if bidirectional else 1\n",
        "        self.out_dim       = hidden_size * self.num_dirs\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # Build per-layer LSTMs and residual projections\n",
        "        self.lstm_layers = nn.ModuleList()\n",
        "        self.proj_layers = nn.ModuleList()\n",
        "        self.drop_layers = nn.ModuleList()\n",
        "\n",
        "        in_dim = embedding_dim\n",
        "        for _ in range(num_layers):\n",
        "            lstm = nn.LSTM(\n",
        "                input_size=in_dim,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=1,\n",
        "                batch_first=True,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "            self.lstm_layers.append(lstm)\n",
        "            self.proj_layers.append(nn.Linear(in_dim, self.out_dim, bias=False))\n",
        "            self.drop_layers.append(nn.Dropout(dropout))  # dropout between layers\n",
        "            in_dim = self.out_dim  # next layer input\n",
        "\n",
        "        self.final_dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(self.out_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        emb = self.embedding(x)  # (B, T, E)\n",
        "        h = emb\n",
        "        for li in range(self.num_layers):\n",
        "            lstm = self.lstm_layers[li]\n",
        "            proj = self.proj_layers[li]\n",
        "            drop = self.drop_layers[li]\n",
        "\n",
        "            y, _ = lstm(h)        # (B, T, H * dirs)\n",
        "            res  = proj(h)        # (B, T, H * dirs)\n",
        "            y    = y + res        # residual connection\n",
        "            if li < self.num_layers - 1:\n",
        "                y = drop(y)       # dropout only between layers\n",
        "            h = y\n",
        "\n",
        "        # Last time step representation\n",
        "        last = h[:, -1, :]        # (B, H * dirs)\n",
        "        last = self.final_dropout(last)\n",
        "        logits = self.fc(last)    # (B, C)\n",
        "        return logits\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval helpers\n",
        "# -----------------------------\n",
        "def current_lr(optimizer):\n",
        "    return float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for sequences, labels in dataloader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(sequences)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * sequences.size(0)\n",
        "        _, pred = torch.max(logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (pred == labels).sum().item()\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            logits = model(sequences)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item() * sequences.size(0)\n",
        "            _, pred = torch.max(logits, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum().item()\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def make_criterion(label_smoothing_value: float):\n",
        "    \"\"\"\n",
        "    CrossEntropyLoss w/ label smoothing if supported by your PyTorch version.\n",
        "    Falls back to standard CrossEntropyLoss for older versions.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return nn.CrossEntropyLoss(label_smoothing=float(label_smoothing_value))\n",
        "    except TypeError:\n",
        "        return nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                scheduler, num_epochs, device, patience=10, log_prefix=\"\",\n",
        "                record_lr=False):\n",
        "    \"\"\"\n",
        "    Standard training loop with:\n",
        "      - gradient clipping\n",
        "      - ReduceLROnPlateau scheduler on val_loss\n",
        "      - early stopping on val_loss\n",
        "      - best-state restore\n",
        "    Returns:\n",
        "      history: dict of lists (train/val loss & acc)\n",
        "      lr_history: list of LR values (after scheduler step) per epoch\n",
        "    \"\"\"\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    lr_history = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    print(f\"{log_prefix}[Train] epochs={num_epochs}, batch_size={train_loader.batch_size}, \"\n",
        "          f\"lr={current_lr(optimizer):.6f}, patience={patience}\")\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Scheduler step\n",
        "        lr_before = current_lr(optimizer)\n",
        "        scheduler.step(val_loss)\n",
        "        lr_after = current_lr(optimizer)\n",
        "        if record_lr:\n",
        "            lr_history.append(lr_after)\n",
        "        lr_note = \"\"\n",
        "        if lr_after < lr_before:\n",
        "            lr_note = f\" (LR reduced from {lr_before:.6f} to {lr_after:.6f})\"\n",
        "\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['train_acc'].append(tr_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"{log_prefix}Epoch {epoch:3d}/{num_epochs} | \"\n",
        "              f\"Train Loss: {tr_loss:.4f} | Train Acc: {tr_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
        "              f\"LR: {current_lr(optimizer):.6f}{lr_note}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"{log_prefix}[EarlyStopping] triggered at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "        print(f\"{log_prefix}[Info] Loaded best model weights\")\n",
        "\n",
        "    return history, lr_history\n",
        "\n",
        "# -----------------------------\n",
        "# Final training with the best hyperparameters\n",
        "# -----------------------------\n",
        "hr(\"FINAL TRAINING WITH BEST HYPERPARAMETERS\")\n",
        "\n",
        "embedding_dim = int(BEST_HP[\"embedding_dim\"])\n",
        "hidden_size   = int(BEST_HP[\"hidden_size\"])\n",
        "num_layers    = int(BEST_HP[\"num_layers\"])\n",
        "bidirectional = bool(BEST_HP[\"bidirectional\"])\n",
        "dropout       = float(BEST_HP[\"dropout\"])\n",
        "batch_size    = int(BEST_HP[\"batch_size\"])\n",
        "learning_rate = float(BEST_HP[\"learning_rate\"])\n",
        "weight_decay  = float(BEST_HP[\"weight_decay\"])\n",
        "label_smooth  = float(BEST_HP[\"label_smoothing\"])\n",
        "num_epochs    = int(BEST_HP[\"max_epochs\"])\n",
        "patience      = int(BEST_HP[\"patience\"])\n",
        "\n",
        "print(\"[Final Config]\")\n",
        "print(f\"  Vocab size:       {len(vocab)}\")\n",
        "print(f\"  Embedding dim:    {embedding_dim}\")\n",
        "print(f\"  Hidden size:      {hidden_size}\")\n",
        "print(f\"  Num layers:       {num_layers}\")\n",
        "print(f\"  Bidirectional:    {bidirectional}\")\n",
        "print(f\"  Dropout:          {dropout}\")\n",
        "print(f\"  Output size:      {len(class_names)}\")\n",
        "print(f\"  Batch size:       {batch_size}\")\n",
        "print(f\"  Learning rate:    {learning_rate}\")\n",
        "print(f\"  Weight decay:     {weight_decay}\")\n",
        "print(f\"  Label smoothing:  {label_smooth}\")\n",
        "print(f\"  Max epochs:       {num_epochs}\")\n",
        "print(f\"  Sequence length:  {SEQ_LEN}\")\n",
        "\n",
        "# NOTE:\n",
        "# On Windows + Jupyter, num_workers>0 may require running as a script (__main__ guard).\n",
        "# If you face DataLoader issues, set num_workers=0.\n",
        "num_workers = 2\n",
        "pin_memory  = (device.type == \"cuda\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "model = ResLSTM(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    output_size=len(class_names),\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout,\n",
        "    bidirectional=bidirectional,\n",
        "    pad_idx=vocab['<PAD>']\n",
        ").to(device)\n",
        "\n",
        "print(\"\\nModel architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "criterion = make_criterion(label_smooth)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "history, lr_hist = train_model(\n",
        "    model, train_loader, val_loader, criterion, optimizer,\n",
        "    scheduler, num_epochs, device, patience=patience, log_prefix=\"[Final] \",\n",
        "    record_lr=True\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation\n",
        "# -----------------------------\n",
        "hr(\"MODEL EVALUATION\")\n",
        "train_loss, train_acc = evaluate(model, train_loader, criterion, device)\n",
        "val_loss, val_acc     = evaluate(model, val_loader,   criterion, device)\n",
        "test_loss, test_acc   = evaluate(model, test_loader,  criterion, device)\n",
        "\n",
        "print(\"\\nFinal Accuracy Scores:\")\n",
        "print(f\"  Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "print(f\"  Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "print(f\"  Test Accuracy:       {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "# Collect test predictions for reports/plots\n",
        "model.eval()\n",
        "all_predictions, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in test_loader:\n",
        "        sequences = sequences.to(device)\n",
        "        logits = model(sequences)\n",
        "        _, pred = torch.max(logits, 1)\n",
        "        all_predictions.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "y_test_pred = np.array(all_predictions)\n",
        "y_test_true = np.array(all_labels)\n",
        "target_names = [class_names[i] for i in range(len(class_names))]\n",
        "\n",
        "print(\"\\n\" + PRINT_LINE)\n",
        "print(\"DETAILED CLASSIFICATION REPORT (Test Set)\")\n",
        "print(PRINT_LINE + \"\\n\")\n",
        "print(classification_report(y_test_true, y_test_pred, target_names=target_names))\n",
        "\n",
        "# -----------------------------\n",
        "# Visualizations (SVG)\n",
        "# -----------------------------\n",
        "hr(\"PLOTTING & SAVING FIGURES (SVG)\")\n",
        "\n",
        "# 1) Training/Validation loss & accuracy curves\n",
        "print(\"[Plot] Training/Validation curves -> rlstm_training_curves.svg\")\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
        "plt.plot(history['val_loss'],   label='Validation Loss', linewidth=2)\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training and Validation Loss')\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Training Accuracy', linewidth=2)\n",
        "plt.plot(history['val_acc'],   label='Validation Accuracy', linewidth=2)\n",
        "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Training and Validation Accuracy')\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('rlstm_training_curves.svg', format='svg')\n",
        "plt.close()\n",
        "\n",
        "# 2) Confusion Matrix\n",
        "print(\"[Plot] Confusion Matrix -> rlstm_confusion_matrix.svg\")\n",
        "cm = confusion_matrix(y_test_true, y_test_pred)\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix (Test Set)')\n",
        "plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('rlstm_confusion_matrix.svg', format='svg')\n",
        "plt.close()\n",
        "\n",
        "# 3) Per-class accuracy\n",
        "print(\"[Plot] Per-class accuracy -> rlstm_per_class_accuracy.svg\")\n",
        "per_class_acc = []\n",
        "for i in range(len(class_names)):\n",
        "    mask = (y_test_true == i)\n",
        "    acc = np.mean(y_test_pred[mask] == y_test_true[mask]) if np.sum(mask) > 0 else 0.0\n",
        "    per_class_acc.append(acc)\n",
        "plt.figure(figsize=(9, 5))\n",
        "bars = plt.bar(range(len(class_names)), per_class_acc)\n",
        "plt.xlabel('Class'); plt.ylabel('Accuracy'); plt.title('Per-Class Accuracy on Test Set')\n",
        "plt.xticks(range(len(class_names)), target_names, rotation=45, ha='right')\n",
        "plt.ylim([0, 1.1]); plt.grid(True, alpha=0.3, axis='y')\n",
        "for bar, acc in zip(bars, per_class_acc):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "             f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('rlstm_per_class_accuracy.svg', format='svg')\n",
        "plt.close()\n",
        "\n",
        "# 4) Final LR schedule\n",
        "print(\"[Plot] Learning Rate schedule -> rlstm_lr_schedule.svg\")\n",
        "if len(lr_hist) > 0:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(range(1, len(lr_hist)+1), lr_hist, marker='o', linewidth=2)\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Learning rate'); plt.title('Final Training LR Schedule')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rlstm_lr_schedule.svg', format='svg')\n",
        "    plt.close()\n",
        "else:\n",
        "    print(\"[Info] No LR history recorded.\")\n",
        "\n",
        "print(\"[Output] Saved SVGs: rlstm_training_curves.svg, rlstm_confusion_matrix.svg, rlstm_per_class_accuracy.svg, rlstm_lr_schedule.svg\")\n",
        "\n",
        "# -----------------------------\n",
        "# Sample predictions (quick sanity check)\n",
        "# -----------------------------\n",
        "hr(\"SAMPLE PREDICTIONS\")\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "k = min(5, len(X_test))\n",
        "sample_indices = np.random.choice(len(X_test), size=k, replace=False)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx in sample_indices:\n",
        "        seq_tensor = torch.LongTensor(X_test[idx]).unsqueeze(0).to(device)\n",
        "        logits = model(seq_tensor)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        pred_label = int(np.argmax(probs))\n",
        "        true_label = int(y_test[idx])\n",
        "\n",
        "        tokens = [inverse_vocab.get(int(tok), \"<UNK>\") for tok in X_test[idx] if int(tok) != vocab['<PAD>']]\n",
        "        text_preview = \" \".join(tokens[:20]) if tokens else \"N/A\"\n",
        "\n",
        "        print(f\"Text preview: {text_preview}...\")\n",
        "        print(f\"True Label: {class_names[true_label]}\")\n",
        "        print(f\"Predicted:  {class_names[pred_label]}\")\n",
        "        print(f\"Confidence: {probs[pred_label]*100:.2f}%\")\n",
        "        print(\"Result: \" + (\"Correct\" if true_label == pred_label else \"Incorrect\"))\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "# -----------------------------\n",
        "# Save model & metadata\n",
        "# -----------------------------\n",
        "print(\"\\nSaving model and metadata ...\")\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'vocab': vocab,\n",
        "    'class_names': class_names,\n",
        "    'hyperparameters': {\n",
        "        'embedding_dim': embedding_dim,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'bidirectional': bidirectional,\n",
        "        'dropout': dropout,\n",
        "        'batch_size': batch_size,\n",
        "        'learning_rate': learning_rate,\n",
        "        'weight_decay': weight_decay,\n",
        "        'label_smoothing': label_smooth,\n",
        "        'seq_len': SEQ_LEN,\n",
        "        'vocab_size': len(vocab),\n",
        "        'architecture': 'ResLSTM'\n",
        "    }\n",
        "}, 'rlstm_pytorch_model.pt')\n",
        "print(\"Saved model as: rlstm_pytorch_model.pt\")\n",
        "\n",
        "print(\"\\n\" + PRINT_LINE)\n",
        "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(PRINT_LINE)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Device used: {device}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
