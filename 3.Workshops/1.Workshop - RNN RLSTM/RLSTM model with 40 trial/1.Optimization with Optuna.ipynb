{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### R-LSTM Text Classification with Optuna (40 Trials)\n",
        "What this notebook does\n",
        "\n",
        "Tunes a Residual LSTM (R-LSTM) classifier with Optuna for 40 trials, then retrains a final model using the best hyperparameters. It evaluates on a test set and saves both Optuna diagnostics and model artifacts.\n",
        "\n",
        "Data and labels\n",
        "\n",
        "File: df_file.csv\n",
        "\n",
        "Columns: Text (string), Label (int)\n",
        "\n",
        "Example mapping (update if needed): 0: Politics, 1: Sport, 2: Technology, 3: Entertainment, 4: Business\n",
        "\n",
        "Pipeline\n",
        "\n",
        "Lowercase text, drop empty rows.\n",
        "\n",
        "Build vocabulary of size 5000 with <PAD>=0 and <UNK>=1.\n",
        "\n",
        "Encode texts to sequences of length 50 (truncate/right-pad).\n",
        "\n",
        "Stratified split: 80% train, 10% validation, 10% test.\n",
        "\n",
        "Optuna study (TPE) maximizes validation accuracy across model/training hyperparameters.\n",
        "\n",
        "Retrain final model up to 150 epochs with early stopping.\n",
        "\n",
        "Evaluate on test set; save plots and checkpoint.\n",
        "\n",
        "Model: Residual LSTM\n",
        "\n",
        "Per-layer residual addition via linear projection to match the LSTM output dimension.\n",
        "\n",
        "Optional bidirectional LSTMs.\n",
        "\n",
        "Last time-step features feed a linear classifier.\n",
        "\n",
        "Optuna search space\n",
        "\n",
        "embedding_dim ∈ {32, 64, 128}\n",
        "\n",
        "hidden_size ∈ {32, 64, 96, 128, 160, 192}\n",
        "\n",
        "num_layers ∈ {1, 2, 3, 4}\n",
        "\n",
        "bidirectional ∈ {False, True}\n",
        "\n",
        "dropout ∈ [0.1, 0.6]\n",
        "\n",
        "batch_size ∈ {16, 24, 32, 48, 64}\n",
        "\n",
        "learning_rate ∈ [1e-4, 5e-3] (log)\n",
        "\n",
        "weight_decay ∈ [1e-6, 1e-3] (log)\n",
        "\n",
        "label_smoothing ∈ [0.0, 0.2]\n",
        "\n",
        "tune_epochs ∈ [25, 60]\n",
        "\n",
        "Final training (post-tuning)\n",
        "\n",
        "Retrain best configuration with max_epochs=150 and patience=20.\n",
        "\n",
        "Adam optimizer, ReduceLROnPlateau scheduler (factor 0.5), gradient clipping at 5.0.\n",
        "\n",
        "Label smoothing enabled if supported by your PyTorch version.\n",
        "\n",
        "Outputs saved\n",
        "\n",
        "Tuning:\n",
        "\n",
        "optuna_trials.csv (all trials)\n",
        "\n",
        "optuna_best_params.json (best hyperparameters)\n",
        "\n",
        "optuna_optimization_history.svg\n",
        "\n",
        "optuna_param_importance.svg (if importances available)\n",
        "\n",
        "Final model and evaluation:\n",
        "\n",
        "rlstm_training_curves.svg\n",
        "\n",
        "rlstm_confusion_matrix.svg\n",
        "\n",
        "rlstm_per_class_accuracy.svg\n",
        "\n",
        "rlstm_lr_schedule.svg\n",
        "\n",
        "rlstm_pytorch_model.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c5667cc234c44ecb9e7479a12b1b74e1",
            "e0e8d4d62ddf44cdbcbd7b1621999ddd",
            "615b32600f4e4efcabd2224167925596",
            "adc69da04c95460f8fd3b2886897b5bb",
            "444603650a684445aaba174addd0645d",
            "63547744ca12426cbd86faed8f28d0ab",
            "991e43d25b044f7db120a279f0eec4b7",
            "8c3bd860039f4d6d92cb4c8aaf72f4f6",
            "76e4adfab9a644bbb02bb7c4ec1a8e31",
            "bee8d116c6f648ad854a81d35dd950b0",
            "774f7586156545ab86e56f63c05fd54d"
          ]
        },
        "id": "FUMR7pEtyPwC",
        "outputId": "7e3d9ce7-d1cb-43e9-a433-c3ffc6a7a6c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Setup] Optuna not found. Installing optuna ...\n",
            "Using CPU\n",
            "Device: cpu\n",
            "\n",
            "\n",
            "======================================================================\n",
            "LOADING DATASET\n",
            "======================================================================\n",
            "Dataset shape: (2225, 2)\n",
            "Columns: ['Text', 'Label']\n",
            "Number of classes: 5\n",
            "\n",
            "Label distribution:\n",
            "Label\n",
            "0    417\n",
            "1    511\n",
            "2    401\n",
            "3    386\n",
            "4    510\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class mapping:\n",
            "  0: Politics (417 samples)\n",
            "  1: Sport (511 samples)\n",
            "  2: Technology (401 samples)\n",
            "  3: Entertainment (386 samples)\n",
            "  4: Business (510 samples)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "PREPROCESSING\n",
            "======================================================================\n",
            "[Step] Lowercasing and dropping empty rows ...\n",
            "[Done] Dataset shape after preprocessing: (2225, 2)\n",
            "\n",
            "======================================================================\n",
            "VOCABULARY\n",
            "======================================================================\n",
            "[Step] Counting token frequencies ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-10 12:42:39,868] A new study created in memory with name: no-name-6c9815eb-10fa-4721-95d0-15df551a22d8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Info] Total unique tokens: 60616\n",
            "[Step] Building vocab size=5000 with <PAD>=0, <UNK>=1 ...\n",
            "[Done] Vocab size: 5000 | Coverage: 8.25%\n",
            "\n",
            "======================================================================\n",
            "SEQUENCE ENCODING\n",
            "======================================================================\n",
            "[Info] Sequence length: 50\n",
            "[Step] Converting texts to index sequences ...\n",
            "[Done] X_sequences shape: (2225, 50)\n",
            "[Done] y_labels shape:  (2225,)\n",
            "\n",
            "======================================================================\n",
            "DATA SPLIT\n",
            "======================================================================\n",
            "[Info] Training set:   1780 samples (80.0%)\n",
            "[Info] Validation set: 222 samples (10.0%)\n",
            "[Info] Test set:       223 samples (10.0%)\n",
            "\n",
            "======================================================================\n",
            "OPTUNA STUDY (40 TRIALS)\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5667cc234c44ecb9e7479a12b1b74e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/40 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 01/40] START\n",
            "[Trial 01/40] Params: embedding_dim=64, hidden_size=160, num_layers=3, bidirectional=False, dropout=0.19, batch_size=64, lr=0.000173, weight_decay=7.5e-06, label_smoothing=0.07, tune_epochs=41\n",
            "[Trial 01/40] [Train] epochs=41, batch_size=64, lr=0.000173, patience=10\n",
            "[Trial 01/40] Epoch   1/41 | Train Loss: 1.6114 | Train Acc: 0.2292 | Val Loss: 1.6093 | Val Acc: 0.2297 | LR: 0.000173\n",
            "[Trial 01/40] Epoch   2/41 | Train Loss: 1.5928 | Train Acc: 0.2522 | Val Loss: 1.6046 | Val Acc: 0.2027 | LR: 0.000173\n",
            "[Trial 01/40] Epoch   3/41 | Train Loss: 1.5772 | Train Acc: 0.2843 | Val Loss: 1.6024 | Val Acc: 0.2342 | LR: 0.000173\n",
            "[Trial 01/40] Epoch   4/41 | Train Loss: 1.5599 | Train Acc: 0.2978 | Val Loss: 1.5985 | Val Acc: 0.2342 | LR: 0.000173\n",
            "[Trial 01/40] Epoch   5/41 | Train Loss: 1.5474 | Train Acc: 0.3292 | Val Loss: 1.5950 | Val Acc: 0.2432 | LR: 0.000173\n",
            "[Trial 01/40] Epoch   6/41 | Train Loss: 1.5240 | Train Acc: 0.3534 | Val Loss: 1.5915 | Val Acc: 0.2477 | LR: 0.000173\n",
            "[Trial 01/40] Epoch   7/41 | Train Loss: 1.4961 | Train Acc: 0.3843 | Val Loss: 1.5834 | Val Acc: 0.2568 | LR: 0.000173\n",
            "[Trial 01/40] Epoch   8/41 | Train Loss: 1.4452 | Train Acc: 0.4096 | Val Loss: 1.5274 | Val Acc: 0.3423 | LR: 0.000173\n",
            "[Trial 01/40] Epoch   9/41 | Train Loss: 1.3345 | Train Acc: 0.4798 | Val Loss: 1.4443 | Val Acc: 0.4099 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  10/41 | Train Loss: 1.2615 | Train Acc: 0.5376 | Val Loss: 1.4264 | Val Acc: 0.4189 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  11/41 | Train Loss: 1.1698 | Train Acc: 0.5584 | Val Loss: 1.3941 | Val Acc: 0.4234 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  12/41 | Train Loss: 1.0923 | Train Acc: 0.6118 | Val Loss: 1.3928 | Val Acc: 0.4595 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  13/41 | Train Loss: 1.0449 | Train Acc: 0.6511 | Val Loss: 1.4299 | Val Acc: 0.4730 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  14/41 | Train Loss: 0.9992 | Train Acc: 0.6528 | Val Loss: 1.3332 | Val Acc: 0.5090 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  15/41 | Train Loss: 0.9459 | Train Acc: 0.6837 | Val Loss: 1.2806 | Val Acc: 0.5495 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  16/41 | Train Loss: 0.8775 | Train Acc: 0.7382 | Val Loss: 1.2533 | Val Acc: 0.5676 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  17/41 | Train Loss: 0.8433 | Train Acc: 0.7404 | Val Loss: 1.3381 | Val Acc: 0.5631 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  18/41 | Train Loss: 0.8059 | Train Acc: 0.7680 | Val Loss: 1.2466 | Val Acc: 0.5856 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  19/41 | Train Loss: 0.7566 | Train Acc: 0.8000 | Val Loss: 1.2068 | Val Acc: 0.6171 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  20/41 | Train Loss: 0.6946 | Train Acc: 0.8275 | Val Loss: 1.1967 | Val Acc: 0.6486 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  21/41 | Train Loss: 0.6667 | Train Acc: 0.8506 | Val Loss: 1.2522 | Val Acc: 0.6081 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  22/41 | Train Loss: 0.6445 | Train Acc: 0.8506 | Val Loss: 1.1999 | Val Acc: 0.6532 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  23/41 | Train Loss: 0.6142 | Train Acc: 0.8624 | Val Loss: 1.1246 | Val Acc: 0.6667 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  24/41 | Train Loss: 0.5784 | Train Acc: 0.8860 | Val Loss: 1.1749 | Val Acc: 0.6622 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  25/41 | Train Loss: 0.5565 | Train Acc: 0.8983 | Val Loss: 1.1845 | Val Acc: 0.6532 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  26/41 | Train Loss: 0.5249 | Train Acc: 0.9180 | Val Loss: 1.1073 | Val Acc: 0.6982 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  27/41 | Train Loss: 0.5071 | Train Acc: 0.9225 | Val Loss: 1.1927 | Val Acc: 0.6892 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  28/41 | Train Loss: 0.5095 | Train Acc: 0.9236 | Val Loss: 1.1475 | Val Acc: 0.6802 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  29/41 | Train Loss: 0.4965 | Train Acc: 0.9264 | Val Loss: 1.0652 | Val Acc: 0.6982 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  30/41 | Train Loss: 0.4638 | Train Acc: 0.9433 | Val Loss: 1.1465 | Val Acc: 0.6757 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  31/41 | Train Loss: 0.4512 | Train Acc: 0.9466 | Val Loss: 1.1662 | Val Acc: 0.6802 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  32/41 | Train Loss: 0.4264 | Train Acc: 0.9640 | Val Loss: 1.1255 | Val Acc: 0.7027 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  33/41 | Train Loss: 0.4142 | Train Acc: 0.9691 | Val Loss: 1.1612 | Val Acc: 0.7072 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  34/41 | Train Loss: 0.4146 | Train Acc: 0.9691 | Val Loss: 1.1025 | Val Acc: 0.7252 | LR: 0.000173\n",
            "[Trial 01/40] Epoch  35/41 | Train Loss: 0.4088 | Train Acc: 0.9652 | Val Loss: 1.2160 | Val Acc: 0.6847 | LR: 0.000086 (LR reduced from 0.000173 to 0.000086)\n",
            "[Trial 01/40] Epoch  36/41 | Train Loss: 0.3929 | Train Acc: 0.9792 | Val Loss: 1.1081 | Val Acc: 0.7207 | LR: 0.000086\n",
            "[Trial 01/40] Epoch  37/41 | Train Loss: 0.3820 | Train Acc: 0.9854 | Val Loss: 1.1841 | Val Acc: 0.7072 | LR: 0.000086\n",
            "[Trial 01/40] Epoch  38/41 | Train Loss: 0.3847 | Train Acc: 0.9826 | Val Loss: 1.1215 | Val Acc: 0.7207 | LR: 0.000086\n",
            "[Trial 01/40] Epoch  39/41 | Train Loss: 0.3817 | Train Acc: 0.9837 | Val Loss: 1.1361 | Val Acc: 0.6982 | LR: 0.000086\n",
            "[Trial 01/40] [EarlyStopping] triggered at epoch 39\n",
            "[Trial 01/40] [Info] Loaded best model weights\n",
            "[Trial 01/40] Done | Val Loss: 1.0652 | Val Acc: 0.6982\n",
            "[I 2025-11-10 12:50:32,016] Trial 0 finished with value: 0.6981981981981982 and parameters: {'embedding_dim': 64, 'hidden_size': 160, 'num_layers': 3, 'bidirectional': False, 'dropout': 0.19170225492671691, 'batch_size': 64, 'learning_rate': 0.00017258215396625024, 'weight_decay': 7.523742884534855e-06, 'label_smoothing': 0.07327236865873835, 'tune_epochs': 41}. Best is trial 0 with value: 0.6981981981981982.\n",
            "[Optuna] Completed Trial 1/40 | Value: 0.6982 | Best so far: 0.6982\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 02/40] START\n",
            "[Trial 02/40] Params: embedding_dim=32, hidden_size=192, num_layers=1, bidirectional=False, dropout=0.16, batch_size=32, lr=0.000339, weight_decay=3.6e-05, label_smoothing=0.11, tune_epochs=31\n",
            "[Trial 02/40] [Train] epochs=31, batch_size=32, lr=0.000339, patience=10\n",
            "[Trial 02/40] Epoch   1/31 | Train Loss: 1.6405 | Train Acc: 0.2146 | Val Loss: 1.5933 | Val Acc: 0.2568 | LR: 0.000339\n",
            "[Trial 02/40] Epoch   2/31 | Train Loss: 1.6086 | Train Acc: 0.2393 | Val Loss: 1.5900 | Val Acc: 0.2613 | LR: 0.000339\n",
            "[Trial 02/40] Epoch   3/31 | Train Loss: 1.5843 | Train Acc: 0.2669 | Val Loss: 1.5879 | Val Acc: 0.2297 | LR: 0.000339\n",
            "[Trial 02/40] Epoch   4/31 | Train Loss: 1.5528 | Train Acc: 0.3124 | Val Loss: 1.5483 | Val Acc: 0.2973 | LR: 0.000339\n",
            "[Trial 02/40] Epoch   5/31 | Train Loss: 1.5077 | Train Acc: 0.3607 | Val Loss: 1.5153 | Val Acc: 0.3559 | LR: 0.000339\n",
            "[Trial 02/40] Epoch   6/31 | Train Loss: 1.4346 | Train Acc: 0.4163 | Val Loss: 1.4843 | Val Acc: 0.3559 | LR: 0.000339\n",
            "[Trial 02/40] Epoch   7/31 | Train Loss: 1.3783 | Train Acc: 0.4478 | Val Loss: 1.4670 | Val Acc: 0.3784 | LR: 0.000339\n",
            "[Trial 02/40] Epoch   8/31 | Train Loss: 1.3654 | Train Acc: 0.4438 | Val Loss: 1.4129 | Val Acc: 0.4279 | LR: 0.000339\n",
            "[Trial 02/40] Epoch   9/31 | Train Loss: 1.2966 | Train Acc: 0.4972 | Val Loss: 1.3975 | Val Acc: 0.4640 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  10/31 | Train Loss: 1.2421 | Train Acc: 0.5517 | Val Loss: 1.3868 | Val Acc: 0.4189 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  11/31 | Train Loss: 1.1943 | Train Acc: 0.5680 | Val Loss: 1.3459 | Val Acc: 0.4865 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  12/31 | Train Loss: 1.1815 | Train Acc: 0.5775 | Val Loss: 1.3438 | Val Acc: 0.4730 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  13/31 | Train Loss: 1.1393 | Train Acc: 0.6079 | Val Loss: 1.4101 | Val Acc: 0.4640 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  14/31 | Train Loss: 1.1001 | Train Acc: 0.6315 | Val Loss: 1.2747 | Val Acc: 0.5450 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  15/31 | Train Loss: 1.0434 | Train Acc: 0.6725 | Val Loss: 1.3240 | Val Acc: 0.5270 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  16/31 | Train Loss: 1.0049 | Train Acc: 0.6876 | Val Loss: 1.2672 | Val Acc: 0.5541 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  17/31 | Train Loss: 0.9570 | Train Acc: 0.7213 | Val Loss: 1.2879 | Val Acc: 0.5721 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  18/31 | Train Loss: 0.9613 | Train Acc: 0.7202 | Val Loss: 1.2483 | Val Acc: 0.6036 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  19/31 | Train Loss: 0.9015 | Train Acc: 0.7421 | Val Loss: 1.1700 | Val Acc: 0.6306 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  20/31 | Train Loss: 0.8658 | Train Acc: 0.7635 | Val Loss: 1.2138 | Val Acc: 0.6216 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  21/31 | Train Loss: 0.8379 | Train Acc: 0.7860 | Val Loss: 1.2164 | Val Acc: 0.6261 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  22/31 | Train Loss: 0.8348 | Train Acc: 0.7854 | Val Loss: 1.2881 | Val Acc: 0.5991 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  23/31 | Train Loss: 0.7903 | Train Acc: 0.8124 | Val Loss: 1.1648 | Val Acc: 0.6622 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  24/31 | Train Loss: 0.7551 | Train Acc: 0.8225 | Val Loss: 1.1862 | Val Acc: 0.6216 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  25/31 | Train Loss: 0.7455 | Train Acc: 0.8444 | Val Loss: 1.1901 | Val Acc: 0.6306 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  26/31 | Train Loss: 0.7200 | Train Acc: 0.8573 | Val Loss: 1.0946 | Val Acc: 0.7027 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  27/31 | Train Loss: 0.6891 | Train Acc: 0.8719 | Val Loss: 1.0860 | Val Acc: 0.6937 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  28/31 | Train Loss: 0.6622 | Train Acc: 0.8899 | Val Loss: 1.1294 | Val Acc: 0.6802 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  29/31 | Train Loss: 0.6432 | Train Acc: 0.9045 | Val Loss: 1.0808 | Val Acc: 0.7342 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  30/31 | Train Loss: 0.6374 | Train Acc: 0.9011 | Val Loss: 1.1404 | Val Acc: 0.6982 | LR: 0.000339\n",
            "[Trial 02/40] Epoch  31/31 | Train Loss: 0.6116 | Train Acc: 0.9197 | Val Loss: 1.1089 | Val Acc: 0.6802 | LR: 0.000339\n",
            "[Trial 02/40] [Info] Loaded best model weights\n",
            "[Trial 02/40] Done | Val Loss: 1.0808 | Val Acc: 0.7342\n",
            "[I 2025-11-10 12:52:21,994] Trial 1 finished with value: 0.7342342342342343 and parameters: {'embedding_dim': 32, 'hidden_size': 192, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.16101911742238942, 'batch_size': 32, 'learning_rate': 0.00033852267834519784, 'weight_decay': 3.632486956676606e-05, 'label_smoothing': 0.10934205586865593, 'tune_epochs': 31}. Best is trial 1 with value: 0.7342342342342343.\n",
            "[Optuna] Completed Trial 2/40 | Value: 0.7342 | Best so far: 0.7342\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 03/40] START\n",
            "[Trial 03/40] Params: embedding_dim=32, hidden_size=96, num_layers=4, bidirectional=False, dropout=0.37, batch_size=48, lr=0.000218, weight_decay=1.0e-06, label_smoothing=0.16, tune_epochs=50\n",
            "[Trial 03/40] [Train] epochs=50, batch_size=48, lr=0.000218, patience=10\n",
            "[Trial 03/40] Epoch   1/50 | Train Loss: 1.6150 | Train Acc: 0.2230 | Val Loss: 1.6026 | Val Acc: 0.2477 | LR: 0.000218\n",
            "[Trial 03/40] Epoch   2/50 | Train Loss: 1.6155 | Train Acc: 0.2292 | Val Loss: 1.6009 | Val Acc: 0.2387 | LR: 0.000218\n",
            "[Trial 03/40] Epoch   3/50 | Train Loss: 1.6015 | Train Acc: 0.2517 | Val Loss: 1.5981 | Val Acc: 0.2387 | LR: 0.000218\n",
            "[Trial 03/40] Epoch   4/50 | Train Loss: 1.5948 | Train Acc: 0.2556 | Val Loss: 1.5971 | Val Acc: 0.2297 | LR: 0.000218\n",
            "[Trial 03/40] Epoch   5/50 | Train Loss: 1.5908 | Train Acc: 0.2573 | Val Loss: 1.5949 | Val Acc: 0.2072 | LR: 0.000218\n",
            "[Trial 03/40] Epoch   6/50 | Train Loss: 1.5922 | Train Acc: 0.2584 | Val Loss: 1.5937 | Val Acc: 0.2162 | LR: 0.000218\n",
            "[Trial 03/40] Epoch   7/50 | Train Loss: 1.5836 | Train Acc: 0.2685 | Val Loss: 1.5939 | Val Acc: 0.2297 | LR: 0.000218\n",
            "[Trial 03/40] Epoch   8/50 | Train Loss: 1.5783 | Train Acc: 0.2871 | Val Loss: 1.5937 | Val Acc: 0.2523 | LR: 0.000218\n",
            "[Trial 03/40] Epoch   9/50 | Train Loss: 1.5722 | Train Acc: 0.2826 | Val Loss: 1.5899 | Val Acc: 0.2387 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  10/50 | Train Loss: 1.5615 | Train Acc: 0.3028 | Val Loss: 1.5832 | Val Acc: 0.2838 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  11/50 | Train Loss: 1.5451 | Train Acc: 0.3253 | Val Loss: 1.5643 | Val Acc: 0.3649 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  12/50 | Train Loss: 1.4950 | Train Acc: 0.3972 | Val Loss: 1.5383 | Val Acc: 0.3919 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  13/50 | Train Loss: 1.4679 | Train Acc: 0.4157 | Val Loss: 1.5225 | Val Acc: 0.3919 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  14/50 | Train Loss: 1.4231 | Train Acc: 0.4500 | Val Loss: 1.5149 | Val Acc: 0.4234 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  15/50 | Train Loss: 1.3745 | Train Acc: 0.4826 | Val Loss: 1.5209 | Val Acc: 0.4324 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  16/50 | Train Loss: 1.3456 | Train Acc: 0.5135 | Val Loss: 1.4738 | Val Acc: 0.4505 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  17/50 | Train Loss: 1.3205 | Train Acc: 0.5213 | Val Loss: 1.4607 | Val Acc: 0.4640 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  18/50 | Train Loss: 1.2767 | Train Acc: 0.5596 | Val Loss: 1.5502 | Val Acc: 0.4369 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  19/50 | Train Loss: 1.2571 | Train Acc: 0.5674 | Val Loss: 1.4215 | Val Acc: 0.5045 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  20/50 | Train Loss: 1.2192 | Train Acc: 0.5966 | Val Loss: 1.4116 | Val Acc: 0.5315 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  21/50 | Train Loss: 1.1833 | Train Acc: 0.6219 | Val Loss: 1.4064 | Val Acc: 0.5270 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  22/50 | Train Loss: 1.1507 | Train Acc: 0.6444 | Val Loss: 1.4017 | Val Acc: 0.5450 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  23/50 | Train Loss: 1.1263 | Train Acc: 0.6612 | Val Loss: 1.3902 | Val Acc: 0.5541 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  24/50 | Train Loss: 1.0942 | Train Acc: 0.6899 | Val Loss: 1.3977 | Val Acc: 0.5405 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  25/50 | Train Loss: 1.0904 | Train Acc: 0.6938 | Val Loss: 1.3648 | Val Acc: 0.5676 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  26/50 | Train Loss: 1.0519 | Train Acc: 0.7107 | Val Loss: 1.3780 | Val Acc: 0.5541 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  27/50 | Train Loss: 1.0243 | Train Acc: 0.7433 | Val Loss: 1.4162 | Val Acc: 0.5450 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  28/50 | Train Loss: 1.0152 | Train Acc: 0.7444 | Val Loss: 1.3405 | Val Acc: 0.5676 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  29/50 | Train Loss: 0.9954 | Train Acc: 0.7596 | Val Loss: 1.3304 | Val Acc: 0.5901 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  30/50 | Train Loss: 0.9610 | Train Acc: 0.7820 | Val Loss: 1.3729 | Val Acc: 0.5811 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  31/50 | Train Loss: 0.9364 | Train Acc: 0.8045 | Val Loss: 1.3545 | Val Acc: 0.5676 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  32/50 | Train Loss: 0.9136 | Train Acc: 0.8112 | Val Loss: 1.4044 | Val Acc: 0.5766 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  33/50 | Train Loss: 0.9175 | Train Acc: 0.8124 | Val Loss: 1.3437 | Val Acc: 0.6081 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  34/50 | Train Loss: 0.8820 | Train Acc: 0.8388 | Val Loss: 1.3130 | Val Acc: 0.6036 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  35/50 | Train Loss: 0.8769 | Train Acc: 0.8427 | Val Loss: 1.3098 | Val Acc: 0.6171 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  36/50 | Train Loss: 0.8647 | Train Acc: 0.8343 | Val Loss: 1.3019 | Val Acc: 0.5901 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  37/50 | Train Loss: 0.8382 | Train Acc: 0.8652 | Val Loss: 1.2613 | Val Acc: 0.6261 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  38/50 | Train Loss: 0.8231 | Train Acc: 0.8742 | Val Loss: 1.2559 | Val Acc: 0.6712 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  39/50 | Train Loss: 0.8050 | Train Acc: 0.8871 | Val Loss: 1.2553 | Val Acc: 0.6351 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  40/50 | Train Loss: 0.7896 | Train Acc: 0.8955 | Val Loss: 1.2574 | Val Acc: 0.6532 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  41/50 | Train Loss: 0.7992 | Train Acc: 0.8803 | Val Loss: 1.3065 | Val Acc: 0.6486 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  42/50 | Train Loss: 0.7822 | Train Acc: 0.8961 | Val Loss: 1.2385 | Val Acc: 0.6577 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  43/50 | Train Loss: 0.7765 | Train Acc: 0.9000 | Val Loss: 1.2315 | Val Acc: 0.6622 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  44/50 | Train Loss: 0.7549 | Train Acc: 0.9101 | Val Loss: 1.2598 | Val Acc: 0.6757 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  45/50 | Train Loss: 0.7475 | Train Acc: 0.9202 | Val Loss: 1.2614 | Val Acc: 0.6757 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  46/50 | Train Loss: 0.7420 | Train Acc: 0.9135 | Val Loss: 1.2754 | Val Acc: 0.6757 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  47/50 | Train Loss: 0.7363 | Train Acc: 0.9185 | Val Loss: 1.2705 | Val Acc: 0.6802 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  48/50 | Train Loss: 0.7277 | Train Acc: 0.9264 | Val Loss: 1.2457 | Val Acc: 0.6802 | LR: 0.000218\n",
            "[Trial 03/40] Epoch  49/50 | Train Loss: 0.7150 | Train Acc: 0.9337 | Val Loss: 1.2954 | Val Acc: 0.6667 | LR: 0.000109 (LR reduced from 0.000218 to 0.000109)\n",
            "[Trial 03/40] Epoch  50/50 | Train Loss: 0.7040 | Train Acc: 0.9410 | Val Loss: 1.2739 | Val Acc: 0.6757 | LR: 0.000109\n",
            "[Trial 03/40] [Info] Loaded best model weights\n",
            "[Trial 03/40] Done | Val Loss: 1.2315 | Val Acc: 0.6622\n",
            "[I 2025-11-10 12:58:34,279] Trial 2 finished with value: 0.6621621621621622 and parameters: {'embedding_dim': 32, 'hidden_size': 96, 'num_layers': 4, 'bidirectional': False, 'dropout': 0.37134804157912427, 'batch_size': 48, 'learning_rate': 0.00021757649801197563, 'weight_decay': 1.0388823104027941e-06, 'label_smoothing': 0.16309228569096684, 'tune_epochs': 50}. Best is trial 1 with value: 0.7342342342342343.\n",
            "[Optuna] Completed Trial 3/40 | Value: 0.6622 | Best so far: 0.7342\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 04/40] START\n",
            "[Trial 04/40] Params: embedding_dim=64, hidden_size=96, num_layers=3, bidirectional=False, dropout=0.16, batch_size=48, lr=0.000773, weight_decay=1.9e-05, label_smoothing=0.01, tune_epochs=28\n",
            "[Trial 04/40] [Train] epochs=28, batch_size=48, lr=0.000773, patience=10\n",
            "[Trial 04/40] Epoch   1/28 | Train Loss: 1.6127 | Train Acc: 0.2062 | Val Loss: 1.5972 | Val Acc: 0.2207 | LR: 0.000773\n",
            "[Trial 04/40] Epoch   2/28 | Train Loss: 1.5584 | Train Acc: 0.2972 | Val Loss: 1.5939 | Val Acc: 0.2883 | LR: 0.000773\n",
            "[Trial 04/40] Epoch   3/28 | Train Loss: 1.4440 | Train Acc: 0.3758 | Val Loss: 1.4908 | Val Acc: 0.3514 | LR: 0.000773\n",
            "[Trial 04/40] Epoch   4/28 | Train Loss: 1.2628 | Train Acc: 0.4865 | Val Loss: 1.3695 | Val Acc: 0.4414 | LR: 0.000773\n",
            "[Trial 04/40] Epoch   5/28 | Train Loss: 1.0346 | Train Acc: 0.5933 | Val Loss: 1.2445 | Val Acc: 0.4910 | LR: 0.000773\n",
            "[Trial 04/40] Epoch   6/28 | Train Loss: 0.8801 | Train Acc: 0.6646 | Val Loss: 1.1805 | Val Acc: 0.5631 | LR: 0.000773\n",
            "[Trial 04/40] Epoch   7/28 | Train Loss: 0.7058 | Train Acc: 0.7590 | Val Loss: 1.1961 | Val Acc: 0.5991 | LR: 0.000773\n",
            "[Trial 04/40] Epoch   8/28 | Train Loss: 0.5605 | Train Acc: 0.8006 | Val Loss: 1.0684 | Val Acc: 0.6396 | LR: 0.000773\n",
            "[Trial 04/40] Epoch   9/28 | Train Loss: 0.4334 | Train Acc: 0.8669 | Val Loss: 1.0948 | Val Acc: 0.6802 | LR: 0.000773\n",
            "[Trial 04/40] Epoch  10/28 | Train Loss: 0.3744 | Train Acc: 0.8888 | Val Loss: 1.0982 | Val Acc: 0.6982 | LR: 0.000773\n",
            "[Trial 04/40] Epoch  11/28 | Train Loss: 0.3086 | Train Acc: 0.9039 | Val Loss: 1.0155 | Val Acc: 0.7162 | LR: 0.000773\n",
            "[Trial 04/40] Epoch  12/28 | Train Loss: 0.2885 | Train Acc: 0.9208 | Val Loss: 1.0784 | Val Acc: 0.7162 | LR: 0.000773\n",
            "[Trial 04/40] Epoch  13/28 | Train Loss: 0.2256 | Train Acc: 0.9343 | Val Loss: 1.1021 | Val Acc: 0.7252 | LR: 0.000773\n",
            "[Trial 04/40] Epoch  14/28 | Train Loss: 0.1490 | Train Acc: 0.9669 | Val Loss: 1.0915 | Val Acc: 0.7432 | LR: 0.000773\n",
            "[Trial 04/40] Epoch  15/28 | Train Loss: 0.1539 | Train Acc: 0.9635 | Val Loss: 1.1415 | Val Acc: 0.7117 | LR: 0.000773\n",
            "[Trial 04/40] Epoch  16/28 | Train Loss: 0.1336 | Train Acc: 0.9702 | Val Loss: 1.1119 | Val Acc: 0.7387 | LR: 0.000773\n",
            "[Trial 04/40] Epoch  17/28 | Train Loss: 0.1284 | Train Acc: 0.9719 | Val Loss: 1.2484 | Val Acc: 0.7523 | LR: 0.000386 (LR reduced from 0.000773 to 0.000386)\n",
            "[Trial 04/40] Epoch  18/28 | Train Loss: 0.0798 | Train Acc: 0.9854 | Val Loss: 1.0697 | Val Acc: 0.7613 | LR: 0.000386\n",
            "[Trial 04/40] Epoch  19/28 | Train Loss: 0.0669 | Train Acc: 0.9933 | Val Loss: 1.0414 | Val Acc: 0.7748 | LR: 0.000386\n",
            "[Trial 04/40] Epoch  20/28 | Train Loss: 0.0606 | Train Acc: 0.9961 | Val Loss: 1.1172 | Val Acc: 0.7748 | LR: 0.000386\n",
            "[Trial 04/40] Epoch  21/28 | Train Loss: 0.0580 | Train Acc: 0.9966 | Val Loss: 1.1639 | Val Acc: 0.7748 | LR: 0.000386\n",
            "[Trial 04/40] [EarlyStopping] triggered at epoch 21\n",
            "[Trial 04/40] [Info] Loaded best model weights\n",
            "[Trial 04/40] Done | Val Loss: 1.0155 | Val Acc: 0.7162\n",
            "[I 2025-11-10 13:00:34,709] Trial 3 finished with value: 0.7162162162162162 and parameters: {'embedding_dim': 64, 'hidden_size': 96, 'num_layers': 3, 'bidirectional': False, 'dropout': 0.15979712296915086, 'batch_size': 48, 'learning_rate': 0.0007728716861851782, 'weight_decay': 1.9170041589170666e-05, 'label_smoothing': 0.005083825348819038, 'tune_epochs': 28}. Best is trial 1 with value: 0.7342342342342343.\n",
            "[Optuna] Completed Trial 4/40 | Value: 0.7162 | Best so far: 0.7342\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 05/40] START\n",
            "[Trial 05/40] Params: embedding_dim=64, hidden_size=64, num_layers=4, bidirectional=False, dropout=0.54, batch_size=32, lr=0.003330, weight_decay=9.0e-06, label_smoothing=0.02, tune_epochs=33\n",
            "[Trial 05/40] [Train] epochs=33, batch_size=32, lr=0.003330, patience=10\n",
            "[Trial 05/40] Epoch   1/33 | Train Loss: 1.6590 | Train Acc: 0.2022 | Val Loss: 1.5953 | Val Acc: 0.2252 | LR: 0.003330\n",
            "[Trial 05/40] Epoch   2/33 | Train Loss: 1.6025 | Train Acc: 0.2517 | Val Loss: 1.6026 | Val Acc: 0.2477 | LR: 0.003330\n",
            "[Trial 05/40] Epoch   3/33 | Train Loss: 1.5897 | Train Acc: 0.2702 | Val Loss: 1.5530 | Val Acc: 0.3063 | LR: 0.003330\n",
            "[Trial 05/40] Epoch   4/33 | Train Loss: 1.4886 | Train Acc: 0.3567 | Val Loss: 1.3756 | Val Acc: 0.4189 | LR: 0.003330\n",
            "[Trial 05/40] Epoch   5/33 | Train Loss: 1.2697 | Train Acc: 0.4893 | Val Loss: 1.4217 | Val Acc: 0.4144 | LR: 0.003330\n",
            "[Trial 05/40] Epoch   6/33 | Train Loss: 1.1074 | Train Acc: 0.5831 | Val Loss: 1.1191 | Val Acc: 0.6216 | LR: 0.003330\n",
            "[Trial 05/40] Epoch   7/33 | Train Loss: 0.8542 | Train Acc: 0.7017 | Val Loss: 1.1246 | Val Acc: 0.6126 | LR: 0.003330\n",
            "[Trial 05/40] Epoch   8/33 | Train Loss: 0.6845 | Train Acc: 0.7865 | Val Loss: 1.1190 | Val Acc: 0.6577 | LR: 0.003330\n",
            "[Trial 05/40] Epoch   9/33 | Train Loss: 0.5794 | Train Acc: 0.8438 | Val Loss: 0.8200 | Val Acc: 0.7703 | LR: 0.003330\n",
            "[Trial 05/40] Epoch  10/33 | Train Loss: 0.3859 | Train Acc: 0.9236 | Val Loss: 0.9688 | Val Acc: 0.7748 | LR: 0.003330\n",
            "[Trial 05/40] Epoch  11/33 | Train Loss: 0.3718 | Train Acc: 0.9298 | Val Loss: 0.9465 | Val Acc: 0.8018 | LR: 0.003330\n",
            "[Trial 05/40] Epoch  12/33 | Train Loss: 0.2848 | Train Acc: 0.9528 | Val Loss: 0.9988 | Val Acc: 0.7613 | LR: 0.003330\n",
            "[Trial 05/40] Epoch  13/33 | Train Loss: 0.2434 | Train Acc: 0.9674 | Val Loss: 0.8677 | Val Acc: 0.8243 | LR: 0.003330\n",
            "[Trial 05/40] Epoch  14/33 | Train Loss: 0.2301 | Train Acc: 0.9736 | Val Loss: 0.9340 | Val Acc: 0.8108 | LR: 0.003330\n",
            "[Trial 05/40] Epoch  15/33 | Train Loss: 0.2117 | Train Acc: 0.9831 | Val Loss: 1.0628 | Val Acc: 0.8063 | LR: 0.001665 (LR reduced from 0.003330 to 0.001665)\n",
            "[Trial 05/40] Epoch  16/33 | Train Loss: 0.1811 | Train Acc: 0.9904 | Val Loss: 0.9217 | Val Acc: 0.8108 | LR: 0.001665\n",
            "[Trial 05/40] Epoch  17/33 | Train Loss: 0.1684 | Train Acc: 0.9933 | Val Loss: 1.0221 | Val Acc: 0.8063 | LR: 0.001665\n",
            "[Trial 05/40] Epoch  18/33 | Train Loss: 0.1627 | Train Acc: 0.9944 | Val Loss: 1.0154 | Val Acc: 0.8063 | LR: 0.001665\n",
            "[Trial 05/40] Epoch  19/33 | Train Loss: 0.1596 | Train Acc: 0.9933 | Val Loss: 0.9716 | Val Acc: 0.8063 | LR: 0.001665\n",
            "[Trial 05/40] [EarlyStopping] triggered at epoch 19\n",
            "[Trial 05/40] [Info] Loaded best model weights\n",
            "[Trial 05/40] Done | Val Loss: 0.8200 | Val Acc: 0.7703\n",
            "[I 2025-11-10 13:01:58,764] Trial 4 finished with value: 0.7702702702702703 and parameters: {'embedding_dim': 64, 'hidden_size': 64, 'num_layers': 4, 'bidirectional': False, 'dropout': 0.5357302950938588, 'batch_size': 32, 'learning_rate': 0.0033299080375866637, 'weight_decay': 8.995191735587168e-06, 'label_smoothing': 0.022010384905535352, 'tune_epochs': 33}. Best is trial 4 with value: 0.7702702702702703.\n",
            "[Optuna] Completed Trial 5/40 | Value: 0.7703 | Best so far: 0.7703\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 06/40] START\n",
            "[Trial 06/40] Params: embedding_dim=128, hidden_size=64, num_layers=1, bidirectional=True, dropout=0.58, batch_size=24, lr=0.001086, weight_decay=3.2e-05, label_smoothing=0.01, tune_epochs=35\n",
            "[Trial 06/40] [Train] epochs=35, batch_size=24, lr=0.001086, patience=10\n",
            "[Trial 06/40] Epoch   1/35 | Train Loss: 1.6871 | Train Acc: 0.2225 | Val Loss: 1.6223 | Val Acc: 0.2477 | LR: 0.001086\n",
            "[Trial 06/40] Epoch   2/35 | Train Loss: 1.5835 | Train Acc: 0.2972 | Val Loss: 1.6426 | Val Acc: 0.2523 | LR: 0.001086\n",
            "[Trial 06/40] Epoch   3/35 | Train Loss: 1.5187 | Train Acc: 0.3371 | Val Loss: 1.6000 | Val Acc: 0.2838 | LR: 0.001086\n",
            "[Trial 06/40] Epoch   4/35 | Train Loss: 1.4352 | Train Acc: 0.3966 | Val Loss: 1.5632 | Val Acc: 0.3198 | LR: 0.001086\n",
            "[Trial 06/40] Epoch   5/35 | Train Loss: 1.2685 | Train Acc: 0.4961 | Val Loss: 1.4305 | Val Acc: 0.4189 | LR: 0.001086\n",
            "[Trial 06/40] Epoch   6/35 | Train Loss: 1.0885 | Train Acc: 0.5978 | Val Loss: 1.3596 | Val Acc: 0.4414 | LR: 0.001086\n",
            "[Trial 06/40] Epoch   7/35 | Train Loss: 0.8858 | Train Acc: 0.6904 | Val Loss: 1.2133 | Val Acc: 0.5360 | LR: 0.001086\n",
            "[Trial 06/40] Epoch   8/35 | Train Loss: 0.7075 | Train Acc: 0.7798 | Val Loss: 1.0467 | Val Acc: 0.6486 | LR: 0.001086\n",
            "[Trial 06/40] Epoch   9/35 | Train Loss: 0.5470 | Train Acc: 0.8449 | Val Loss: 0.9447 | Val Acc: 0.6847 | LR: 0.001086\n",
            "[Trial 06/40] Epoch  10/35 | Train Loss: 0.4150 | Train Acc: 0.8933 | Val Loss: 0.9007 | Val Acc: 0.7072 | LR: 0.001086\n",
            "[Trial 06/40] Epoch  11/35 | Train Loss: 0.3024 | Train Acc: 0.9348 | Val Loss: 0.8881 | Val Acc: 0.7252 | LR: 0.001086\n",
            "[Trial 06/40] Epoch  12/35 | Train Loss: 0.2382 | Train Acc: 0.9573 | Val Loss: 0.8950 | Val Acc: 0.7477 | LR: 0.001086\n",
            "[Trial 06/40] Epoch  13/35 | Train Loss: 0.1978 | Train Acc: 0.9691 | Val Loss: 0.9634 | Val Acc: 0.7523 | LR: 0.001086\n",
            "[Trial 06/40] Epoch  14/35 | Train Loss: 0.1753 | Train Acc: 0.9764 | Val Loss: 0.9217 | Val Acc: 0.7523 | LR: 0.001086\n",
            "[Trial 06/40] Epoch  15/35 | Train Loss: 0.1363 | Train Acc: 0.9854 | Val Loss: 0.9927 | Val Acc: 0.7297 | LR: 0.001086\n",
            "[Trial 06/40] Epoch  16/35 | Train Loss: 0.1251 | Train Acc: 0.9876 | Val Loss: 0.9834 | Val Acc: 0.7523 | LR: 0.001086\n",
            "[Trial 06/40] Epoch  17/35 | Train Loss: 0.1136 | Train Acc: 0.9899 | Val Loss: 0.9954 | Val Acc: 0.7523 | LR: 0.000543 (LR reduced from 0.001086 to 0.000543)\n",
            "[Trial 06/40] Epoch  18/35 | Train Loss: 0.0988 | Train Acc: 0.9949 | Val Loss: 0.9743 | Val Acc: 0.7523 | LR: 0.000543\n",
            "[Trial 06/40] Epoch  19/35 | Train Loss: 0.0946 | Train Acc: 0.9972 | Val Loss: 0.9939 | Val Acc: 0.7523 | LR: 0.000543\n",
            "[Trial 06/40] Epoch  20/35 | Train Loss: 0.0908 | Train Acc: 0.9983 | Val Loss: 0.9978 | Val Acc: 0.7613 | LR: 0.000543\n",
            "[Trial 06/40] Epoch  21/35 | Train Loss: 0.0910 | Train Acc: 0.9978 | Val Loss: 0.9960 | Val Acc: 0.7703 | LR: 0.000543\n",
            "[Trial 06/40] [EarlyStopping] triggered at epoch 21\n",
            "[Trial 06/40] [Info] Loaded best model weights\n",
            "[Trial 06/40] Done | Val Loss: 0.8881 | Val Acc: 0.7252\n",
            "[I 2025-11-10 13:03:08,460] Trial 5 finished with value: 0.7252252252252253 and parameters: {'embedding_dim': 128, 'hidden_size': 64, 'num_layers': 1, 'bidirectional': True, 'dropout': 0.5812236474710556, 'batch_size': 24, 'learning_rate': 0.0010855042274745086, 'weight_decay': 3.2213437409123405e-05, 'label_smoothing': 0.01029575024999787, 'tune_epochs': 35}. Best is trial 4 with value: 0.7702702702702703.\n",
            "[Optuna] Completed Trial 6/40 | Value: 0.7252 | Best so far: 0.7703\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 07/40] START\n",
            "[Trial 07/40] Params: embedding_dim=32, hidden_size=64, num_layers=1, bidirectional=False, dropout=0.52, batch_size=64, lr=0.000107, weight_decay=3.4e-05, label_smoothing=0.05, tune_epochs=48\n",
            "[Trial 07/40] [Train] epochs=48, batch_size=64, lr=0.000107, patience=10\n",
            "[Trial 07/40] Epoch   1/48 | Train Loss: 1.6948 | Train Acc: 0.1949 | Val Loss: 1.6472 | Val Acc: 0.1937 | LR: 0.000107\n",
            "[Trial 07/40] Epoch   2/48 | Train Loss: 1.6903 | Train Acc: 0.1860 | Val Loss: 1.6431 | Val Acc: 0.1847 | LR: 0.000107\n",
            "[Trial 07/40] Epoch   3/48 | Train Loss: 1.6799 | Train Acc: 0.1904 | Val Loss: 1.6394 | Val Acc: 0.2027 | LR: 0.000107\n",
            "[Trial 07/40] Epoch   4/48 | Train Loss: 1.6754 | Train Acc: 0.2045 | Val Loss: 1.6367 | Val Acc: 0.2072 | LR: 0.000107\n",
            "[Trial 07/40] Epoch   5/48 | Train Loss: 1.6662 | Train Acc: 0.2140 | Val Loss: 1.6341 | Val Acc: 0.2117 | LR: 0.000107\n",
            "[Trial 07/40] Epoch   6/48 | Train Loss: 1.6589 | Train Acc: 0.2129 | Val Loss: 1.6320 | Val Acc: 0.2162 | LR: 0.000107\n",
            "[Trial 07/40] Epoch   7/48 | Train Loss: 1.6674 | Train Acc: 0.2073 | Val Loss: 1.6303 | Val Acc: 0.1982 | LR: 0.000107\n",
            "[Trial 07/40] Epoch   8/48 | Train Loss: 1.6430 | Train Acc: 0.2242 | Val Loss: 1.6288 | Val Acc: 0.1982 | LR: 0.000107\n",
            "[Trial 07/40] Epoch   9/48 | Train Loss: 1.6481 | Train Acc: 0.2112 | Val Loss: 1.6271 | Val Acc: 0.2117 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  10/48 | Train Loss: 1.6479 | Train Acc: 0.2090 | Val Loss: 1.6260 | Val Acc: 0.2117 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  11/48 | Train Loss: 1.6267 | Train Acc: 0.2433 | Val Loss: 1.6248 | Val Acc: 0.2162 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  12/48 | Train Loss: 1.6335 | Train Acc: 0.2247 | Val Loss: 1.6234 | Val Acc: 0.2072 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  13/48 | Train Loss: 1.6343 | Train Acc: 0.2416 | Val Loss: 1.6225 | Val Acc: 0.2162 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  14/48 | Train Loss: 1.6250 | Train Acc: 0.2096 | Val Loss: 1.6217 | Val Acc: 0.2117 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  15/48 | Train Loss: 1.6264 | Train Acc: 0.2331 | Val Loss: 1.6204 | Val Acc: 0.2117 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  16/48 | Train Loss: 1.6176 | Train Acc: 0.2348 | Val Loss: 1.6194 | Val Acc: 0.2117 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  17/48 | Train Loss: 1.6262 | Train Acc: 0.2399 | Val Loss: 1.6180 | Val Acc: 0.2162 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  18/48 | Train Loss: 1.6298 | Train Acc: 0.2236 | Val Loss: 1.6172 | Val Acc: 0.2252 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  19/48 | Train Loss: 1.6093 | Train Acc: 0.2556 | Val Loss: 1.6162 | Val Acc: 0.2252 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  20/48 | Train Loss: 1.6053 | Train Acc: 0.2461 | Val Loss: 1.6150 | Val Acc: 0.2162 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  21/48 | Train Loss: 1.6097 | Train Acc: 0.2438 | Val Loss: 1.6140 | Val Acc: 0.2207 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  22/48 | Train Loss: 1.6044 | Train Acc: 0.2629 | Val Loss: 1.6130 | Val Acc: 0.2252 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  23/48 | Train Loss: 1.5984 | Train Acc: 0.2539 | Val Loss: 1.6116 | Val Acc: 0.2252 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  24/48 | Train Loss: 1.6003 | Train Acc: 0.2489 | Val Loss: 1.6105 | Val Acc: 0.2207 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  25/48 | Train Loss: 1.6001 | Train Acc: 0.2596 | Val Loss: 1.6097 | Val Acc: 0.2162 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  26/48 | Train Loss: 1.5811 | Train Acc: 0.2607 | Val Loss: 1.6077 | Val Acc: 0.2207 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  27/48 | Train Loss: 1.5855 | Train Acc: 0.2528 | Val Loss: 1.6059 | Val Acc: 0.2252 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  28/48 | Train Loss: 1.5895 | Train Acc: 0.2624 | Val Loss: 1.6039 | Val Acc: 0.2477 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  29/48 | Train Loss: 1.5818 | Train Acc: 0.2551 | Val Loss: 1.6006 | Val Acc: 0.2523 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  30/48 | Train Loss: 1.5839 | Train Acc: 0.2697 | Val Loss: 1.5989 | Val Acc: 0.2523 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  31/48 | Train Loss: 1.5716 | Train Acc: 0.2798 | Val Loss: 1.5958 | Val Acc: 0.2568 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  32/48 | Train Loss: 1.5654 | Train Acc: 0.2927 | Val Loss: 1.5936 | Val Acc: 0.2793 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  33/48 | Train Loss: 1.5607 | Train Acc: 0.2719 | Val Loss: 1.5881 | Val Acc: 0.2793 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  34/48 | Train Loss: 1.5439 | Train Acc: 0.3034 | Val Loss: 1.5857 | Val Acc: 0.2703 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  35/48 | Train Loss: 1.5285 | Train Acc: 0.3146 | Val Loss: 1.5866 | Val Acc: 0.2793 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  36/48 | Train Loss: 1.5379 | Train Acc: 0.3096 | Val Loss: 1.5848 | Val Acc: 0.2658 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  37/48 | Train Loss: 1.5388 | Train Acc: 0.2966 | Val Loss: 1.5769 | Val Acc: 0.2703 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  38/48 | Train Loss: 1.5352 | Train Acc: 0.3096 | Val Loss: 1.5824 | Val Acc: 0.2748 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  39/48 | Train Loss: 1.5096 | Train Acc: 0.3197 | Val Loss: 1.5785 | Val Acc: 0.2793 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  40/48 | Train Loss: 1.5186 | Train Acc: 0.3073 | Val Loss: 1.5708 | Val Acc: 0.2793 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  41/48 | Train Loss: 1.5005 | Train Acc: 0.3281 | Val Loss: 1.5673 | Val Acc: 0.2793 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  42/48 | Train Loss: 1.4939 | Train Acc: 0.3292 | Val Loss: 1.5664 | Val Acc: 0.2883 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  43/48 | Train Loss: 1.4854 | Train Acc: 0.3343 | Val Loss: 1.5666 | Val Acc: 0.2883 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  44/48 | Train Loss: 1.4786 | Train Acc: 0.3545 | Val Loss: 1.5655 | Val Acc: 0.3018 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  45/48 | Train Loss: 1.4744 | Train Acc: 0.3607 | Val Loss: 1.5583 | Val Acc: 0.3108 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  46/48 | Train Loss: 1.4622 | Train Acc: 0.3854 | Val Loss: 1.5693 | Val Acc: 0.3108 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  47/48 | Train Loss: 1.4538 | Train Acc: 0.3770 | Val Loss: 1.5649 | Val Acc: 0.3243 | LR: 0.000107\n",
            "[Trial 07/40] Epoch  48/48 | Train Loss: 1.4428 | Train Acc: 0.3882 | Val Loss: 1.5519 | Val Acc: 0.3378 | LR: 0.000107\n",
            "[Trial 07/40] [Info] Loaded best model weights\n",
            "[Trial 07/40] Done | Val Loss: 1.5519 | Val Acc: 0.3378\n",
            "[I 2025-11-10 13:03:51,203] Trial 6 finished with value: 0.33783783783783783 and parameters: {'embedding_dim': 32, 'hidden_size': 64, 'num_layers': 1, 'bidirectional': False, 'dropout': 0.517651247794619, 'batch_size': 64, 'learning_rate': 0.00010670437436886878, 'weight_decay': 3.4377886617795816e-05, 'label_smoothing': 0.04529915503958759, 'tune_epochs': 48}. Best is trial 4 with value: 0.7702702702702703.\n",
            "[Optuna] Completed Trial 7/40 | Value: 0.3378 | Best so far: 0.7703\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 08/40] START\n",
            "[Trial 08/40] Params: embedding_dim=64, hidden_size=32, num_layers=3, bidirectional=False, dropout=0.15, batch_size=24, lr=0.001712, weight_decay=4.9e-04, label_smoothing=0.18, tune_epochs=53\n",
            "[Trial 08/40] [Train] epochs=53, batch_size=24, lr=0.001712, patience=10\n",
            "[Trial 08/40] Epoch   1/53 | Train Loss: 1.6082 | Train Acc: 0.2135 | Val Loss: 1.6037 | Val Acc: 0.2252 | LR: 0.001712\n",
            "[Trial 08/40] Epoch   2/53 | Train Loss: 1.5720 | Train Acc: 0.3056 | Val Loss: 1.6289 | Val Acc: 0.2027 | LR: 0.001712\n",
            "[Trial 08/40] Epoch   3/53 | Train Loss: 1.4827 | Train Acc: 0.4112 | Val Loss: 1.5123 | Val Acc: 0.4279 | LR: 0.001712\n",
            "[Trial 08/40] Epoch   4/53 | Train Loss: 1.3331 | Train Acc: 0.5331 | Val Loss: 1.4469 | Val Acc: 0.4685 | LR: 0.001712\n",
            "[Trial 08/40] Epoch   5/53 | Train Loss: 1.1982 | Train Acc: 0.6376 | Val Loss: 1.4244 | Val Acc: 0.5360 | LR: 0.001712\n",
            "[Trial 08/40] Epoch   6/53 | Train Loss: 1.0753 | Train Acc: 0.7202 | Val Loss: 1.2832 | Val Acc: 0.5991 | LR: 0.001712\n",
            "[Trial 08/40] Epoch   7/53 | Train Loss: 0.9980 | Train Acc: 0.7882 | Val Loss: 1.2273 | Val Acc: 0.6351 | LR: 0.001712\n",
            "[Trial 08/40] Epoch   8/53 | Train Loss: 0.8860 | Train Acc: 0.8545 | Val Loss: 1.1725 | Val Acc: 0.6892 | LR: 0.001712\n",
            "[Trial 08/40] Epoch   9/53 | Train Loss: 0.8170 | Train Acc: 0.8966 | Val Loss: 1.1518 | Val Acc: 0.7027 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  10/53 | Train Loss: 0.7623 | Train Acc: 0.9242 | Val Loss: 1.0979 | Val Acc: 0.7523 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  11/53 | Train Loss: 0.7300 | Train Acc: 0.9433 | Val Loss: 1.1296 | Val Acc: 0.7387 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  12/53 | Train Loss: 0.6999 | Train Acc: 0.9635 | Val Loss: 1.0699 | Val Acc: 0.7658 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  13/53 | Train Loss: 0.6751 | Train Acc: 0.9758 | Val Loss: 1.0425 | Val Acc: 0.7793 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  14/53 | Train Loss: 0.6861 | Train Acc: 0.9601 | Val Loss: 1.0390 | Val Acc: 0.7748 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  15/53 | Train Loss: 0.6750 | Train Acc: 0.9713 | Val Loss: 1.1021 | Val Acc: 0.7613 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  16/53 | Train Loss: 0.6506 | Train Acc: 0.9865 | Val Loss: 1.0192 | Val Acc: 0.7883 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  17/53 | Train Loss: 0.6477 | Train Acc: 0.9860 | Val Loss: 1.0745 | Val Acc: 0.7613 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  18/53 | Train Loss: 0.6469 | Train Acc: 0.9843 | Val Loss: 1.0349 | Val Acc: 0.7973 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  19/53 | Train Loss: 0.6451 | Train Acc: 0.9882 | Val Loss: 0.9918 | Val Acc: 0.8018 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  20/53 | Train Loss: 0.6353 | Train Acc: 0.9910 | Val Loss: 1.0191 | Val Acc: 0.8153 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  21/53 | Train Loss: 0.6708 | Train Acc: 0.9747 | Val Loss: 0.9571 | Val Acc: 0.8153 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  22/53 | Train Loss: 0.6437 | Train Acc: 0.9871 | Val Loss: 0.9745 | Val Acc: 0.8243 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  23/53 | Train Loss: 0.6521 | Train Acc: 0.9826 | Val Loss: 0.9625 | Val Acc: 0.8243 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  24/53 | Train Loss: 0.6613 | Train Acc: 0.9809 | Val Loss: 1.0284 | Val Acc: 0.7793 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  25/53 | Train Loss: 0.6461 | Train Acc: 0.9871 | Val Loss: 1.0023 | Val Acc: 0.8153 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  26/53 | Train Loss: 0.6502 | Train Acc: 0.9826 | Val Loss: 0.9702 | Val Acc: 0.8153 | LR: 0.001712\n",
            "[Trial 08/40] Epoch  27/53 | Train Loss: 0.6417 | Train Acc: 0.9882 | Val Loss: 0.9805 | Val Acc: 0.8108 | LR: 0.000856 (LR reduced from 0.001712 to 0.000856)\n",
            "[Trial 08/40] Epoch  28/53 | Train Loss: 0.6262 | Train Acc: 0.9949 | Val Loss: 0.9679 | Val Acc: 0.8153 | LR: 0.000856\n",
            "[Trial 08/40] Epoch  29/53 | Train Loss: 0.6222 | Train Acc: 0.9966 | Val Loss: 0.9618 | Val Acc: 0.8243 | LR: 0.000856\n",
            "[Trial 08/40] Epoch  30/53 | Train Loss: 0.6175 | Train Acc: 0.9989 | Val Loss: 0.9635 | Val Acc: 0.8153 | LR: 0.000856\n",
            "[Trial 08/40] Epoch  31/53 | Train Loss: 0.6167 | Train Acc: 0.9994 | Val Loss: 0.9784 | Val Acc: 0.8063 | LR: 0.000856\n",
            "[Trial 08/40] [EarlyStopping] triggered at epoch 31\n",
            "[Trial 08/40] [Info] Loaded best model weights\n",
            "[Trial 08/40] Done | Val Loss: 0.9571 | Val Acc: 0.8153\n",
            "[I 2025-11-10 13:04:51,804] Trial 7 finished with value: 0.8153153153153153 and parameters: {'embedding_dim': 64, 'hidden_size': 32, 'num_layers': 3, 'bidirectional': False, 'dropout': 0.1465513839029496, 'batch_size': 24, 'learning_rate': 0.0017115000764688566, 'weight_decay': 0.0004912819179585587, 'label_smoothing': 0.17741728485302347, 'tune_epochs': 53}. Best is trial 7 with value: 0.8153153153153153.\n",
            "[Optuna] Completed Trial 8/40 | Value: 0.8153 | Best so far: 0.8153\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 09/40] START\n",
            "[Trial 09/40] Params: embedding_dim=32, hidden_size=32, num_layers=3, bidirectional=True, dropout=0.22, batch_size=48, lr=0.000924, weight_decay=1.9e-06, label_smoothing=0.07, tune_epochs=34\n",
            "[Trial 09/40] [Train] epochs=34, batch_size=48, lr=0.000924, patience=10\n",
            "[Trial 09/40] Epoch   1/34 | Train Loss: 1.6170 | Train Acc: 0.2169 | Val Loss: 1.6065 | Val Acc: 0.2117 | LR: 0.000924\n",
            "[Trial 09/40] Epoch   2/34 | Train Loss: 1.5982 | Train Acc: 0.2371 | Val Loss: 1.5910 | Val Acc: 0.2342 | LR: 0.000924\n",
            "[Trial 09/40] Epoch   3/34 | Train Loss: 1.5801 | Train Acc: 0.2697 | Val Loss: 1.5872 | Val Acc: 0.2748 | LR: 0.000924\n",
            "[Trial 09/40] Epoch   4/34 | Train Loss: 1.5660 | Train Acc: 0.2938 | Val Loss: 1.5768 | Val Acc: 0.2613 | LR: 0.000924\n",
            "[Trial 09/40] Epoch   5/34 | Train Loss: 1.5120 | Train Acc: 0.3506 | Val Loss: 1.5209 | Val Acc: 0.2748 | LR: 0.000924\n",
            "[Trial 09/40] Epoch   6/34 | Train Loss: 1.4516 | Train Acc: 0.4112 | Val Loss: 1.4628 | Val Acc: 0.3829 | LR: 0.000924\n",
            "[Trial 09/40] Epoch   7/34 | Train Loss: 1.3284 | Train Acc: 0.4781 | Val Loss: 1.4384 | Val Acc: 0.4459 | LR: 0.000924\n",
            "[Trial 09/40] Epoch   8/34 | Train Loss: 1.2298 | Train Acc: 0.5438 | Val Loss: 1.3498 | Val Acc: 0.5045 | LR: 0.000924\n",
            "[Trial 09/40] Epoch   9/34 | Train Loss: 1.1484 | Train Acc: 0.5916 | Val Loss: 1.2933 | Val Acc: 0.5225 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  10/34 | Train Loss: 1.0372 | Train Acc: 0.6635 | Val Loss: 1.2927 | Val Acc: 0.5225 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  11/34 | Train Loss: 0.9775 | Train Acc: 0.6764 | Val Loss: 1.2309 | Val Acc: 0.5811 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  12/34 | Train Loss: 0.8927 | Train Acc: 0.7343 | Val Loss: 1.2592 | Val Acc: 0.5721 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  13/34 | Train Loss: 0.8224 | Train Acc: 0.7596 | Val Loss: 1.1523 | Val Acc: 0.6577 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  14/34 | Train Loss: 0.7432 | Train Acc: 0.8079 | Val Loss: 1.1397 | Val Acc: 0.6802 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  15/34 | Train Loss: 0.7052 | Train Acc: 0.8354 | Val Loss: 1.1050 | Val Acc: 0.6577 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  16/34 | Train Loss: 0.6842 | Train Acc: 0.8449 | Val Loss: 1.1461 | Val Acc: 0.6577 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  17/34 | Train Loss: 0.6222 | Train Acc: 0.8725 | Val Loss: 1.1598 | Val Acc: 0.6712 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  18/34 | Train Loss: 0.5611 | Train Acc: 0.9028 | Val Loss: 1.1010 | Val Acc: 0.7207 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  19/34 | Train Loss: 0.5166 | Train Acc: 0.9180 | Val Loss: 1.0525 | Val Acc: 0.7297 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  20/34 | Train Loss: 0.4984 | Train Acc: 0.9270 | Val Loss: 1.0367 | Val Acc: 0.7387 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  21/34 | Train Loss: 0.4689 | Train Acc: 0.9421 | Val Loss: 1.0686 | Val Acc: 0.7342 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  22/34 | Train Loss: 0.4642 | Train Acc: 0.9444 | Val Loss: 1.0691 | Val Acc: 0.7297 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  23/34 | Train Loss: 0.4361 | Train Acc: 0.9562 | Val Loss: 1.0400 | Val Acc: 0.7432 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  24/34 | Train Loss: 0.4445 | Train Acc: 0.9562 | Val Loss: 1.0743 | Val Acc: 0.7162 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  25/34 | Train Loss: 0.4143 | Train Acc: 0.9685 | Val Loss: 1.0277 | Val Acc: 0.7523 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  26/34 | Train Loss: 0.4003 | Train Acc: 0.9753 | Val Loss: 1.0689 | Val Acc: 0.7387 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  27/34 | Train Loss: 0.3943 | Train Acc: 0.9781 | Val Loss: 1.0358 | Val Acc: 0.7523 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  28/34 | Train Loss: 0.3809 | Train Acc: 0.9792 | Val Loss: 1.0412 | Val Acc: 0.7703 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  29/34 | Train Loss: 0.3793 | Train Acc: 0.9815 | Val Loss: 1.0078 | Val Acc: 0.7748 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  30/34 | Train Loss: 0.3741 | Train Acc: 0.9848 | Val Loss: 1.0644 | Val Acc: 0.7342 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  31/34 | Train Loss: 0.3701 | Train Acc: 0.9848 | Val Loss: 1.0991 | Val Acc: 0.7342 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  32/34 | Train Loss: 0.3701 | Train Acc: 0.9837 | Val Loss: 1.0404 | Val Acc: 0.7568 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  33/34 | Train Loss: 0.3808 | Train Acc: 0.9803 | Val Loss: 1.0854 | Val Acc: 0.7477 | LR: 0.000924\n",
            "[Trial 09/40] Epoch  34/34 | Train Loss: 0.3562 | Train Acc: 0.9910 | Val Loss: 0.9825 | Val Acc: 0.7613 | LR: 0.000924\n",
            "[Trial 09/40] [Info] Loaded best model weights\n",
            "[Trial 09/40] Done | Val Loss: 0.9825 | Val Acc: 0.7613\n",
            "[I 2025-11-10 13:06:29,405] Trial 8 finished with value: 0.7612612612612613 and parameters: {'embedding_dim': 32, 'hidden_size': 32, 'num_layers': 3, 'bidirectional': True, 'dropout': 0.21862454374840004, 'batch_size': 48, 'learning_rate': 0.0009237177101041925, 'weight_decay': 1.909960138550539e-06, 'label_smoothing': 0.0735431606118867, 'tune_epochs': 34}. Best is trial 7 with value: 0.8153153153153153.\n",
            "[Optuna] Completed Trial 9/40 | Value: 0.7613 | Best so far: 0.8153\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 10/40] START\n",
            "[Trial 10/40] Params: embedding_dim=64, hidden_size=32, num_layers=2, bidirectional=False, dropout=0.57, batch_size=16, lr=0.000534, weight_decay=7.9e-04, label_smoothing=0.19, tune_epochs=55\n",
            "[Trial 10/40] [Train] epochs=55, batch_size=16, lr=0.000534, patience=10\n",
            "[Trial 10/40] Epoch   1/55 | Train Loss: 1.6674 | Train Acc: 0.2112 | Val Loss: 1.6059 | Val Acc: 0.2342 | LR: 0.000534\n",
            "[Trial 10/40] Epoch   2/55 | Train Loss: 1.6331 | Train Acc: 0.2174 | Val Loss: 1.6045 | Val Acc: 0.2117 | LR: 0.000534\n",
            "[Trial 10/40] Epoch   3/55 | Train Loss: 1.6210 | Train Acc: 0.2213 | Val Loss: 1.6020 | Val Acc: 0.2477 | LR: 0.000534\n",
            "[Trial 10/40] Epoch   4/55 | Train Loss: 1.6097 | Train Acc: 0.2466 | Val Loss: 1.6038 | Val Acc: 0.2613 | LR: 0.000534\n",
            "[Trial 10/40] Epoch   5/55 | Train Loss: 1.6054 | Train Acc: 0.2494 | Val Loss: 1.6029 | Val Acc: 0.2523 | LR: 0.000534\n",
            "[Trial 10/40] Epoch   6/55 | Train Loss: 1.5983 | Train Acc: 0.2455 | Val Loss: 1.6021 | Val Acc: 0.2568 | LR: 0.000534\n",
            "[Trial 10/40] Epoch   7/55 | Train Loss: 1.5976 | Train Acc: 0.2697 | Val Loss: 1.6013 | Val Acc: 0.2568 | LR: 0.000534\n",
            "[Trial 10/40] Epoch   8/55 | Train Loss: 1.5938 | Train Acc: 0.2708 | Val Loss: 1.5999 | Val Acc: 0.2613 | LR: 0.000534\n",
            "[Trial 10/40] Epoch   9/55 | Train Loss: 1.5876 | Train Acc: 0.2702 | Val Loss: 1.5996 | Val Acc: 0.2523 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  10/55 | Train Loss: 1.5787 | Train Acc: 0.2899 | Val Loss: 1.5960 | Val Acc: 0.2523 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  11/55 | Train Loss: 1.5776 | Train Acc: 0.2876 | Val Loss: 1.5936 | Val Acc: 0.2207 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  12/55 | Train Loss: 1.5653 | Train Acc: 0.3034 | Val Loss: 1.5937 | Val Acc: 0.2162 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  13/55 | Train Loss: 1.5570 | Train Acc: 0.3090 | Val Loss: 1.5824 | Val Acc: 0.2342 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  14/55 | Train Loss: 1.5308 | Train Acc: 0.3444 | Val Loss: 1.5556 | Val Acc: 0.3153 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  15/55 | Train Loss: 1.5187 | Train Acc: 0.3556 | Val Loss: 1.5339 | Val Acc: 0.3423 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  16/55 | Train Loss: 1.4703 | Train Acc: 0.4011 | Val Loss: 1.5286 | Val Acc: 0.3378 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  17/55 | Train Loss: 1.4357 | Train Acc: 0.4343 | Val Loss: 1.4878 | Val Acc: 0.3964 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  18/55 | Train Loss: 1.3807 | Train Acc: 0.4787 | Val Loss: 1.4510 | Val Acc: 0.4459 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  19/55 | Train Loss: 1.3201 | Train Acc: 0.5287 | Val Loss: 1.4045 | Val Acc: 0.4685 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  20/55 | Train Loss: 1.2452 | Train Acc: 0.6135 | Val Loss: 1.3170 | Val Acc: 0.5495 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  21/55 | Train Loss: 1.1716 | Train Acc: 0.6781 | Val Loss: 1.2706 | Val Acc: 0.5901 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  22/55 | Train Loss: 1.1022 | Train Acc: 0.7152 | Val Loss: 1.2633 | Val Acc: 0.6036 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  23/55 | Train Loss: 1.0517 | Train Acc: 0.7674 | Val Loss: 1.2180 | Val Acc: 0.6216 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  24/55 | Train Loss: 1.0095 | Train Acc: 0.7983 | Val Loss: 1.1734 | Val Acc: 0.6802 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  25/55 | Train Loss: 0.9712 | Train Acc: 0.8264 | Val Loss: 1.2225 | Val Acc: 0.6486 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  26/55 | Train Loss: 0.9655 | Train Acc: 0.8388 | Val Loss: 1.1157 | Val Acc: 0.7162 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  27/55 | Train Loss: 0.9084 | Train Acc: 0.8764 | Val Loss: 1.0946 | Val Acc: 0.7297 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  28/55 | Train Loss: 0.8819 | Train Acc: 0.9039 | Val Loss: 1.0705 | Val Acc: 0.7613 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  29/55 | Train Loss: 0.8367 | Train Acc: 0.9388 | Val Loss: 1.1268 | Val Acc: 0.7252 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  30/55 | Train Loss: 0.8230 | Train Acc: 0.9449 | Val Loss: 1.0990 | Val Acc: 0.7477 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  31/55 | Train Loss: 0.7956 | Train Acc: 0.9573 | Val Loss: 1.1229 | Val Acc: 0.7387 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  32/55 | Train Loss: 0.7806 | Train Acc: 0.9685 | Val Loss: 1.1083 | Val Acc: 0.7252 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  33/55 | Train Loss: 0.7794 | Train Acc: 0.9663 | Val Loss: 1.1087 | Val Acc: 0.7523 | LR: 0.000534\n",
            "[Trial 10/40] Epoch  34/55 | Train Loss: 0.7840 | Train Acc: 0.9657 | Val Loss: 1.0959 | Val Acc: 0.7523 | LR: 0.000267 (LR reduced from 0.000534 to 0.000267)\n",
            "[Trial 10/40] Epoch  35/55 | Train Loss: 0.7605 | Train Acc: 0.9775 | Val Loss: 1.1031 | Val Acc: 0.7523 | LR: 0.000267\n",
            "[Trial 10/40] Epoch  36/55 | Train Loss: 0.7468 | Train Acc: 0.9860 | Val Loss: 1.1098 | Val Acc: 0.7477 | LR: 0.000267\n",
            "[Trial 10/40] Epoch  37/55 | Train Loss: 0.7374 | Train Acc: 0.9876 | Val Loss: 1.0983 | Val Acc: 0.7432 | LR: 0.000267\n",
            "[Trial 10/40] Epoch  38/55 | Train Loss: 0.7338 | Train Acc: 0.9899 | Val Loss: 1.0912 | Val Acc: 0.7523 | LR: 0.000267\n",
            "[Trial 10/40] [EarlyStopping] triggered at epoch 38\n",
            "[Trial 10/40] [Info] Loaded best model weights\n",
            "[Trial 10/40] Done | Val Loss: 1.0705 | Val Acc: 0.7613\n",
            "[I 2025-11-10 13:07:37,509] Trial 9 finished with value: 0.7612612612612613 and parameters: {'embedding_dim': 64, 'hidden_size': 32, 'num_layers': 2, 'bidirectional': False, 'dropout': 0.5702292921764571, 'batch_size': 16, 'learning_rate': 0.00053391487960469, 'weight_decay': 0.0007942632291801132, 'label_smoothing': 0.19272399541785057, 'tune_epochs': 55}. Best is trial 7 with value: 0.8153153153153153.\n",
            "[Optuna] Completed Trial 10/40 | Value: 0.7613 | Best so far: 0.8153\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 11/40] START\n",
            "[Trial 11/40] Params: embedding_dim=128, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.31, batch_size=24, lr=0.003001, weight_decay=8.5e-04, label_smoothing=0.13, tune_epochs=60\n",
            "[Trial 11/40] [Train] epochs=60, batch_size=24, lr=0.003001, patience=10\n",
            "[Trial 11/40] Epoch   1/60 | Train Loss: 1.7029 | Train Acc: 0.2343 | Val Loss: 1.6265 | Val Acc: 0.2838 | LR: 0.003001\n",
            "[Trial 11/40] Epoch   2/60 | Train Loss: 1.5781 | Train Acc: 0.3107 | Val Loss: 1.6076 | Val Acc: 0.2568 | LR: 0.003001\n",
            "[Trial 11/40] Epoch   3/60 | Train Loss: 1.5341 | Train Acc: 0.3528 | Val Loss: 1.6197 | Val Acc: 0.2568 | LR: 0.003001\n",
            "[Trial 11/40] Epoch   4/60 | Train Loss: 1.4532 | Train Acc: 0.4236 | Val Loss: 1.5472 | Val Acc: 0.3649 | LR: 0.003001\n",
            "[Trial 11/40] Epoch   5/60 | Train Loss: 1.3108 | Train Acc: 0.5264 | Val Loss: 1.5062 | Val Acc: 0.4865 | LR: 0.003001\n",
            "[Trial 11/40] Epoch   6/60 | Train Loss: 1.1542 | Train Acc: 0.6258 | Val Loss: 1.3834 | Val Acc: 0.4775 | LR: 0.003001\n",
            "[Trial 11/40] Epoch   7/60 | Train Loss: 1.0555 | Train Acc: 0.6955 | Val Loss: 1.3084 | Val Acc: 0.5135 | LR: 0.003001\n",
            "[Trial 11/40] Epoch   8/60 | Train Loss: 1.0031 | Train Acc: 0.7337 | Val Loss: 1.4354 | Val Acc: 0.4685 | LR: 0.003001\n",
            "[Trial 11/40] Epoch   9/60 | Train Loss: 0.9470 | Train Acc: 0.7652 | Val Loss: 1.2137 | Val Acc: 0.5946 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  10/60 | Train Loss: 0.8160 | Train Acc: 0.8472 | Val Loss: 1.1134 | Val Acc: 0.7297 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  11/60 | Train Loss: 0.7213 | Train Acc: 0.9017 | Val Loss: 1.0921 | Val Acc: 0.7027 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  12/60 | Train Loss: 0.6453 | Train Acc: 0.9404 | Val Loss: 0.9345 | Val Acc: 0.7883 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  13/60 | Train Loss: 0.6102 | Train Acc: 0.9607 | Val Loss: 0.9446 | Val Acc: 0.7928 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  14/60 | Train Loss: 0.6158 | Train Acc: 0.9506 | Val Loss: 0.9509 | Val Acc: 0.8108 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  15/60 | Train Loss: 0.5765 | Train Acc: 0.9669 | Val Loss: 1.1843 | Val Acc: 0.7162 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  16/60 | Train Loss: 0.5970 | Train Acc: 0.9601 | Val Loss: 0.8911 | Val Acc: 0.8153 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  17/60 | Train Loss: 0.5486 | Train Acc: 0.9837 | Val Loss: 0.8768 | Val Acc: 0.8243 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  18/60 | Train Loss: 0.5703 | Train Acc: 0.9685 | Val Loss: 0.8830 | Val Acc: 0.8153 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  19/60 | Train Loss: 0.6106 | Train Acc: 0.9522 | Val Loss: 0.8417 | Val Acc: 0.8378 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  20/60 | Train Loss: 0.5997 | Train Acc: 0.9590 | Val Loss: 0.8978 | Val Acc: 0.8198 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  21/60 | Train Loss: 0.5519 | Train Acc: 0.9815 | Val Loss: 0.7938 | Val Acc: 0.8604 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  22/60 | Train Loss: 0.5453 | Train Acc: 0.9826 | Val Loss: 0.8116 | Val Acc: 0.8468 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  23/60 | Train Loss: 0.5646 | Train Acc: 0.9747 | Val Loss: 0.8125 | Val Acc: 0.8514 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  24/60 | Train Loss: 0.5551 | Train Acc: 0.9764 | Val Loss: 0.8731 | Val Acc: 0.8153 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  25/60 | Train Loss: 0.5577 | Train Acc: 0.9770 | Val Loss: 0.7772 | Val Acc: 0.8694 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  26/60 | Train Loss: 0.5317 | Train Acc: 0.9871 | Val Loss: 0.7862 | Val Acc: 0.8829 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  27/60 | Train Loss: 0.5627 | Train Acc: 0.9736 | Val Loss: 0.8333 | Val Acc: 0.8559 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  28/60 | Train Loss: 0.5602 | Train Acc: 0.9764 | Val Loss: 0.9123 | Val Acc: 0.8423 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  29/60 | Train Loss: 0.6051 | Train Acc: 0.9590 | Val Loss: 0.8251 | Val Acc: 0.8514 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  30/60 | Train Loss: 0.5668 | Train Acc: 0.9764 | Val Loss: 0.8077 | Val Acc: 0.8649 | LR: 0.003001\n",
            "[Trial 11/40] Epoch  31/60 | Train Loss: 0.5485 | Train Acc: 0.9792 | Val Loss: 0.8672 | Val Acc: 0.8333 | LR: 0.001500 (LR reduced from 0.003001 to 0.001500)\n",
            "[Trial 11/40] Epoch  32/60 | Train Loss: 0.5231 | Train Acc: 0.9893 | Val Loss: 0.7800 | Val Acc: 0.8559 | LR: 0.001500\n",
            "[Trial 11/40] Epoch  33/60 | Train Loss: 0.5014 | Train Acc: 0.9989 | Val Loss: 0.7965 | Val Acc: 0.8649 | LR: 0.001500\n",
            "[Trial 11/40] Epoch  34/60 | Train Loss: 0.5006 | Train Acc: 0.9978 | Val Loss: 0.8015 | Val Acc: 0.8649 | LR: 0.001500\n",
            "[Trial 11/40] Epoch  35/60 | Train Loss: 0.5014 | Train Acc: 0.9972 | Val Loss: 0.7805 | Val Acc: 0.8694 | LR: 0.001500\n",
            "[Trial 11/40] [EarlyStopping] triggered at epoch 35\n",
            "[Trial 11/40] [Info] Loaded best model weights\n",
            "[Trial 11/40] Done | Val Loss: 0.7772 | Val Acc: 0.8694\n",
            "[I 2025-11-10 13:31:31,048] Trial 10 finished with value: 0.8693693693693694 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.3069178891075858, 'batch_size': 24, 'learning_rate': 0.0030005511876929733, 'weight_decay': 0.0008506545407706322, 'label_smoothing': 0.13269125480360605, 'tune_epochs': 60}. Best is trial 10 with value: 0.8693693693693694.\n",
            "[Optuna] Completed Trial 11/40 | Value: 0.8694 | Best so far: 0.8694\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 12/40] START\n",
            "[Trial 12/40] Params: embedding_dim=128, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.34, batch_size=24, lr=0.003113, weight_decay=9.9e-04, label_smoothing=0.14, tune_epochs=59\n",
            "[Trial 12/40] [Train] epochs=59, batch_size=24, lr=0.003113, patience=10\n",
            "[Trial 12/40] Epoch   1/59 | Train Loss: 1.7132 | Train Acc: 0.2292 | Val Loss: 1.6609 | Val Acc: 0.2252 | LR: 0.003113\n",
            "[Trial 12/40] Epoch   2/59 | Train Loss: 1.5803 | Train Acc: 0.3062 | Val Loss: 1.5873 | Val Acc: 0.3018 | LR: 0.003113\n",
            "[Trial 12/40] Epoch   3/59 | Train Loss: 1.5473 | Train Acc: 0.3517 | Val Loss: 1.6228 | Val Acc: 0.2568 | LR: 0.003113\n",
            "[Trial 12/40] Epoch   4/59 | Train Loss: 1.4580 | Train Acc: 0.4202 | Val Loss: 1.5164 | Val Acc: 0.3423 | LR: 0.003113\n",
            "[Trial 12/40] Epoch   5/59 | Train Loss: 1.3074 | Train Acc: 0.5281 | Val Loss: 1.5133 | Val Acc: 0.4369 | LR: 0.003113\n",
            "[Trial 12/40] Epoch   6/59 | Train Loss: 1.1798 | Train Acc: 0.6433 | Val Loss: 1.3479 | Val Acc: 0.5586 | LR: 0.003113\n",
            "[Trial 12/40] Epoch   7/59 | Train Loss: 1.0188 | Train Acc: 0.7326 | Val Loss: 1.3774 | Val Acc: 0.5315 | LR: 0.003113\n",
            "[Trial 12/40] Epoch   8/59 | Train Loss: 0.9335 | Train Acc: 0.7888 | Val Loss: 1.2543 | Val Acc: 0.6216 | LR: 0.003113\n",
            "[Trial 12/40] Epoch   9/59 | Train Loss: 0.8760 | Train Acc: 0.8242 | Val Loss: 1.0973 | Val Acc: 0.7117 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  10/59 | Train Loss: 0.7840 | Train Acc: 0.8697 | Val Loss: 0.9763 | Val Acc: 0.7973 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  11/59 | Train Loss: 0.6936 | Train Acc: 0.9253 | Val Loss: 0.8932 | Val Acc: 0.8108 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  12/59 | Train Loss: 0.7062 | Train Acc: 0.9242 | Val Loss: 0.9250 | Val Acc: 0.8153 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  13/59 | Train Loss: 0.6641 | Train Acc: 0.9455 | Val Loss: 0.8530 | Val Acc: 0.8423 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  14/59 | Train Loss: 0.6291 | Train Acc: 0.9534 | Val Loss: 0.9261 | Val Acc: 0.8153 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  15/59 | Train Loss: 0.6332 | Train Acc: 0.9601 | Val Loss: 0.8976 | Val Acc: 0.8333 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  16/59 | Train Loss: 0.6365 | Train Acc: 0.9522 | Val Loss: 0.7989 | Val Acc: 0.8694 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  17/59 | Train Loss: 0.5997 | Train Acc: 0.9674 | Val Loss: 0.8334 | Val Acc: 0.8423 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  18/59 | Train Loss: 0.5920 | Train Acc: 0.9685 | Val Loss: 0.7815 | Val Acc: 0.8649 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  19/59 | Train Loss: 0.5819 | Train Acc: 0.9725 | Val Loss: 0.8251 | Val Acc: 0.8604 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  20/59 | Train Loss: 0.5812 | Train Acc: 0.9770 | Val Loss: 0.8707 | Val Acc: 0.8153 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  21/59 | Train Loss: 0.5869 | Train Acc: 0.9697 | Val Loss: 0.8133 | Val Acc: 0.8649 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  22/59 | Train Loss: 0.5870 | Train Acc: 0.9713 | Val Loss: 0.7722 | Val Acc: 0.8919 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  23/59 | Train Loss: 0.5596 | Train Acc: 0.9860 | Val Loss: 0.7749 | Val Acc: 0.8874 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  24/59 | Train Loss: 0.5866 | Train Acc: 0.9753 | Val Loss: 0.8208 | Val Acc: 0.8604 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  25/59 | Train Loss: 0.5821 | Train Acc: 0.9713 | Val Loss: 0.8731 | Val Acc: 0.8378 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  26/59 | Train Loss: 0.6013 | Train Acc: 0.9691 | Val Loss: 0.8128 | Val Acc: 0.8559 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  27/59 | Train Loss: 0.5856 | Train Acc: 0.9747 | Val Loss: 0.8022 | Val Acc: 0.8784 | LR: 0.003113\n",
            "[Trial 12/40] Epoch  28/59 | Train Loss: 0.5905 | Train Acc: 0.9685 | Val Loss: 0.8607 | Val Acc: 0.8378 | LR: 0.001556 (LR reduced from 0.003113 to 0.001556)\n",
            "[Trial 12/40] Epoch  29/59 | Train Loss: 0.5526 | Train Acc: 0.9860 | Val Loss: 0.8304 | Val Acc: 0.8604 | LR: 0.001556\n",
            "[Trial 12/40] Epoch  30/59 | Train Loss: 0.5256 | Train Acc: 0.9978 | Val Loss: 0.8077 | Val Acc: 0.8649 | LR: 0.001556\n",
            "[Trial 12/40] Epoch  31/59 | Train Loss: 0.5247 | Train Acc: 0.9983 | Val Loss: 0.8139 | Val Acc: 0.8514 | LR: 0.001556\n",
            "[Trial 12/40] Epoch  32/59 | Train Loss: 0.5212 | Train Acc: 0.9983 | Val Loss: 0.8089 | Val Acc: 0.8604 | LR: 0.001556\n",
            "[Trial 12/40] [EarlyStopping] triggered at epoch 32\n",
            "[Trial 12/40] [Info] Loaded best model weights\n",
            "[Trial 12/40] Done | Val Loss: 0.7722 | Val Acc: 0.8919\n",
            "[I 2025-11-10 13:51:18,931] Trial 11 finished with value: 0.8918918918918919 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.3363570033289744, 'batch_size': 24, 'learning_rate': 0.0031129293632976705, 'weight_decay': 0.0009927264735488336, 'label_smoothing': 0.14057282069260202, 'tune_epochs': 59}. Best is trial 11 with value: 0.8918918918918919.\n",
            "[Optuna] Completed Trial 12/40 | Value: 0.8919 | Best so far: 0.8919\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 13/40] START\n",
            "[Trial 13/40] Params: embedding_dim=128, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.35, batch_size=24, lr=0.004841, weight_decay=1.4e-04, label_smoothing=0.13, tune_epochs=60\n",
            "[Trial 13/40] [Train] epochs=60, batch_size=24, lr=0.004841, patience=10\n",
            "[Trial 13/40] Epoch   1/60 | Train Loss: 1.7874 | Train Acc: 0.2146 | Val Loss: 1.6675 | Val Acc: 0.2252 | LR: 0.004841\n",
            "[Trial 13/40] Epoch   2/60 | Train Loss: 1.6792 | Train Acc: 0.2635 | Val Loss: 1.6991 | Val Acc: 0.2613 | LR: 0.004841\n",
            "[Trial 13/40] Epoch   3/60 | Train Loss: 1.6106 | Train Acc: 0.3579 | Val Loss: 1.7647 | Val Acc: 0.3108 | LR: 0.004841\n",
            "[Trial 13/40] Epoch   4/60 | Train Loss: 1.5062 | Train Acc: 0.4534 | Val Loss: 1.6582 | Val Acc: 0.3514 | LR: 0.004841\n",
            "[Trial 13/40] Epoch   5/60 | Train Loss: 1.2488 | Train Acc: 0.5983 | Val Loss: 1.5020 | Val Acc: 0.4865 | LR: 0.004841\n",
            "[Trial 13/40] Epoch   6/60 | Train Loss: 1.0448 | Train Acc: 0.7213 | Val Loss: 1.2542 | Val Acc: 0.5901 | LR: 0.004841\n",
            "[Trial 13/40] Epoch   7/60 | Train Loss: 0.8890 | Train Acc: 0.8051 | Val Loss: 1.1783 | Val Acc: 0.6486 | LR: 0.004841\n",
            "[Trial 13/40] Epoch   8/60 | Train Loss: 0.8030 | Train Acc: 0.8573 | Val Loss: 0.9867 | Val Acc: 0.7613 | LR: 0.004841\n",
            "[Trial 13/40] Epoch   9/60 | Train Loss: 0.6791 | Train Acc: 0.9230 | Val Loss: 0.9795 | Val Acc: 0.7613 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  10/60 | Train Loss: 0.6138 | Train Acc: 0.9539 | Val Loss: 0.9059 | Val Acc: 0.8243 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  11/60 | Train Loss: 0.5511 | Train Acc: 0.9843 | Val Loss: 0.8137 | Val Acc: 0.8243 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  12/60 | Train Loss: 0.5416 | Train Acc: 0.9848 | Val Loss: 0.8147 | Val Acc: 0.8378 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  13/60 | Train Loss: 0.5158 | Train Acc: 0.9955 | Val Loss: 0.7759 | Val Acc: 0.8874 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  14/60 | Train Loss: 0.5637 | Train Acc: 0.9713 | Val Loss: 0.8297 | Val Acc: 0.8468 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  15/60 | Train Loss: 0.5527 | Train Acc: 0.9775 | Val Loss: 0.8031 | Val Acc: 0.8739 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  16/60 | Train Loss: 0.5769 | Train Acc: 0.9702 | Val Loss: 0.7940 | Val Acc: 0.8739 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  17/60 | Train Loss: 0.5382 | Train Acc: 0.9848 | Val Loss: 0.7698 | Val Acc: 0.8694 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  18/60 | Train Loss: 0.5391 | Train Acc: 0.9871 | Val Loss: 0.8290 | Val Acc: 0.8559 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  19/60 | Train Loss: 0.5536 | Train Acc: 0.9764 | Val Loss: 0.7589 | Val Acc: 0.8919 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  20/60 | Train Loss: 0.5252 | Train Acc: 0.9899 | Val Loss: 0.7711 | Val Acc: 0.8784 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  21/60 | Train Loss: 0.5095 | Train Acc: 0.9949 | Val Loss: 0.7877 | Val Acc: 0.8649 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  22/60 | Train Loss: 0.5098 | Train Acc: 0.9921 | Val Loss: 0.8203 | Val Acc: 0.8694 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  23/60 | Train Loss: 0.5371 | Train Acc: 0.9826 | Val Loss: 0.7886 | Val Acc: 0.8468 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  24/60 | Train Loss: 0.5500 | Train Acc: 0.9798 | Val Loss: 0.7821 | Val Acc: 0.8694 | LR: 0.004841\n",
            "[Trial 13/40] Epoch  25/60 | Train Loss: 0.5688 | Train Acc: 0.9747 | Val Loss: 0.8280 | Val Acc: 0.8468 | LR: 0.002421 (LR reduced from 0.004841 to 0.002421)\n",
            "[Trial 13/40] Epoch  26/60 | Train Loss: 0.5210 | Train Acc: 0.9893 | Val Loss: 0.7515 | Val Acc: 0.8784 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  27/60 | Train Loss: 0.5022 | Train Acc: 0.9961 | Val Loss: 0.7671 | Val Acc: 0.8739 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  28/60 | Train Loss: 0.4959 | Train Acc: 0.9978 | Val Loss: 0.7510 | Val Acc: 0.8829 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  29/60 | Train Loss: 0.4912 | Train Acc: 0.9989 | Val Loss: 0.7511 | Val Acc: 0.8829 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  30/60 | Train Loss: 0.4943 | Train Acc: 0.9972 | Val Loss: 0.7650 | Val Acc: 0.8829 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  31/60 | Train Loss: 0.4934 | Train Acc: 0.9972 | Val Loss: 0.7959 | Val Acc: 0.8604 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  32/60 | Train Loss: 0.5061 | Train Acc: 0.9916 | Val Loss: 0.7442 | Val Acc: 0.8919 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  33/60 | Train Loss: 0.5018 | Train Acc: 0.9933 | Val Loss: 0.7654 | Val Acc: 0.8739 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  34/60 | Train Loss: 0.5053 | Train Acc: 0.9933 | Val Loss: 0.7561 | Val Acc: 0.8829 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  35/60 | Train Loss: 0.5007 | Train Acc: 0.9944 | Val Loss: 0.7606 | Val Acc: 0.8874 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  36/60 | Train Loss: 0.4971 | Train Acc: 0.9949 | Val Loss: 0.7245 | Val Acc: 0.8919 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  37/60 | Train Loss: 0.4926 | Train Acc: 0.9978 | Val Loss: 0.7165 | Val Acc: 0.9009 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  38/60 | Train Loss: 0.4875 | Train Acc: 0.9994 | Val Loss: 0.7171 | Val Acc: 0.9099 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  39/60 | Train Loss: 0.4891 | Train Acc: 0.9983 | Val Loss: 0.7626 | Val Acc: 0.8874 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  40/60 | Train Loss: 0.4989 | Train Acc: 0.9938 | Val Loss: 0.8348 | Val Acc: 0.8468 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  41/60 | Train Loss: 0.5123 | Train Acc: 0.9899 | Val Loss: 0.7596 | Val Acc: 0.8829 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  42/60 | Train Loss: 0.5106 | Train Acc: 0.9904 | Val Loss: 0.7172 | Val Acc: 0.9144 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  43/60 | Train Loss: 0.4974 | Train Acc: 0.9949 | Val Loss: 0.7123 | Val Acc: 0.8964 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  44/60 | Train Loss: 0.4995 | Train Acc: 0.9955 | Val Loss: 0.7479 | Val Acc: 0.8829 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  45/60 | Train Loss: 0.5068 | Train Acc: 0.9910 | Val Loss: 0.7447 | Val Acc: 0.8829 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  46/60 | Train Loss: 0.4951 | Train Acc: 0.9955 | Val Loss: 0.7202 | Val Acc: 0.9009 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  47/60 | Train Loss: 0.4884 | Train Acc: 0.9983 | Val Loss: 0.7692 | Val Acc: 0.8964 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  48/60 | Train Loss: 0.4852 | Train Acc: 0.9994 | Val Loss: 0.7565 | Val Acc: 0.8919 | LR: 0.002421\n",
            "[Trial 13/40] Epoch  49/60 | Train Loss: 0.4852 | Train Acc: 0.9994 | Val Loss: 0.7562 | Val Acc: 0.8919 | LR: 0.001210 (LR reduced from 0.002421 to 0.001210)\n",
            "[Trial 13/40] Epoch  50/60 | Train Loss: 0.4842 | Train Acc: 0.9994 | Val Loss: 0.7588 | Val Acc: 0.8919 | LR: 0.001210\n",
            "[Trial 13/40] Epoch  51/60 | Train Loss: 0.4837 | Train Acc: 0.9994 | Val Loss: 0.7594 | Val Acc: 0.8919 | LR: 0.001210\n",
            "[Trial 13/40] Epoch  52/60 | Train Loss: 0.4834 | Train Acc: 0.9994 | Val Loss: 0.7595 | Val Acc: 0.8919 | LR: 0.001210\n",
            "[Trial 13/40] Epoch  53/60 | Train Loss: 0.4838 | Train Acc: 0.9994 | Val Loss: 0.7597 | Val Acc: 0.8874 | LR: 0.001210\n",
            "[Trial 13/40] [EarlyStopping] triggered at epoch 53\n",
            "[Trial 13/40] [Info] Loaded best model weights\n",
            "[Trial 13/40] Done | Val Loss: 0.7123 | Val Acc: 0.8964\n",
            "[I 2025-11-10 14:37:42,525] Trial 12 finished with value: 0.8963963963963963 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.34957689839330824, 'batch_size': 24, 'learning_rate': 0.004841492789520561, 'weight_decay': 0.00014110735246739424, 'label_smoothing': 0.13020316063431892, 'tune_epochs': 60}. Best is trial 12 with value: 0.8963963963963963.\n",
            "[Optuna] Completed Trial 13/40 | Value: 0.8964 | Best so far: 0.8964\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 14/40] START\n",
            "[Trial 14/40] Params: embedding_dim=128, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.38, batch_size=24, lr=0.004752, weight_decay=1.8e-04, label_smoothing=0.14, tune_epochs=60\n",
            "[Trial 14/40] [Train] epochs=60, batch_size=24, lr=0.004752, patience=10\n",
            "[Trial 14/40] Epoch   1/60 | Train Loss: 1.8025 | Train Acc: 0.2039 | Val Loss: 1.6766 | Val Acc: 0.2252 | LR: 0.004752\n",
            "[Trial 14/40] Epoch   2/60 | Train Loss: 1.6705 | Train Acc: 0.2809 | Val Loss: 1.7016 | Val Acc: 0.2387 | LR: 0.004752\n",
            "[Trial 14/40] Epoch   3/60 | Train Loss: 1.7243 | Train Acc: 0.2758 | Val Loss: 1.8517 | Val Acc: 0.2117 | LR: 0.004752\n",
            "[Trial 14/40] Epoch   4/60 | Train Loss: 1.6312 | Train Acc: 0.3416 | Val Loss: 1.9160 | Val Acc: 0.2523 | LR: 0.004752\n",
            "[Trial 14/40] Epoch   5/60 | Train Loss: 1.5501 | Train Acc: 0.4416 | Val Loss: 1.8196 | Val Acc: 0.2432 | LR: 0.004752\n",
            "[Trial 14/40] Epoch   6/60 | Train Loss: 1.4043 | Train Acc: 0.5292 | Val Loss: 1.6353 | Val Acc: 0.3694 | LR: 0.004752\n",
            "[Trial 14/40] Epoch   7/60 | Train Loss: 1.1809 | Train Acc: 0.6461 | Val Loss: 1.2021 | Val Acc: 0.6261 | LR: 0.004752\n",
            "[Trial 14/40] Epoch   8/60 | Train Loss: 1.0380 | Train Acc: 0.7225 | Val Loss: 1.1812 | Val Acc: 0.6486 | LR: 0.004752\n",
            "[Trial 14/40] Epoch   9/60 | Train Loss: 0.7933 | Train Acc: 0.8792 | Val Loss: 0.9489 | Val Acc: 0.7748 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  10/60 | Train Loss: 0.7272 | Train Acc: 0.9124 | Val Loss: 0.9393 | Val Acc: 0.8108 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  11/60 | Train Loss: 0.7090 | Train Acc: 0.9326 | Val Loss: 1.0534 | Val Acc: 0.7523 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  12/60 | Train Loss: 0.6490 | Train Acc: 0.9573 | Val Loss: 0.9090 | Val Acc: 0.8198 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  13/60 | Train Loss: 0.5995 | Train Acc: 0.9781 | Val Loss: 0.8849 | Val Acc: 0.8333 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  14/60 | Train Loss: 0.5867 | Train Acc: 0.9798 | Val Loss: 0.8571 | Val Acc: 0.8423 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  15/60 | Train Loss: 0.5772 | Train Acc: 0.9826 | Val Loss: 0.8799 | Val Acc: 0.8198 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  16/60 | Train Loss: 0.5846 | Train Acc: 0.9781 | Val Loss: 0.8766 | Val Acc: 0.8378 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  17/60 | Train Loss: 0.5672 | Train Acc: 0.9865 | Val Loss: 0.8791 | Val Acc: 0.8333 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  18/60 | Train Loss: 0.5713 | Train Acc: 0.9831 | Val Loss: 0.8589 | Val Acc: 0.8604 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  19/60 | Train Loss: 0.5622 | Train Acc: 0.9860 | Val Loss: 0.8735 | Val Acc: 0.8333 | LR: 0.004752\n",
            "[Trial 14/40] Epoch  20/60 | Train Loss: 0.5873 | Train Acc: 0.9787 | Val Loss: 0.8740 | Val Acc: 0.8468 | LR: 0.002376 (LR reduced from 0.004752 to 0.002376)\n",
            "[Trial 14/40] Epoch  21/60 | Train Loss: 0.5474 | Train Acc: 0.9938 | Val Loss: 0.8014 | Val Acc: 0.8739 | LR: 0.002376\n",
            "[Trial 14/40] Epoch  22/60 | Train Loss: 0.5322 | Train Acc: 0.9983 | Val Loss: 0.8169 | Val Acc: 0.8649 | LR: 0.002376\n",
            "[Trial 14/40] Epoch  23/60 | Train Loss: 0.5281 | Train Acc: 0.9972 | Val Loss: 0.8235 | Val Acc: 0.8739 | LR: 0.002376\n",
            "[Trial 14/40] Epoch  24/60 | Train Loss: 0.5315 | Train Acc: 0.9966 | Val Loss: 0.8388 | Val Acc: 0.8604 | LR: 0.002376\n",
            "[Trial 14/40] Epoch  25/60 | Train Loss: 0.5283 | Train Acc: 0.9966 | Val Loss: 0.8797 | Val Acc: 0.8378 | LR: 0.002376\n",
            "[Trial 14/40] Epoch  26/60 | Train Loss: 0.5267 | Train Acc: 0.9983 | Val Loss: 0.8505 | Val Acc: 0.8604 | LR: 0.002376\n",
            "[Trial 14/40] Epoch  27/60 | Train Loss: 0.5346 | Train Acc: 0.9938 | Val Loss: 0.8459 | Val Acc: 0.8559 | LR: 0.001188 (LR reduced from 0.002376 to 0.001188)\n",
            "[Trial 14/40] Epoch  28/60 | Train Loss: 0.5296 | Train Acc: 0.9966 | Val Loss: 0.8789 | Val Acc: 0.8378 | LR: 0.001188\n",
            "[Trial 14/40] Epoch  29/60 | Train Loss: 0.5245 | Train Acc: 0.9983 | Val Loss: 0.8900 | Val Acc: 0.8288 | LR: 0.001188\n",
            "[Trial 14/40] Epoch  30/60 | Train Loss: 0.5233 | Train Acc: 0.9989 | Val Loss: 0.8589 | Val Acc: 0.8514 | LR: 0.001188\n",
            "[Trial 14/40] Epoch  31/60 | Train Loss: 0.5222 | Train Acc: 0.9989 | Val Loss: 0.8371 | Val Acc: 0.8604 | LR: 0.001188\n",
            "[Trial 14/40] [EarlyStopping] triggered at epoch 31\n",
            "[Trial 14/40] [Info] Loaded best model weights\n",
            "[Trial 14/40] Done | Val Loss: 0.8014 | Val Acc: 0.8739\n",
            "[I 2025-11-10 14:56:16,623] Trial 13 finished with value: 0.8738738738738738 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.3800172816229794, 'batch_size': 24, 'learning_rate': 0.004752086119398767, 'weight_decay': 0.0001812134647149277, 'label_smoothing': 0.14221088765449696, 'tune_epochs': 60}. Best is trial 12 with value: 0.8963963963963963.\n",
            "[Optuna] Completed Trial 14/40 | Value: 0.8739 | Best so far: 0.8964\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 15/40] START\n",
            "[Trial 15/40] Params: embedding_dim=128, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.45, batch_size=24, lr=0.001969, weight_decay=1.5e-04, label_smoothing=0.12, tune_epochs=44\n",
            "[Trial 15/40] [Train] epochs=44, batch_size=24, lr=0.001969, patience=10\n",
            "[Trial 15/40] Epoch   1/44 | Train Loss: 1.7085 | Train Acc: 0.2174 | Val Loss: 1.6454 | Val Acc: 0.2432 | LR: 0.001969\n",
            "[Trial 15/40] Epoch   2/44 | Train Loss: 1.5586 | Train Acc: 0.3444 | Val Loss: 1.5977 | Val Acc: 0.3153 | LR: 0.001969\n",
            "[Trial 15/40] Epoch   3/44 | Train Loss: 1.4316 | Train Acc: 0.4303 | Val Loss: 1.5795 | Val Acc: 0.3468 | LR: 0.001969\n",
            "[Trial 15/40] Epoch   4/44 | Train Loss: 1.3153 | Train Acc: 0.5135 | Val Loss: 1.3964 | Val Acc: 0.4550 | LR: 0.001969\n",
            "[Trial 15/40] Epoch   5/44 | Train Loss: 1.0730 | Train Acc: 0.6646 | Val Loss: 1.3126 | Val Acc: 0.5901 | LR: 0.001969\n",
            "[Trial 15/40] Epoch   6/44 | Train Loss: 0.9441 | Train Acc: 0.7528 | Val Loss: 1.1699 | Val Acc: 0.6261 | LR: 0.001969\n",
            "[Trial 15/40] Epoch   7/44 | Train Loss: 0.7530 | Train Acc: 0.8573 | Val Loss: 1.1542 | Val Acc: 0.6892 | LR: 0.001969\n",
            "[Trial 15/40] Epoch   8/44 | Train Loss: 0.6375 | Train Acc: 0.9247 | Val Loss: 1.0192 | Val Acc: 0.7523 | LR: 0.001969\n",
            "[Trial 15/40] Epoch   9/44 | Train Loss: 0.5837 | Train Acc: 0.9539 | Val Loss: 0.9329 | Val Acc: 0.7568 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  10/44 | Train Loss: 0.5285 | Train Acc: 0.9826 | Val Loss: 0.9332 | Val Acc: 0.7793 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  11/44 | Train Loss: 0.5161 | Train Acc: 0.9809 | Val Loss: 0.9038 | Val Acc: 0.8063 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  12/44 | Train Loss: 0.5285 | Train Acc: 0.9770 | Val Loss: 0.9290 | Val Acc: 0.7973 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  13/44 | Train Loss: 0.5374 | Train Acc: 0.9713 | Val Loss: 0.9305 | Val Acc: 0.8018 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  14/44 | Train Loss: 0.5483 | Train Acc: 0.9674 | Val Loss: 0.9121 | Val Acc: 0.8063 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  15/44 | Train Loss: 0.4928 | Train Acc: 0.9927 | Val Loss: 0.8773 | Val Acc: 0.8243 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  16/44 | Train Loss: 0.4857 | Train Acc: 0.9933 | Val Loss: 0.8741 | Val Acc: 0.8243 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  17/44 | Train Loss: 0.4696 | Train Acc: 0.9972 | Val Loss: 0.8338 | Val Acc: 0.8288 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  18/44 | Train Loss: 0.4807 | Train Acc: 0.9904 | Val Loss: 0.8372 | Val Acc: 0.8333 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  19/44 | Train Loss: 0.4691 | Train Acc: 0.9955 | Val Loss: 0.8660 | Val Acc: 0.8243 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  20/44 | Train Loss: 0.4660 | Train Acc: 0.9983 | Val Loss: 0.8087 | Val Acc: 0.8514 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  21/44 | Train Loss: 0.5352 | Train Acc: 0.9713 | Val Loss: 0.8822 | Val Acc: 0.8063 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  22/44 | Train Loss: 0.4859 | Train Acc: 0.9882 | Val Loss: 0.8063 | Val Acc: 0.8423 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  23/44 | Train Loss: 0.5086 | Train Acc: 0.9792 | Val Loss: 0.8891 | Val Acc: 0.8108 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  24/44 | Train Loss: 0.5075 | Train Acc: 0.9815 | Val Loss: 0.8320 | Val Acc: 0.8468 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  25/44 | Train Loss: 0.4879 | Train Acc: 0.9871 | Val Loss: 0.8499 | Val Acc: 0.8243 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  26/44 | Train Loss: 0.4759 | Train Acc: 0.9921 | Val Loss: 0.7957 | Val Acc: 0.8468 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  27/44 | Train Loss: 0.4876 | Train Acc: 0.9871 | Val Loss: 0.8541 | Val Acc: 0.8333 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  28/44 | Train Loss: 0.4997 | Train Acc: 0.9803 | Val Loss: 0.9349 | Val Acc: 0.8063 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  29/44 | Train Loss: 0.5030 | Train Acc: 0.9820 | Val Loss: 0.7985 | Val Acc: 0.8604 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  30/44 | Train Loss: 0.4761 | Train Acc: 0.9927 | Val Loss: 0.7634 | Val Acc: 0.8694 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  31/44 | Train Loss: 0.4562 | Train Acc: 0.9994 | Val Loss: 0.8002 | Val Acc: 0.8649 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  32/44 | Train Loss: 0.4655 | Train Acc: 0.9949 | Val Loss: 0.8475 | Val Acc: 0.8288 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  33/44 | Train Loss: 0.4672 | Train Acc: 0.9949 | Val Loss: 0.8324 | Val Acc: 0.8604 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  34/44 | Train Loss: 0.4649 | Train Acc: 0.9966 | Val Loss: 0.7884 | Val Acc: 0.8739 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  35/44 | Train Loss: 0.4799 | Train Acc: 0.9910 | Val Loss: 0.8749 | Val Acc: 0.8108 | LR: 0.001969\n",
            "[Trial 15/40] Epoch  36/44 | Train Loss: 0.4744 | Train Acc: 0.9921 | Val Loss: 0.8188 | Val Acc: 0.8514 | LR: 0.000985 (LR reduced from 0.001969 to 0.000985)\n",
            "[Trial 15/40] Epoch  37/44 | Train Loss: 0.4683 | Train Acc: 0.9927 | Val Loss: 0.8087 | Val Acc: 0.8468 | LR: 0.000985\n",
            "[Trial 15/40] Epoch  38/44 | Train Loss: 0.4640 | Train Acc: 0.9955 | Val Loss: 0.8180 | Val Acc: 0.8514 | LR: 0.000985\n",
            "[Trial 15/40] Epoch  39/44 | Train Loss: 0.4527 | Train Acc: 0.9994 | Val Loss: 0.8189 | Val Acc: 0.8559 | LR: 0.000985\n",
            "[Trial 15/40] Epoch  40/44 | Train Loss: 0.4517 | Train Acc: 0.9994 | Val Loss: 0.8423 | Val Acc: 0.8514 | LR: 0.000985\n",
            "[Trial 15/40] [EarlyStopping] triggered at epoch 40\n",
            "[Trial 15/40] [Info] Loaded best model weights\n",
            "[Trial 15/40] Done | Val Loss: 0.7634 | Val Acc: 0.8694\n",
            "[I 2025-11-10 15:25:35,723] Trial 14 finished with value: 0.8693693693693694 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.4524635413007555, 'batch_size': 24, 'learning_rate': 0.0019693324238833827, 'weight_decay': 0.00015304693344945033, 'label_smoothing': 0.11733273277813562, 'tune_epochs': 44}. Best is trial 12 with value: 0.8963963963963963.\n",
            "[Optuna] Completed Trial 15/40 | Value: 0.8694 | Best so far: 0.8964\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 16/40] START\n",
            "[Trial 16/40] Params: embedding_dim=128, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.28, batch_size=16, lr=0.004840, weight_decay=2.3e-04, label_smoothing=0.15, tune_epochs=56\n",
            "[Trial 16/40] [Train] epochs=56, batch_size=16, lr=0.004840, patience=10\n",
            "[Trial 16/40] Epoch   1/56 | Train Loss: 1.7959 | Train Acc: 0.2062 | Val Loss: 1.6777 | Val Acc: 0.1982 | LR: 0.004840\n",
            "[Trial 16/40] Epoch   2/56 | Train Loss: 1.6812 | Train Acc: 0.2736 | Val Loss: 1.7813 | Val Acc: 0.2838 | LR: 0.004840\n",
            "[Trial 16/40] Epoch   3/56 | Train Loss: 1.6202 | Train Acc: 0.3635 | Val Loss: 1.6534 | Val Acc: 0.3514 | LR: 0.004840\n",
            "[Trial 16/40] Epoch   4/56 | Train Loss: 1.5103 | Train Acc: 0.4253 | Val Loss: 1.5840 | Val Acc: 0.3468 | LR: 0.004840\n",
            "[Trial 16/40] Epoch   5/56 | Train Loss: 1.4388 | Train Acc: 0.5017 | Val Loss: 1.7190 | Val Acc: 0.3468 | LR: 0.004840\n",
            "[Trial 16/40] Epoch   6/56 | Train Loss: 1.2765 | Train Acc: 0.5865 | Val Loss: 1.5304 | Val Acc: 0.4189 | LR: 0.004840\n",
            "[Trial 16/40] Epoch   7/56 | Train Loss: 1.1365 | Train Acc: 0.6753 | Val Loss: 1.4295 | Val Acc: 0.5045 | LR: 0.004840\n",
            "[Trial 16/40] Epoch   8/56 | Train Loss: 0.9420 | Train Acc: 0.7904 | Val Loss: 1.2392 | Val Acc: 0.6622 | LR: 0.004840\n",
            "[Trial 16/40] Epoch   9/56 | Train Loss: 0.7914 | Train Acc: 0.8882 | Val Loss: 1.1258 | Val Acc: 0.7117 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  10/56 | Train Loss: 0.6939 | Train Acc: 0.9433 | Val Loss: 0.9302 | Val Acc: 0.7973 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  11/56 | Train Loss: 0.6302 | Train Acc: 0.9770 | Val Loss: 0.9136 | Val Acc: 0.8018 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  12/56 | Train Loss: 0.6023 | Train Acc: 0.9831 | Val Loss: 0.9053 | Val Acc: 0.7928 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  13/56 | Train Loss: 0.6075 | Train Acc: 0.9798 | Val Loss: 0.9125 | Val Acc: 0.8153 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  14/56 | Train Loss: 0.6355 | Train Acc: 0.9618 | Val Loss: 0.9114 | Val Acc: 0.8559 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  15/56 | Train Loss: 0.6239 | Train Acc: 0.9719 | Val Loss: 0.8483 | Val Acc: 0.8514 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  16/56 | Train Loss: 0.6010 | Train Acc: 0.9826 | Val Loss: 0.9006 | Val Acc: 0.8468 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  17/56 | Train Loss: 0.6064 | Train Acc: 0.9781 | Val Loss: 0.8352 | Val Acc: 0.8739 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  18/56 | Train Loss: 0.6185 | Train Acc: 0.9753 | Val Loss: 0.8406 | Val Acc: 0.8378 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  19/56 | Train Loss: 0.6076 | Train Acc: 0.9764 | Val Loss: 0.8691 | Val Acc: 0.8559 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  20/56 | Train Loss: 0.6042 | Train Acc: 0.9831 | Val Loss: 0.8533 | Val Acc: 0.8559 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  21/56 | Train Loss: 0.6008 | Train Acc: 0.9815 | Val Loss: 0.8167 | Val Acc: 0.8604 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  22/56 | Train Loss: 0.5998 | Train Acc: 0.9809 | Val Loss: 0.8369 | Val Acc: 0.8514 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  23/56 | Train Loss: 0.5889 | Train Acc: 0.9871 | Val Loss: 0.8166 | Val Acc: 0.8784 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  24/56 | Train Loss: 0.5973 | Train Acc: 0.9831 | Val Loss: 0.8872 | Val Acc: 0.8378 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  25/56 | Train Loss: 0.5965 | Train Acc: 0.9860 | Val Loss: 0.8550 | Val Acc: 0.8559 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  26/56 | Train Loss: 0.5785 | Train Acc: 0.9921 | Val Loss: 0.8061 | Val Acc: 0.8874 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  27/56 | Train Loss: 0.5745 | Train Acc: 0.9904 | Val Loss: 0.8321 | Val Acc: 0.8559 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  28/56 | Train Loss: 0.5737 | Train Acc: 0.9916 | Val Loss: 0.8125 | Val Acc: 0.8694 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  29/56 | Train Loss: 0.5888 | Train Acc: 0.9843 | Val Loss: 0.8283 | Val Acc: 0.8739 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  30/56 | Train Loss: 0.6029 | Train Acc: 0.9781 | Val Loss: 0.7953 | Val Acc: 0.8829 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  31/56 | Train Loss: 0.5957 | Train Acc: 0.9848 | Val Loss: 0.7759 | Val Acc: 0.8739 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  32/56 | Train Loss: 0.5780 | Train Acc: 0.9888 | Val Loss: 0.8111 | Val Acc: 0.8649 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  33/56 | Train Loss: 0.5621 | Train Acc: 0.9972 | Val Loss: 0.7837 | Val Acc: 0.8919 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  34/56 | Train Loss: 0.5587 | Train Acc: 0.9972 | Val Loss: 0.8829 | Val Acc: 0.8378 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  35/56 | Train Loss: 0.5991 | Train Acc: 0.9787 | Val Loss: 0.9270 | Val Acc: 0.8333 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  36/56 | Train Loss: 0.6056 | Train Acc: 0.9781 | Val Loss: 0.8232 | Val Acc: 0.8739 | LR: 0.004840\n",
            "[Trial 16/40] Epoch  37/56 | Train Loss: 0.5926 | Train Acc: 0.9848 | Val Loss: 0.8773 | Val Acc: 0.8378 | LR: 0.002420 (LR reduced from 0.004840 to 0.002420)\n",
            "[Trial 16/40] Epoch  38/56 | Train Loss: 0.5745 | Train Acc: 0.9910 | Val Loss: 0.7839 | Val Acc: 0.8829 | LR: 0.002420\n",
            "[Trial 16/40] Epoch  39/56 | Train Loss: 0.5541 | Train Acc: 0.9994 | Val Loss: 0.7977 | Val Acc: 0.8784 | LR: 0.002420\n",
            "[Trial 16/40] Epoch  40/56 | Train Loss: 0.5520 | Train Acc: 0.9989 | Val Loss: 0.7904 | Val Acc: 0.8919 | LR: 0.002420\n",
            "[Trial 16/40] Epoch  41/56 | Train Loss: 0.5490 | Train Acc: 1.0000 | Val Loss: 0.7830 | Val Acc: 0.8919 | LR: 0.002420\n",
            "[Trial 16/40] [EarlyStopping] triggered at epoch 41\n",
            "[Trial 16/40] [Info] Loaded best model weights\n",
            "[Trial 16/40] Done | Val Loss: 0.7759 | Val Acc: 0.8739\n",
            "[I 2025-11-10 16:05:26,135] Trial 15 finished with value: 0.8738738738738738 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2771014306216843, 'batch_size': 16, 'learning_rate': 0.004840277716794806, 'weight_decay': 0.00022682888649614832, 'label_smoothing': 0.1527494789829706, 'tune_epochs': 56}. Best is trial 12 with value: 0.8963963963963963.\n",
            "[Optuna] Completed Trial 16/40 | Value: 0.8739 | Best so far: 0.8964\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 17/40] START\n",
            "[Trial 17/40] Params: embedding_dim=128, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.43, batch_size=24, lr=0.001887, weight_decay=8.5e-05, label_smoothing=0.09, tune_epochs=43\n",
            "[Trial 17/40] [Train] epochs=43, batch_size=24, lr=0.001887, patience=10\n",
            "[Trial 17/40] Epoch   1/43 | Train Loss: 1.7050 | Train Acc: 0.2438 | Val Loss: 1.6677 | Val Acc: 0.2252 | LR: 0.001887\n",
            "[Trial 17/40] Epoch   2/43 | Train Loss: 1.5461 | Train Acc: 0.3433 | Val Loss: 1.5571 | Val Acc: 0.3288 | LR: 0.001887\n",
            "[Trial 17/40] Epoch   3/43 | Train Loss: 1.3206 | Train Acc: 0.5073 | Val Loss: 1.4183 | Val Acc: 0.4550 | LR: 0.001887\n",
            "[Trial 17/40] Epoch   4/43 | Train Loss: 1.0979 | Train Acc: 0.6393 | Val Loss: 1.2859 | Val Acc: 0.5631 | LR: 0.001887\n",
            "[Trial 17/40] Epoch   5/43 | Train Loss: 0.9377 | Train Acc: 0.7219 | Val Loss: 1.2947 | Val Acc: 0.5495 | LR: 0.001887\n",
            "[Trial 17/40] Epoch   6/43 | Train Loss: 0.7512 | Train Acc: 0.8264 | Val Loss: 1.0932 | Val Acc: 0.6847 | LR: 0.001887\n",
            "[Trial 17/40] Epoch   7/43 | Train Loss: 0.6094 | Train Acc: 0.8989 | Val Loss: 0.9904 | Val Acc: 0.7748 | LR: 0.001887\n",
            "[Trial 17/40] Epoch   8/43 | Train Loss: 0.5061 | Train Acc: 0.9483 | Val Loss: 0.9463 | Val Acc: 0.7523 | LR: 0.001887\n",
            "[Trial 17/40] Epoch   9/43 | Train Loss: 0.4586 | Train Acc: 0.9652 | Val Loss: 1.0321 | Val Acc: 0.7432 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  10/43 | Train Loss: 0.4317 | Train Acc: 0.9815 | Val Loss: 0.9045 | Val Acc: 0.7928 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  11/43 | Train Loss: 0.4081 | Train Acc: 0.9876 | Val Loss: 0.8805 | Val Acc: 0.8153 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  12/43 | Train Loss: 0.4556 | Train Acc: 0.9702 | Val Loss: 0.9459 | Val Acc: 0.7838 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  13/43 | Train Loss: 0.4212 | Train Acc: 0.9848 | Val Loss: 0.9412 | Val Acc: 0.7838 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  14/43 | Train Loss: 0.3917 | Train Acc: 0.9949 | Val Loss: 0.8630 | Val Acc: 0.8018 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  15/43 | Train Loss: 0.3890 | Train Acc: 0.9927 | Val Loss: 0.8607 | Val Acc: 0.8153 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  16/43 | Train Loss: 0.3832 | Train Acc: 0.9955 | Val Loss: 0.8300 | Val Acc: 0.8018 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  17/43 | Train Loss: 0.3701 | Train Acc: 0.9989 | Val Loss: 0.8412 | Val Acc: 0.8153 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  18/43 | Train Loss: 0.3663 | Train Acc: 1.0000 | Val Loss: 0.7644 | Val Acc: 0.8378 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  19/43 | Train Loss: 0.3665 | Train Acc: 0.9983 | Val Loss: 0.7990 | Val Acc: 0.8288 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  20/43 | Train Loss: 0.4304 | Train Acc: 0.9747 | Val Loss: 0.8580 | Val Acc: 0.8153 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  21/43 | Train Loss: 0.4274 | Train Acc: 0.9781 | Val Loss: 0.8891 | Val Acc: 0.7928 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  22/43 | Train Loss: 0.4129 | Train Acc: 0.9831 | Val Loss: 0.9944 | Val Acc: 0.7658 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  23/43 | Train Loss: 0.4360 | Train Acc: 0.9758 | Val Loss: 0.7720 | Val Acc: 0.8378 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  24/43 | Train Loss: 0.3747 | Train Acc: 0.9978 | Val Loss: 0.7474 | Val Acc: 0.8514 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  25/43 | Train Loss: 0.3668 | Train Acc: 0.9994 | Val Loss: 0.7322 | Val Acc: 0.8468 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  26/43 | Train Loss: 0.3651 | Train Acc: 0.9989 | Val Loss: 0.7431 | Val Acc: 0.8378 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  27/43 | Train Loss: 0.3750 | Train Acc: 0.9944 | Val Loss: 0.7615 | Val Acc: 0.8468 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  28/43 | Train Loss: 0.4061 | Train Acc: 0.9848 | Val Loss: 0.7115 | Val Acc: 0.8423 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  29/43 | Train Loss: 0.4003 | Train Acc: 0.9865 | Val Loss: 0.7622 | Val Acc: 0.8378 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  30/43 | Train Loss: 0.3859 | Train Acc: 0.9938 | Val Loss: 0.6727 | Val Acc: 0.8694 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  31/43 | Train Loss: 0.3705 | Train Acc: 0.9966 | Val Loss: 0.6616 | Val Acc: 0.8784 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  32/43 | Train Loss: 0.3656 | Train Acc: 0.9994 | Val Loss: 0.6506 | Val Acc: 0.8874 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  33/43 | Train Loss: 0.3610 | Train Acc: 1.0000 | Val Loss: 0.6777 | Val Acc: 0.8874 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  34/43 | Train Loss: 0.3723 | Train Acc: 0.9961 | Val Loss: 0.7068 | Val Acc: 0.8694 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  35/43 | Train Loss: 0.3664 | Train Acc: 0.9994 | Val Loss: 0.7207 | Val Acc: 0.8649 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  36/43 | Train Loss: 0.3622 | Train Acc: 0.9994 | Val Loss: 0.7224 | Val Acc: 0.8694 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  37/43 | Train Loss: 0.3786 | Train Acc: 0.9955 | Val Loss: 0.7521 | Val Acc: 0.8514 | LR: 0.001887\n",
            "[Trial 17/40] Epoch  38/43 | Train Loss: 0.3948 | Train Acc: 0.9888 | Val Loss: 0.8014 | Val Acc: 0.8559 | LR: 0.000944 (LR reduced from 0.001887 to 0.000944)\n",
            "[Trial 17/40] Epoch  39/43 | Train Loss: 0.3878 | Train Acc: 0.9927 | Val Loss: 0.7421 | Val Acc: 0.8559 | LR: 0.000944\n",
            "[Trial 17/40] Epoch  40/43 | Train Loss: 0.3676 | Train Acc: 0.9983 | Val Loss: 0.7137 | Val Acc: 0.8649 | LR: 0.000944\n",
            "[Trial 17/40] Epoch  41/43 | Train Loss: 0.3632 | Train Acc: 0.9983 | Val Loss: 0.7334 | Val Acc: 0.8514 | LR: 0.000944\n",
            "[Trial 17/40] Epoch  42/43 | Train Loss: 0.3622 | Train Acc: 0.9989 | Val Loss: 0.7371 | Val Acc: 0.8604 | LR: 0.000944\n",
            "[Trial 17/40] [EarlyStopping] triggered at epoch 42\n",
            "[Trial 17/40] [Info] Loaded best model weights\n",
            "[Trial 17/40] Done | Val Loss: 0.6506 | Val Acc: 0.8874\n",
            "[I 2025-11-10 16:37:13,819] Trial 16 finished with value: 0.8873873873873874 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.42544025666377844, 'batch_size': 24, 'learning_rate': 0.0018871577381807001, 'weight_decay': 8.474770713956048e-05, 'label_smoothing': 0.08721996671445602, 'tune_epochs': 43}. Best is trial 12 with value: 0.8963963963963963.\n",
            "[Optuna] Completed Trial 17/40 | Value: 0.8874 | Best so far: 0.8964\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 18/40] START\n",
            "[Trial 18/40] Params: embedding_dim=128, hidden_size=192, num_layers=2, bidirectional=True, dropout=0.27, batch_size=24, lr=0.002821, weight_decay=3.4e-04, label_smoothing=0.13, tune_epochs=57\n",
            "[Trial 18/40] [Train] epochs=57, batch_size=24, lr=0.002821, patience=10\n",
            "[Trial 18/40] Epoch   1/57 | Train Loss: 1.7753 | Train Acc: 0.2169 | Val Loss: 1.6077 | Val Acc: 0.2883 | LR: 0.002821\n",
            "[Trial 18/40] Epoch   2/57 | Train Loss: 1.5988 | Train Acc: 0.3213 | Val Loss: 1.6546 | Val Acc: 0.2432 | LR: 0.002821\n",
            "[Trial 18/40] Epoch   3/57 | Train Loss: 1.5217 | Train Acc: 0.3713 | Val Loss: 1.5813 | Val Acc: 0.3784 | LR: 0.002821\n",
            "[Trial 18/40] Epoch   4/57 | Train Loss: 1.3443 | Train Acc: 0.5073 | Val Loss: 1.4164 | Val Acc: 0.4369 | LR: 0.002821\n",
            "[Trial 18/40] Epoch   5/57 | Train Loss: 1.0871 | Train Acc: 0.6691 | Val Loss: 1.2501 | Val Acc: 0.6712 | LR: 0.002821\n",
            "[Trial 18/40] Epoch   6/57 | Train Loss: 0.9342 | Train Acc: 0.7719 | Val Loss: 1.2143 | Val Acc: 0.6667 | LR: 0.002821\n",
            "[Trial 18/40] Epoch   7/57 | Train Loss: 0.7529 | Train Acc: 0.8815 | Val Loss: 0.9300 | Val Acc: 0.8063 | LR: 0.002821\n",
            "[Trial 18/40] Epoch   8/57 | Train Loss: 0.6470 | Train Acc: 0.9376 | Val Loss: 0.9178 | Val Acc: 0.7973 | LR: 0.002821\n",
            "[Trial 18/40] Epoch   9/57 | Train Loss: 0.6039 | Train Acc: 0.9500 | Val Loss: 0.8823 | Val Acc: 0.8153 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  10/57 | Train Loss: 0.5606 | Train Acc: 0.9730 | Val Loss: 0.8544 | Val Acc: 0.8514 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  11/57 | Train Loss: 0.5384 | Train Acc: 0.9820 | Val Loss: 0.8138 | Val Acc: 0.8604 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  12/57 | Train Loss: 0.5273 | Train Acc: 0.9848 | Val Loss: 0.7997 | Val Acc: 0.8649 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  13/57 | Train Loss: 0.5242 | Train Acc: 0.9865 | Val Loss: 0.8357 | Val Acc: 0.8649 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  14/57 | Train Loss: 0.5337 | Train Acc: 0.9826 | Val Loss: 0.7565 | Val Acc: 0.8784 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  15/57 | Train Loss: 0.5229 | Train Acc: 0.9899 | Val Loss: 0.7822 | Val Acc: 0.8468 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  16/57 | Train Loss: 0.5292 | Train Acc: 0.9854 | Val Loss: 0.7822 | Val Acc: 0.8739 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  17/57 | Train Loss: 0.5216 | Train Acc: 0.9837 | Val Loss: 0.9223 | Val Acc: 0.8018 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  18/57 | Train Loss: 0.5380 | Train Acc: 0.9815 | Val Loss: 0.7937 | Val Acc: 0.8514 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  19/57 | Train Loss: 0.5065 | Train Acc: 0.9933 | Val Loss: 0.7787 | Val Acc: 0.8559 | LR: 0.002821\n",
            "[Trial 18/40] Epoch  20/57 | Train Loss: 0.5259 | Train Acc: 0.9815 | Val Loss: 0.8250 | Val Acc: 0.8559 | LR: 0.001410 (LR reduced from 0.002821 to 0.001410)\n",
            "[Trial 18/40] Epoch  21/57 | Train Loss: 0.5065 | Train Acc: 0.9938 | Val Loss: 0.8166 | Val Acc: 0.8333 | LR: 0.001410\n",
            "[Trial 18/40] Epoch  22/57 | Train Loss: 0.4895 | Train Acc: 0.9983 | Val Loss: 0.7805 | Val Acc: 0.8694 | LR: 0.001410\n",
            "[Trial 18/40] Epoch  23/57 | Train Loss: 0.4857 | Train Acc: 0.9994 | Val Loss: 0.8233 | Val Acc: 0.8514 | LR: 0.001410\n",
            "[Trial 18/40] Epoch  24/57 | Train Loss: 0.4860 | Train Acc: 0.9989 | Val Loss: 0.8082 | Val Acc: 0.8468 | LR: 0.001410\n",
            "[Trial 18/40] [EarlyStopping] triggered at epoch 24\n",
            "[Trial 18/40] [Info] Loaded best model weights\n",
            "[Trial 18/40] Done | Val Loss: 0.7565 | Val Acc: 0.8784\n",
            "[I 2025-11-10 16:57:36,676] Trial 17 finished with value: 0.8783783783783784 and parameters: {'embedding_dim': 128, 'hidden_size': 192, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.2671876770805229, 'batch_size': 24, 'learning_rate': 0.00282068874272823, 'weight_decay': 0.00033705014454215204, 'label_smoothing': 0.1293079460109771, 'tune_epochs': 57}. Best is trial 12 with value: 0.8963963963963963.\n",
            "[Optuna] Completed Trial 18/40 | Value: 0.8784 | Best so far: 0.8964\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 19/40] START\n",
            "[Trial 19/40] Params: embedding_dim=128, hidden_size=160, num_layers=2, bidirectional=True, dropout=0.32, batch_size=24, lr=0.001142, weight_decay=7.9e-05, label_smoothing=0.20, tune_epochs=50\n",
            "[Trial 19/40] [Train] epochs=50, batch_size=24, lr=0.001142, patience=10\n",
            "[Trial 19/40] Epoch   1/50 | Train Loss: 1.6793 | Train Acc: 0.2331 | Val Loss: 1.6493 | Val Acc: 0.2928 | LR: 0.001142\n",
            "[Trial 19/40] Epoch   2/50 | Train Loss: 1.5509 | Train Acc: 0.3534 | Val Loss: 1.6521 | Val Acc: 0.2523 | LR: 0.001142\n",
            "[Trial 19/40] Epoch   3/50 | Train Loss: 1.4360 | Train Acc: 0.4697 | Val Loss: 1.6083 | Val Acc: 0.3649 | LR: 0.001142\n",
            "[Trial 19/40] Epoch   4/50 | Train Loss: 1.2616 | Train Acc: 0.6096 | Val Loss: 1.4138 | Val Acc: 0.5090 | LR: 0.001142\n",
            "[Trial 19/40] Epoch   5/50 | Train Loss: 1.1051 | Train Acc: 0.7320 | Val Loss: 1.2693 | Val Acc: 0.6396 | LR: 0.001142\n",
            "[Trial 19/40] Epoch   6/50 | Train Loss: 0.9555 | Train Acc: 0.8410 | Val Loss: 1.1996 | Val Acc: 0.7162 | LR: 0.001142\n",
            "[Trial 19/40] Epoch   7/50 | Train Loss: 0.8541 | Train Acc: 0.9101 | Val Loss: 1.2033 | Val Acc: 0.6802 | LR: 0.001142\n",
            "[Trial 19/40] Epoch   8/50 | Train Loss: 0.8116 | Train Acc: 0.9337 | Val Loss: 1.1027 | Val Acc: 0.7568 | LR: 0.001142\n",
            "[Trial 19/40] Epoch   9/50 | Train Loss: 0.7568 | Train Acc: 0.9618 | Val Loss: 1.1617 | Val Acc: 0.7387 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  10/50 | Train Loss: 0.7383 | Train Acc: 0.9719 | Val Loss: 1.2066 | Val Acc: 0.7207 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  11/50 | Train Loss: 0.7418 | Train Acc: 0.9669 | Val Loss: 1.1222 | Val Acc: 0.7297 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  12/50 | Train Loss: 0.7217 | Train Acc: 0.9792 | Val Loss: 1.1180 | Val Acc: 0.7568 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  13/50 | Train Loss: 0.7326 | Train Acc: 0.9697 | Val Loss: 1.1629 | Val Acc: 0.7342 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  14/50 | Train Loss: 0.7290 | Train Acc: 0.9747 | Val Loss: 1.0815 | Val Acc: 0.7613 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  15/50 | Train Loss: 0.7125 | Train Acc: 0.9820 | Val Loss: 1.1126 | Val Acc: 0.7793 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  16/50 | Train Loss: 0.7072 | Train Acc: 0.9843 | Val Loss: 1.0771 | Val Acc: 0.7613 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  17/50 | Train Loss: 0.6948 | Train Acc: 0.9916 | Val Loss: 1.0428 | Val Acc: 0.7973 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  18/50 | Train Loss: 0.6943 | Train Acc: 0.9888 | Val Loss: 1.0610 | Val Acc: 0.7838 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  19/50 | Train Loss: 0.7021 | Train Acc: 0.9865 | Val Loss: 1.0689 | Val Acc: 0.7748 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  20/50 | Train Loss: 0.7097 | Train Acc: 0.9809 | Val Loss: 1.0852 | Val Acc: 0.7838 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  21/50 | Train Loss: 0.6914 | Train Acc: 0.9899 | Val Loss: 1.0572 | Val Acc: 0.7838 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  22/50 | Train Loss: 0.6827 | Train Acc: 0.9944 | Val Loss: 1.0598 | Val Acc: 0.7793 | LR: 0.001142\n",
            "[Trial 19/40] Epoch  23/50 | Train Loss: 0.6749 | Train Acc: 0.9978 | Val Loss: 1.0891 | Val Acc: 0.7703 | LR: 0.000571 (LR reduced from 0.001142 to 0.000571)\n",
            "[Trial 19/40] Epoch  24/50 | Train Loss: 0.6702 | Train Acc: 0.9994 | Val Loss: 1.0490 | Val Acc: 0.8018 | LR: 0.000571\n",
            "[Trial 19/40] Epoch  25/50 | Train Loss: 0.6686 | Train Acc: 1.0000 | Val Loss: 1.0471 | Val Acc: 0.8063 | LR: 0.000571\n",
            "[Trial 19/40] Epoch  26/50 | Train Loss: 0.6680 | Train Acc: 0.9994 | Val Loss: 1.0534 | Val Acc: 0.8108 | LR: 0.000571\n",
            "[Trial 19/40] Epoch  27/50 | Train Loss: 0.6669 | Train Acc: 1.0000 | Val Loss: 1.0620 | Val Acc: 0.8018 | LR: 0.000571\n",
            "[Trial 19/40] [EarlyStopping] triggered at epoch 27\n",
            "[Trial 19/40] [Info] Loaded best model weights\n",
            "[Trial 19/40] Done | Val Loss: 1.0428 | Val Acc: 0.7973\n",
            "[I 2025-11-10 17:17:06,218] Trial 18 finished with value: 0.7972972972972973 and parameters: {'embedding_dim': 128, 'hidden_size': 160, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.3159288929998385, 'batch_size': 24, 'learning_rate': 0.0011419924759240275, 'weight_decay': 7.904805314839799e-05, 'label_smoothing': 0.199323347426894, 'tune_epochs': 50}. Best is trial 12 with value: 0.8963963963963963.\n",
            "[Optuna] Completed Trial 19/40 | Value: 0.7973 | Best so far: 0.8964\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 20/40] START\n",
            "[Trial 20/40] Params: embedding_dim=128, hidden_size=128, num_layers=4, bidirectional=True, dropout=0.45, batch_size=32, lr=0.000520, weight_decay=9.4e-05, label_smoothing=0.09, tune_epochs=38\n",
            "[Trial 20/40] [Train] epochs=38, batch_size=32, lr=0.000520, patience=10\n",
            "[Trial 20/40] Epoch   1/38 | Train Loss: 1.6403 | Train Acc: 0.2017 | Val Loss: 1.6136 | Val Acc: 0.2523 | LR: 0.000520\n",
            "[Trial 20/40] Epoch   2/38 | Train Loss: 1.6063 | Train Acc: 0.2489 | Val Loss: 1.6057 | Val Acc: 0.1802 | LR: 0.000520\n",
            "[Trial 20/40] Epoch   3/38 | Train Loss: 1.5793 | Train Acc: 0.2775 | Val Loss: 1.5903 | Val Acc: 0.2838 | LR: 0.000520\n",
            "[Trial 20/40] Epoch   4/38 | Train Loss: 1.5412 | Train Acc: 0.3225 | Val Loss: 1.5459 | Val Acc: 0.2883 | LR: 0.000520\n",
            "[Trial 20/40] Epoch   5/38 | Train Loss: 1.4475 | Train Acc: 0.4045 | Val Loss: 1.5380 | Val Acc: 0.3604 | LR: 0.000520\n",
            "[Trial 20/40] Epoch   6/38 | Train Loss: 1.2684 | Train Acc: 0.5230 | Val Loss: 1.4981 | Val Acc: 0.4009 | LR: 0.000520\n",
            "[Trial 20/40] Epoch   7/38 | Train Loss: 1.1209 | Train Acc: 0.6163 | Val Loss: 1.3584 | Val Acc: 0.4820 | LR: 0.000520\n",
            "[Trial 20/40] Epoch   8/38 | Train Loss: 0.9200 | Train Acc: 0.7354 | Val Loss: 1.2711 | Val Acc: 0.5541 | LR: 0.000520\n",
            "[Trial 20/40] Epoch   9/38 | Train Loss: 0.7622 | Train Acc: 0.8298 | Val Loss: 1.2520 | Val Acc: 0.6126 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  10/38 | Train Loss: 0.6667 | Train Acc: 0.8742 | Val Loss: 1.2394 | Val Acc: 0.6757 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  11/38 | Train Loss: 0.5895 | Train Acc: 0.9101 | Val Loss: 1.1350 | Val Acc: 0.7072 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  12/38 | Train Loss: 0.5323 | Train Acc: 0.9393 | Val Loss: 1.0986 | Val Acc: 0.7207 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  13/38 | Train Loss: 0.5047 | Train Acc: 0.9522 | Val Loss: 1.1294 | Val Acc: 0.7252 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  14/38 | Train Loss: 0.4495 | Train Acc: 0.9775 | Val Loss: 1.0903 | Val Acc: 0.7342 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  15/38 | Train Loss: 0.4717 | Train Acc: 0.9663 | Val Loss: 1.1810 | Val Acc: 0.7252 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  16/38 | Train Loss: 0.4452 | Train Acc: 0.9742 | Val Loss: 1.1094 | Val Acc: 0.7162 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  17/38 | Train Loss: 0.4265 | Train Acc: 0.9876 | Val Loss: 1.1276 | Val Acc: 0.7297 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  18/38 | Train Loss: 0.4100 | Train Acc: 0.9910 | Val Loss: 1.1027 | Val Acc: 0.7523 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  19/38 | Train Loss: 0.4054 | Train Acc: 0.9916 | Val Loss: 1.1042 | Val Acc: 0.7658 | LR: 0.000520\n",
            "[Trial 20/40] Epoch  20/38 | Train Loss: 0.4040 | Train Acc: 0.9921 | Val Loss: 1.1271 | Val Acc: 0.7207 | LR: 0.000260 (LR reduced from 0.000520 to 0.000260)\n",
            "[Trial 20/40] Epoch  21/38 | Train Loss: 0.3927 | Train Acc: 0.9972 | Val Loss: 1.0863 | Val Acc: 0.7387 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  22/38 | Train Loss: 0.3871 | Train Acc: 0.9983 | Val Loss: 1.1098 | Val Acc: 0.7432 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  23/38 | Train Loss: 0.3849 | Train Acc: 0.9989 | Val Loss: 1.1035 | Val Acc: 0.7477 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  24/38 | Train Loss: 0.3841 | Train Acc: 0.9994 | Val Loss: 1.0852 | Val Acc: 0.7523 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  25/38 | Train Loss: 0.3855 | Train Acc: 0.9972 | Val Loss: 1.1327 | Val Acc: 0.7207 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  26/38 | Train Loss: 0.3833 | Train Acc: 0.9989 | Val Loss: 1.1305 | Val Acc: 0.7342 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  27/38 | Train Loss: 0.3807 | Train Acc: 1.0000 | Val Loss: 1.1014 | Val Acc: 0.7523 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  28/38 | Train Loss: 0.3802 | Train Acc: 1.0000 | Val Loss: 1.0985 | Val Acc: 0.7432 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  29/38 | Train Loss: 0.3835 | Train Acc: 0.9989 | Val Loss: 1.0691 | Val Acc: 0.7568 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  30/38 | Train Loss: 0.3797 | Train Acc: 1.0000 | Val Loss: 1.1199 | Val Acc: 0.7342 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  31/38 | Train Loss: 0.3804 | Train Acc: 0.9994 | Val Loss: 1.1109 | Val Acc: 0.7568 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  32/38 | Train Loss: 0.3795 | Train Acc: 0.9989 | Val Loss: 1.1452 | Val Acc: 0.7252 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  33/38 | Train Loss: 0.3828 | Train Acc: 0.9983 | Val Loss: 1.1086 | Val Acc: 0.7342 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  34/38 | Train Loss: 0.3888 | Train Acc: 0.9949 | Val Loss: 1.2317 | Val Acc: 0.7117 | LR: 0.000260\n",
            "[Trial 20/40] Epoch  35/38 | Train Loss: 0.3896 | Train Acc: 0.9949 | Val Loss: 1.1450 | Val Acc: 0.7568 | LR: 0.000130 (LR reduced from 0.000260 to 0.000130)\n",
            "[Trial 20/40] Epoch  36/38 | Train Loss: 0.3832 | Train Acc: 0.9983 | Val Loss: 1.1327 | Val Acc: 0.7342 | LR: 0.000130\n",
            "[Trial 20/40] Epoch  37/38 | Train Loss: 0.3793 | Train Acc: 0.9994 | Val Loss: 1.1293 | Val Acc: 0.7342 | LR: 0.000130\n",
            "[Trial 20/40] Epoch  38/38 | Train Loss: 0.3805 | Train Acc: 0.9989 | Val Loss: 1.1580 | Val Acc: 0.7252 | LR: 0.000130\n",
            "[Trial 20/40] [Info] Loaded best model weights\n",
            "[Trial 20/40] Done | Val Loss: 1.0691 | Val Acc: 0.7568\n",
            "[I 2025-11-10 17:46:54,717] Trial 19 finished with value: 0.7567567567567568 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 4, 'bidirectional': True, 'dropout': 0.4483325185331661, 'batch_size': 32, 'learning_rate': 0.0005202557916902402, 'weight_decay': 9.369678713229782e-05, 'label_smoothing': 0.09278522545389886, 'tune_epochs': 38}. Best is trial 12 with value: 0.8963963963963963.\n",
            "[Optuna] Completed Trial 20/40 | Value: 0.7568 | Best so far: 0.8964\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 21/40] START\n",
            "[Trial 21/40] Params: embedding_dim=128, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.23, batch_size=16, lr=0.001486, weight_decay=4.3e-04, label_smoothing=0.17, tune_epochs=52\n",
            "[Trial 21/40] [Train] epochs=52, batch_size=16, lr=0.001486, patience=10\n",
            "[Trial 21/40] Epoch   1/52 | Train Loss: 1.6713 | Train Acc: 0.2354 | Val Loss: 1.6459 | Val Acc: 0.2162 | LR: 0.001486\n",
            "[Trial 21/40] Epoch   2/52 | Train Loss: 1.5403 | Train Acc: 0.3494 | Val Loss: 1.5987 | Val Acc: 0.2703 | LR: 0.001486\n",
            "[Trial 21/40] Epoch   3/52 | Train Loss: 1.4479 | Train Acc: 0.4438 | Val Loss: 1.5482 | Val Acc: 0.3559 | LR: 0.001486\n",
            "[Trial 21/40] Epoch   4/52 | Train Loss: 1.3577 | Train Acc: 0.5107 | Val Loss: 1.4545 | Val Acc: 0.4099 | LR: 0.001486\n",
            "[Trial 21/40] Epoch   5/52 | Train Loss: 1.1584 | Train Acc: 0.6629 | Val Loss: 1.3623 | Val Acc: 0.5360 | LR: 0.001486\n",
            "[Trial 21/40] Epoch   6/52 | Train Loss: 1.0318 | Train Acc: 0.7478 | Val Loss: 1.2359 | Val Acc: 0.6126 | LR: 0.001486\n",
            "[Trial 21/40] Epoch   7/52 | Train Loss: 0.8855 | Train Acc: 0.8567 | Val Loss: 1.1712 | Val Acc: 0.6802 | LR: 0.001486\n",
            "[Trial 21/40] Epoch   8/52 | Train Loss: 0.7857 | Train Acc: 0.9084 | Val Loss: 1.0935 | Val Acc: 0.7207 | LR: 0.001486\n",
            "[Trial 21/40] Epoch   9/52 | Train Loss: 0.7257 | Train Acc: 0.9455 | Val Loss: 1.0453 | Val Acc: 0.7748 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  10/52 | Train Loss: 0.6864 | Train Acc: 0.9635 | Val Loss: 0.9763 | Val Acc: 0.8063 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  11/52 | Train Loss: 0.6616 | Train Acc: 0.9792 | Val Loss: 0.9859 | Val Acc: 0.7928 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  12/52 | Train Loss: 0.6699 | Train Acc: 0.9719 | Val Loss: 0.9526 | Val Acc: 0.8198 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  13/52 | Train Loss: 0.6694 | Train Acc: 0.9713 | Val Loss: 0.9671 | Val Acc: 0.7928 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  14/52 | Train Loss: 0.6626 | Train Acc: 0.9719 | Val Loss: 0.9325 | Val Acc: 0.8108 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  15/52 | Train Loss: 0.6310 | Train Acc: 0.9904 | Val Loss: 0.9389 | Val Acc: 0.8198 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  16/52 | Train Loss: 0.6362 | Train Acc: 0.9837 | Val Loss: 0.8976 | Val Acc: 0.8514 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  17/52 | Train Loss: 0.6409 | Train Acc: 0.9837 | Val Loss: 0.9431 | Val Acc: 0.8243 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  18/52 | Train Loss: 0.6507 | Train Acc: 0.9758 | Val Loss: 1.0194 | Val Acc: 0.8063 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  19/52 | Train Loss: 0.6578 | Train Acc: 0.9736 | Val Loss: 0.8849 | Val Acc: 0.8514 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  20/52 | Train Loss: 0.6266 | Train Acc: 0.9882 | Val Loss: 0.9123 | Val Acc: 0.8468 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  21/52 | Train Loss: 0.6251 | Train Acc: 0.9888 | Val Loss: 0.8830 | Val Acc: 0.8559 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  22/52 | Train Loss: 0.6425 | Train Acc: 0.9798 | Val Loss: 0.9036 | Val Acc: 0.8468 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  23/52 | Train Loss: 0.6387 | Train Acc: 0.9871 | Val Loss: 0.8807 | Val Acc: 0.8514 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  24/52 | Train Loss: 0.6258 | Train Acc: 0.9882 | Val Loss: 0.8757 | Val Acc: 0.8468 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  25/52 | Train Loss: 0.6186 | Train Acc: 0.9893 | Val Loss: 0.8522 | Val Acc: 0.8739 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  26/52 | Train Loss: 0.6282 | Train Acc: 0.9876 | Val Loss: 0.8815 | Val Acc: 0.8514 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  27/52 | Train Loss: 0.6177 | Train Acc: 0.9904 | Val Loss: 0.8861 | Val Acc: 0.8559 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  28/52 | Train Loss: 0.6320 | Train Acc: 0.9871 | Val Loss: 0.8539 | Val Acc: 0.8694 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  29/52 | Train Loss: 0.6180 | Train Acc: 0.9916 | Val Loss: 0.8704 | Val Acc: 0.8604 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  30/52 | Train Loss: 0.6142 | Train Acc: 0.9933 | Val Loss: 0.8989 | Val Acc: 0.8423 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  31/52 | Train Loss: 0.6235 | Train Acc: 0.9888 | Val Loss: 0.8336 | Val Acc: 0.8739 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  32/52 | Train Loss: 0.6102 | Train Acc: 0.9949 | Val Loss: 0.8511 | Val Acc: 0.8694 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  33/52 | Train Loss: 0.6353 | Train Acc: 0.9854 | Val Loss: 0.8961 | Val Acc: 0.8514 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  34/52 | Train Loss: 0.6335 | Train Acc: 0.9843 | Val Loss: 0.8746 | Val Acc: 0.8694 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  35/52 | Train Loss: 0.6415 | Train Acc: 0.9798 | Val Loss: 0.9130 | Val Acc: 0.8423 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  36/52 | Train Loss: 0.6182 | Train Acc: 0.9927 | Val Loss: 0.8949 | Val Acc: 0.8694 | LR: 0.001486\n",
            "[Trial 21/40] Epoch  37/52 | Train Loss: 0.6237 | Train Acc: 0.9893 | Val Loss: 0.9663 | Val Acc: 0.8153 | LR: 0.000743 (LR reduced from 0.001486 to 0.000743)\n",
            "[Trial 21/40] Epoch  38/52 | Train Loss: 0.6117 | Train Acc: 0.9944 | Val Loss: 0.8602 | Val Acc: 0.8604 | LR: 0.000743\n",
            "[Trial 21/40] Epoch  39/52 | Train Loss: 0.6009 | Train Acc: 0.9978 | Val Loss: 0.9013 | Val Acc: 0.8423 | LR: 0.000743\n",
            "[Trial 21/40] Epoch  40/52 | Train Loss: 0.5974 | Train Acc: 1.0000 | Val Loss: 0.8825 | Val Acc: 0.8514 | LR: 0.000743\n",
            "[Trial 21/40] Epoch  41/52 | Train Loss: 0.5972 | Train Acc: 1.0000 | Val Loss: 0.8967 | Val Acc: 0.8468 | LR: 0.000743\n",
            "[Trial 21/40] [EarlyStopping] triggered at epoch 41\n",
            "[Trial 21/40] [Info] Loaded best model weights\n",
            "[Trial 21/40] Done | Val Loss: 0.8336 | Val Acc: 0.8739\n",
            "[I 2025-11-10 18:26:57,150] Trial 20 finished with value: 0.8738738738738738 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 2, 'bidirectional': True, 'dropout': 0.23323950745284916, 'batch_size': 16, 'learning_rate': 0.0014862513558799053, 'weight_decay': 0.00043239535484478883, 'label_smoothing': 0.17249617983611412, 'tune_epochs': 52}. Best is trial 12 with value: 0.8963963963963963.\n",
            "[Optuna] Completed Trial 21/40 | Value: 0.8739 | Best so far: 0.8964\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 22/40] START\n",
            "[Trial 22/40] Params: embedding_dim=128, hidden_size=128, num_layers=2, bidirectional=True, dropout=0.42, batch_size=24, lr=0.002292, weight_decay=5.4e-05, label_smoothing=0.09, tune_epochs=47\n",
            "[Trial 22/40] [Train] epochs=47, batch_size=24, lr=0.002292, patience=10\n",
            "[Trial 22/40] Epoch   1/47 | Train Loss: 1.7368 | Train Acc: 0.2045 | Val Loss: 1.6628 | Val Acc: 0.1892 | LR: 0.002292\n",
            "[Trial 22/40] Epoch   2/47 | Train Loss: 1.5587 | Train Acc: 0.3365 | Val Loss: 1.5746 | Val Acc: 0.2973 | LR: 0.002292\n",
            "[Trial 22/40] Epoch   3/47 | Train Loss: 1.3825 | Train Acc: 0.4567 | Val Loss: 1.6097 | Val Acc: 0.4054 | LR: 0.002292\n",
            "[Trial 22/40] Epoch   4/47 | Train Loss: 1.1885 | Train Acc: 0.6062 | Val Loss: 1.2224 | Val Acc: 0.6171 | LR: 0.002292\n",
            "[Trial 22/40] Epoch   5/47 | Train Loss: 0.8418 | Train Acc: 0.7747 | Val Loss: 1.1295 | Val Acc: 0.6712 | LR: 0.002292\n",
            "[Trial 22/40] Epoch   6/47 | Train Loss: 0.6780 | Train Acc: 0.8663 | Val Loss: 1.0300 | Val Acc: 0.7072 | LR: 0.002292\n",
            "[Trial 22/40] Epoch   7/47 | Train Loss: 0.5360 | Train Acc: 0.9421 | Val Loss: 0.9157 | Val Acc: 0.7703 | LR: 0.002292\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "PyTorch R-LSTM Text Classification + Optuna (40 trials) — Verbose, SVG plots\n",
        "Dataset: df_file.csv with columns ['Text', 'Label'] and 5 classes\n",
        "\n",
        "Pipeline:\n",
        "- Preprocess -> vocabulary (5000) -> indexify (seq_len=50)\n",
        "- R-LSTM (Residual LSTM): stacked LSTM layers with residual projections per layer\n",
        "- Optuna tunes: embedding_dim, hidden_size, num_layers, bidirectional,\n",
        "                dropout, batch_size, learning_rate, weight_decay,\n",
        "                label_smoothing, tune_epochs\n",
        "- Retrain final model with best hyperparameters (max 150 epochs, early stopping)\n",
        "- Saves:\n",
        "    * optuna_trials.csv\n",
        "    * optuna_best_params.json\n",
        "    * rlstm_training_curves.svg\n",
        "    * rlstm_confusion_matrix.svg\n",
        "    * rlstm_per_class_accuracy.svg\n",
        "    * optuna_optimization_history.svg\n",
        "    * optuna_param_importance.svg\n",
        "    * rlstm_lr_schedule.svg\n",
        "    * rlstm_pytorch_model.pt\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib; matplotlib.use(\"Agg\")  # headless\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# -----------------------------\n",
        "# Optuna import / auto-install\n",
        "# -----------------------------\n",
        "try:\n",
        "    import optuna\n",
        "    from optuna.importance import get_param_importances\n",
        "except ImportError:\n",
        "    print(\"[Setup] Optuna not found. Installing optuna ...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna\"])\n",
        "    import optuna\n",
        "    from optuna.importance import get_param_importances\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
        "\n",
        "# -----------------------------\n",
        "# Config / reproducibility\n",
        "# -----------------------------\n",
        "GLOBAL_SEED = 42\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "torch.manual_seed(GLOBAL_SEED)\n",
        "\n",
        "N_TRIALS = 40\n",
        "SEQ_LEN = 50\n",
        "VOCAB_SIZE = 5000\n",
        "FINAL_MAX_EPOCHS = 150\n",
        "\n",
        "PRINT_LINE = \"=\" * 70\n",
        "def hr(msg: str):\n",
        "    print(\"\\n\" + PRINT_LINE)\n",
        "    print(msg)\n",
        "    print(PRINT_LINE)\n",
        "\n",
        "# -----------------------------\n",
        "# Device selection\n",
        "# -----------------------------\n",
        "def get_device():\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        print(\"Using Mac GPU (MPS)\")\n",
        "        return torch.device(\"mps\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        return torch.device(\"cuda\")\n",
        "    print(\"Using CPU\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "print(f\"Device: {device}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load data\n",
        "# -----------------------------\n",
        "hr(\"LOADING DATASET\")\n",
        "# For Colab:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# df = pd.read_csv(list(uploaded.keys())[0])\n",
        "\n",
        "df = pd.read_csv('df_file.csv')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"Number of classes: {df['Label'].nunique()}\")\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['Label'].value_counts().sort_index())\n",
        "\n",
        "class_names = {0: 'Politics', 1: 'Sport', 2: 'Technology', 3: 'Entertainment', 4: 'Business'}\n",
        "print(\"\\nClass mapping:\")\n",
        "for label, name in class_names.items():\n",
        "    print(f\"  {label}: {name} ({len(df[df['Label'] == label])} samples)\")\n",
        "print(PRINT_LINE)\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocess\n",
        "# -----------------------------\n",
        "hr(\"PREPROCESSING\")\n",
        "print(\"[Step] Lowercasing and dropping empty rows ...\")\n",
        "df['Text'] = df['Text'].astype(str).str.lower()\n",
        "df = df[df['Text'].str.len() > 0].reset_index(drop=True)\n",
        "print(f\"[Done] Dataset shape after preprocessing: {df.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Vocabulary\n",
        "# -----------------------------\n",
        "hr(\"VOCABULARY\")\n",
        "print(\"[Step] Counting token frequencies ...\")\n",
        "all_words = []\n",
        "for text in df['Text'].values:\n",
        "    all_words.extend(text.split())\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "print(f\"[Info] Total unique tokens: {len(word_counts)}\")\n",
        "\n",
        "print(f\"[Step] Building vocab size={VOCAB_SIZE} with <PAD>=0, <UNK>=1 ...\")\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
        "for w, _ in word_counts.most_common(VOCAB_SIZE - 2):\n",
        "    vocab[w] = len(vocab)\n",
        "coverage = (len(vocab) / max(1, len(word_counts))) * 100\n",
        "print(f\"[Done] Vocab size: {len(vocab)} | Coverage: {coverage:.2f}%\")\n",
        "\n",
        "inverse_vocab = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "# -----------------------------\n",
        "# Sequences\n",
        "# -----------------------------\n",
        "hr(\"SEQUENCE ENCODING\")\n",
        "print(f\"[Info] Sequence length: {SEQ_LEN}\")\n",
        "print(\"[Step] Converting texts to index sequences ...\")\n",
        "\n",
        "X_sequences = []\n",
        "for text in df['Text'].values:\n",
        "    words = text.split()[:SEQ_LEN]\n",
        "    seq = [vocab.get(w, vocab['<UNK>']) for w in words]\n",
        "    if len(seq) < SEQ_LEN:\n",
        "        seq += [vocab['<PAD>']] * (SEQ_LEN - len(seq))\n",
        "    X_sequences.append(seq)\n",
        "\n",
        "X_sequences = np.array(X_sequences, dtype=np.int64)\n",
        "y_labels = df['Label'].astype(int).values\n",
        "\n",
        "print(f\"[Done] X_sequences shape: {X_sequences.shape}\")\n",
        "print(f\"[Done] y_labels shape:  {y_labels.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Split\n",
        "# -----------------------------\n",
        "hr(\"DATA SPLIT\")\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_sequences, y_labels, test_size=0.2, random_state=GLOBAL_SEED, stratify=y_labels\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=GLOBAL_SEED, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"[Info] Training set:   {X_train.shape[0]} samples ({X_train.shape[0]/len(X_sequences)*100:.1f}%)\")\n",
        "print(f\"[Info] Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X_sequences)*100:.1f}%)\")\n",
        "print(f\"[Info] Test set:       {X_test.shape[0]} samples ({X_test.shape[0]/len(X_sequences)*100:.1f}%)\")\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset class\n",
        "# -----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = torch.LongTensor(sequences)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "val_dataset   = TextDataset(X_val,   y_val)\n",
        "test_dataset  = TextDataset(X_test,  y_test)\n",
        "\n",
        "# -----------------------------\n",
        "# R-LSTM model (Residual LSTM)\n",
        "# -----------------------------\n",
        "class ResLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual LSTM stack:\n",
        "      For each layer i:\n",
        "        y_i = LSTM_i(x_i)\n",
        "        y_i = y_i + Proj_i(x_i)  # residual projection to match dims\n",
        "        x_{i+1} = Dropout(y_i)\n",
        "    After the last layer, use last time-step features for classification.\n",
        "    Supports bidirectionality at each layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size,\n",
        "                 num_layers=2, dropout=0.5, bidirectional=False, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.hidden_size   = hidden_size\n",
        "        self.num_layers    = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_dirs      = 2 if bidirectional else 1\n",
        "        self.out_dim       = hidden_size * self.num_dirs\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # Build per-layer LSTMs and residual projections\n",
        "        self.lstm_layers = nn.ModuleList()\n",
        "        self.proj_layers = nn.ModuleList()\n",
        "        self.drop_layers = nn.ModuleList()\n",
        "\n",
        "        in_dim = embedding_dim\n",
        "        for li in range(num_layers):\n",
        "            lstm = nn.LSTM(\n",
        "                input_size=in_dim,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers=1,\n",
        "                batch_first=True,\n",
        "                bidirectional=bidirectional\n",
        "            )\n",
        "            self.lstm_layers.append(lstm)\n",
        "            self.proj_layers.append(nn.Linear(in_dim, self.out_dim, bias=False))\n",
        "            # Dropout after residual addition; no dropout after the last by design choice\n",
        "            self.drop_layers.append(nn.Dropout(dropout))\n",
        "            in_dim = self.out_dim  # next layer input\n",
        "\n",
        "        self.final_dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(self.out_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T)\n",
        "        emb = self.embedding(x)  # (B, T, E)\n",
        "        h = emb\n",
        "        for li in range(self.num_layers):\n",
        "            lstm = self.lstm_layers[li]\n",
        "            proj = self.proj_layers[li]\n",
        "            drop = self.drop_layers[li]\n",
        "\n",
        "            y, _ = lstm(h)                    # (B, T, H * dirs)\n",
        "            res  = proj(h)                    # (B, T, H * dirs)\n",
        "            y    = y + res                    # residual add\n",
        "            if li < self.num_layers - 1:      # dropout between layers\n",
        "                y = drop(y)\n",
        "            h = y\n",
        "\n",
        "        # Last time step representation\n",
        "        last = h[:, -1, :]                    # (B, H * dirs)\n",
        "        last = self.final_dropout(last)\n",
        "        logits = self.fc(last)                # (B, C)\n",
        "        return logits\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval helpers\n",
        "# -----------------------------\n",
        "def current_lr(optimizer):\n",
        "    return float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for sequences, labels in dataloader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(sequences)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * sequences.size(0)\n",
        "        _, pred = torch.max(logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (pred == labels).sum().item()\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            logits = model(sequences)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item() * sequences.size(0)\n",
        "            _, pred = torch.max(logits, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum().item()\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def make_criterion(label_smoothing_value: float):\n",
        "    # Fallback for older torch that may not support label_smoothing\n",
        "    try:\n",
        "        return nn.CrossEntropyLoss(label_smoothing=float(label_smoothing_value))\n",
        "    except TypeError:\n",
        "        return nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                scheduler, num_epochs, device, patience=10, log_prefix=\"\",\n",
        "                record_lr=False):\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    lr_history = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    print(f\"{log_prefix}[Train] epochs={num_epochs}, batch_size={train_loader.batch_size}, \"\n",
        "          f\"lr={current_lr(optimizer):.6f}, patience={patience}\")\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Scheduler step with explicit LR-change log (no 'verbose' arg used)\n",
        "        lr_before = current_lr(optimizer)\n",
        "        scheduler.step(val_loss)\n",
        "        lr_after = current_lr(optimizer)\n",
        "        if record_lr:\n",
        "            lr_history.append(lr_after)\n",
        "        lr_note = \"\"\n",
        "        if lr_after < lr_before:\n",
        "            lr_note = f\" (LR reduced from {lr_before:.6f} to {lr_after:.6f})\"\n",
        "\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['train_acc'].append(tr_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"{log_prefix}Epoch {epoch:3d}/{num_epochs} | \"\n",
        "              f\"Train Loss: {tr_loss:.4f} | Train Acc: {tr_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
        "              f\"LR: {current_lr(optimizer):.6f}{lr_note}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"{log_prefix}[EarlyStopping] triggered at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "        print(f\"{log_prefix}[Info] Loaded best model weights\")\n",
        "\n",
        "    return history, lr_history\n",
        "\n",
        "# -----------------------------\n",
        "# Optuna Objective\n",
        "# -----------------------------\n",
        "def objective(trial: optuna.trial.Trial) -> float:\n",
        "    # Hyperparameter search space\n",
        "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [32, 64, 128])\n",
        "    hidden_size   = trial.suggest_categorical(\"hidden_size\",   [32, 64, 96, 128, 160, 192])\n",
        "    num_layers    = trial.suggest_categorical(\"num_layers\",    [1, 2, 3, 4])\n",
        "    bidirectional = trial.suggest_categorical(\"bidirectional\", [False, True])\n",
        "    dropout       = trial.suggest_float(\"dropout\", 0.1, 0.6)\n",
        "    batch_size    = trial.suggest_categorical(\"batch_size\",    [16, 24, 32, 48, 64])\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 5e-3, log=True)\n",
        "    weight_decay  = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "    label_smooth  = trial.suggest_float(\"label_smoothing\", 0.0, 0.2)\n",
        "    tune_epochs   = trial.suggest_int(\"tune_epochs\", 25, 60)\n",
        "    patience      = 10\n",
        "\n",
        "    tnum = trial.number + 1\n",
        "    header = f\"[Trial {tnum:02d}/{N_TRIALS}] \"\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"{header}START\")\n",
        "    print(f\"{header}Params: embedding_dim={embedding_dim}, hidden_size={hidden_size}, \"\n",
        "          f\"num_layers={num_layers}, bidirectional={bidirectional}, dropout={dropout:.2f}, \"\n",
        "          f\"batch_size={batch_size}, lr={learning_rate:.6f}, weight_decay={weight_decay:.1e}, \"\n",
        "          f\"label_smoothing={label_smooth:.2f}, tune_epochs={tune_epochs}\")\n",
        "\n",
        "    # DataLoaders for this trial\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model\n",
        "    model = ResLSTM(\n",
        "        vocab_size=len(vocab),\n",
        "        embedding_dim=embedding_dim,\n",
        "        hidden_size=hidden_size,\n",
        "        output_size=len(class_names),\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        bidirectional=bidirectional,\n",
        "        pad_idx=vocab['<PAD>']\n",
        "    ).to(device)\n",
        "\n",
        "    # Loss/optimizer/scheduler\n",
        "    criterion = make_criterion(label_smooth)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    # Train (verbose per-epoch with prefix)\n",
        "    _history, _lr = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "        num_epochs=int(tune_epochs), device=device, patience=patience, log_prefix=header\n",
        "    )\n",
        "\n",
        "    # Validation accuracy\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    print(f\"{header}Done | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    trial.set_user_attr(\"val_acc\", float(val_acc))\n",
        "    return float(val_acc)\n",
        "\n",
        "def trial_callback(total_trials: int):\n",
        "    def _cb(study: optuna.study.Study, trial: optuna.trial.FrozenTrial):\n",
        "        best = study.best_value if study.best_trial is not None else None\n",
        "        print(f\"[Optuna] Completed Trial {trial.number + 1}/{total_trials} | \"\n",
        "              f\"Value: {trial.value:.4f} | Best so far: {best:.4f}\")\n",
        "    return _cb\n",
        "\n",
        "# -----------------------------\n",
        "# Run Optuna\n",
        "# -----------------------------\n",
        "hr(\"OPTUNA STUDY (40 TRIALS)\")\n",
        "sampler = optuna.samplers.TPESampler(seed=GLOBAL_SEED)\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "study.optimize(objective, n_trials=N_TRIALS, callbacks=[trial_callback(N_TRIALS)], show_progress_bar=True)\n",
        "\n",
        "print(\"\\n[Optuna] Best trial:\")\n",
        "best_trial = study.best_trial\n",
        "print(f\"  Value (Val Acc): {best_trial.value:.4f}\")\n",
        "print(\"  Params:\")\n",
        "for k, v in best_trial.params.items():\n",
        "    print(f\"    {k}: {v}\")\n",
        "\n",
        "# Save trials dataframe\n",
        "try:\n",
        "    df_trials = study.trials_dataframe()\n",
        "    df_trials.to_csv(\"optuna_trials.csv\", index=False)\n",
        "    print(\"[Output] Saved optuna_trials.csv\")\n",
        "except Exception as e:\n",
        "    print(f\"[Warn] Could not save trials dataframe: {e}\")\n",
        "\n",
        "# Save best params\n",
        "with open(\"optuna_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(best_trial.params, f, indent=2)\n",
        "print(\"[Output] Saved optuna_best_params.json\")\n",
        "\n",
        "# -----------------------------\n",
        "# Plot Optuna optimization history & importances (SVG)\n",
        "# -----------------------------\n",
        "try:\n",
        "    # Optimization history (best value so far vs. trial number)\n",
        "    vals = []\n",
        "    best_so_far = []\n",
        "    for t in study.get_trials(deepcopy=False):\n",
        "        if t.value is not None:\n",
        "            vals.append(t.value)\n",
        "            best_so_far.append(max(best_so_far[-1], t.value) if best_so_far else t.value)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, len(vals)+1), vals, marker='o', linewidth=1, label='Trial value')\n",
        "    plt.plot(range(1, len(best_so_far)+1), best_so_far, linewidth=2, label='Best so far')\n",
        "    plt.xlabel('Trial')\n",
        "    plt.ylabel('Validation accuracy')\n",
        "    plt.title('Optuna Optimization History')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('optuna_optimization_history.svg', format='svg')\n",
        "    plt.close()\n",
        "    print(\"[Output] Saved optuna_optimization_history.svg\")\n",
        "\n",
        "    # Parameter importance\n",
        "    try:\n",
        "        importances = get_param_importances(study)\n",
        "        names = list(importances.keys())\n",
        "        scores = [importances[n] for n in names]\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        y_pos = np.arange(len(names))\n",
        "        plt.barh(y_pos, scores)\n",
        "        plt.yticks(y_pos, names)\n",
        "        plt.xlabel('Importance')\n",
        "        plt.title('Optuna Parameter Importance')\n",
        "        plt.grid(True, axis='x', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('optuna_param_importance.svg', format='svg')\n",
        "        plt.close()\n",
        "        print(\"[Output] Saved optuna_param_importance.svg\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Warn] Could not compute parameter importances: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"[Warn] Could not save Optuna plots: {e}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Final training with best params\n",
        "# -----------------------------\n",
        "hr(\"FINAL TRAINING WITH BEST HYPERPARAMETERS\")\n",
        "\n",
        "bp = best_trial.params\n",
        "embedding_dim = int(bp[\"embedding_dim\"])\n",
        "hidden_size   = int(bp[\"hidden_size\"])\n",
        "num_layers    = int(bp[\"num_layers\"])\n",
        "bidirectional = bool(bp[\"bidirectional\"])\n",
        "dropout       = float(bp[\"dropout\"])\n",
        "batch_size    = int(bp[\"batch_size\"])\n",
        "learning_rate = float(bp[\"learning_rate\"])\n",
        "weight_decay  = float(bp[\"weight_decay\"])\n",
        "label_smooth  = float(bp[\"label_smoothing\"])\n",
        "num_epochs    = FINAL_MAX_EPOCHS\n",
        "patience      = 20\n",
        "\n",
        "print(\"[Final Config]\")\n",
        "print(f\"  Vocab size:       {len(vocab)}\")\n",
        "print(f\"  Embedding dim:    {embedding_dim}\")\n",
        "print(f\"  Hidden size:      {hidden_size}\")\n",
        "print(f\"  Num layers:       {num_layers}\")\n",
        "print(f\"  Bidirectional:    {bidirectional}\")\n",
        "print(f\"  Dropout:          {dropout}\")\n",
        "print(f\"  Output size:      {len(class_names)}\")\n",
        "print(f\"  Batch size:       {batch_size}\")\n",
        "print(f\"  Learning rate:    {learning_rate}\")\n",
        "print(f\"  Weight decay:     {weight_decay}\")\n",
        "print(f\"  Label smoothing:  {label_smooth}\")\n",
        "print(f\"  Max epochs:       {num_epochs}\")\n",
        "print(f\"  Sequence length:  {SEQ_LEN}\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model = ResLSTM(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    output_size=len(class_names),\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout,\n",
        "    bidirectional=bidirectional,\n",
        "    pad_idx=vocab['<PAD>']\n",
        ").to(device)\n",
        "\n",
        "print(\"\\nModel architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "criterion = make_criterion(label_smooth)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "history, lr_hist = train_model(\n",
        "    model, train_loader, val_loader, criterion, optimizer,\n",
        "    scheduler, num_epochs, device, patience=patience, log_prefix=\"[Final] \",\n",
        "    record_lr=True\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation\n",
        "# -----------------------------\n",
        "hr(\"MODEL EVALUATION\")\n",
        "train_loss, train_acc = evaluate(model, train_loader, criterion, device)\n",
        "val_loss, val_acc     = evaluate(model, val_loader,   criterion, device)\n",
        "test_loss, test_acc   = evaluate(model, test_loader,  criterion, device)\n",
        "\n",
        "print(\"\\nFinal Accuracy Scores:\")\n",
        "print(f\"  Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "print(f\"  Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "print(f\"  Test Accuracy:       {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "# Collect test predictions\n",
        "model.eval()\n",
        "all_predictions, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in test_loader:\n",
        "        sequences = sequences.to(device)\n",
        "        logits = model(sequences)\n",
        "        _, pred = torch.max(logits, 1)\n",
        "        all_predictions.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "y_test_pred = np.array(all_predictions)\n",
        "y_test_true = np.array(all_labels)\n",
        "target_names = [class_names[i] for i in range(len(class_names))]\n",
        "\n",
        "print(\"\\n\" + PRINT_LINE)\n",
        "print(\"DETAILED CLASSIFICATION REPORT (Test Set)\")\n",
        "print(PRINT_LINE + \"\\n\")\n",
        "print(classification_report(y_test_true, y_test_pred, target_names=target_names))\n",
        "\n",
        "# -----------------------------\n",
        "# Visualizations (SVG)\n",
        "# -----------------------------\n",
        "hr(\"PLOTTING & SAVING FIGURES (SVG)\")\n",
        "\n",
        "# 1) Training/Validation loss & accuracy curves\n",
        "print(\"[Plot] Training/Validation curves -> rlstm_training_curves.svg\")\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
        "plt.plot(history['val_loss'],   label='Validation Loss', linewidth=2)\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training and Validation Loss')\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Training Accuracy', linewidth=2)\n",
        "plt.plot(history['val_acc'],   label='Validation Accuracy', linewidth=2)\n",
        "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Training and Validation Accuracy')\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('rlstm_training_curves.svg', format='svg')\n",
        "plt.close()\n",
        "\n",
        "# 2) Confusion Matrix\n",
        "print(\"[Plot] Confusion Matrix -> rlstm_confusion_matrix.svg\")\n",
        "cm = confusion_matrix(y_test_true, y_test_pred)\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix (Test Set)')\n",
        "plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('rlstm_confusion_matrix.svg', format='svg')\n",
        "plt.close()\n",
        "\n",
        "# 3) Per-class accuracy\n",
        "print(\"[Plot] Per-class accuracy -> rlstm_per_class_accuracy.svg\")\n",
        "per_class_acc = []\n",
        "for i in range(len(class_names)):\n",
        "    mask = (y_test_true == i)\n",
        "    acc = np.mean(y_test_pred[mask] == y_test_true[mask]) if np.sum(mask) > 0 else 0.0\n",
        "    per_class_acc.append(acc)\n",
        "plt.figure(figsize=(9, 5))\n",
        "bars = plt.bar(range(len(class_names)), per_class_acc)\n",
        "plt.xlabel('Class'); plt.ylabel('Accuracy'); plt.title('Per-Class Accuracy on Test Set')\n",
        "plt.xticks(range(len(class_names)), target_names, rotation=45, ha='right')\n",
        "plt.ylim([0, 1.1]); plt.grid(True, alpha=0.3, axis='y')\n",
        "for bar, acc in zip(bars, per_class_acc):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "             f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('rlstm_per_class_accuracy.svg', format='svg')\n",
        "plt.close()\n",
        "\n",
        "# 4) Final LR schedule\n",
        "print(\"[Plot] Learning Rate schedule -> rlstm_lr_schedule.svg\")\n",
        "if len(lr_hist) > 0:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(range(1, len(lr_hist)+1), lr_hist, marker='o', linewidth=2)\n",
        "    plt.xlabel('Epoch'); plt.ylabel('Learning rate'); plt.title('Final Training LR Schedule')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rlstm_lr_schedule.svg', format='svg')\n",
        "    plt.close()\n",
        "else:\n",
        "    print(\"[Info] No LR history recorded.\")\n",
        "\n",
        "print(\"[Output] Saved SVGs: rlstm_training_curves.svg, rlstm_confusion_matrix.svg, rlstm_per_class_accuracy.svg, rlstm_lr_schedule.svg\")\n",
        "\n",
        "# -----------------------------\n",
        "# Sample predictions\n",
        "# -----------------------------\n",
        "hr(\"SAMPLE PREDICTIONS\")\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "k = min(5, len(X_test))\n",
        "sample_indices = np.random.choice(len(X_test), size=k, replace=False)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx in sample_indices:\n",
        "        seq_tensor = torch.LongTensor(X_test[idx]).unsqueeze(0).to(device)\n",
        "        logits = model(seq_tensor)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        pred_label = int(np.argmax(probs))\n",
        "        true_label = int(y_test[idx])\n",
        "\n",
        "        tokens = [inverse_vocab.get(int(tok), \"<UNK>\") for tok in X_test[idx] if int(tok) != vocab['<PAD>']]\n",
        "        text_preview = \" \".join(tokens[:20]) if tokens else \"N/A\"\n",
        "\n",
        "        print(f\"Text preview: {text_preview}...\")\n",
        "        print(f\"True Label: {class_names[true_label]}\")\n",
        "        print(f\"Predicted:  {class_names[pred_label]}\")\n",
        "        print(f\"Confidence: {probs[pred_label]*100:.2f}%\")\n",
        "        print(\"Result: \" + (\"Correct\" if true_label == pred_label else \"Incorrect\"))\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "# -----------------------------\n",
        "# Save model\n",
        "# -----------------------------\n",
        "print(\"\\nSaving model and metadata ...\")\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'vocab': vocab,\n",
        "    'class_names': class_names,\n",
        "    'hyperparameters': {\n",
        "        'embedding_dim': embedding_dim,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'bidirectional': bidirectional,\n",
        "        'dropout': dropout,\n",
        "        'batch_size': batch_size,\n",
        "        'learning_rate': learning_rate,\n",
        "        'weight_decay': weight_decay,\n",
        "        'label_smoothing': label_smooth,\n",
        "        'seq_len': SEQ_LEN,\n",
        "        'vocab_size': len(vocab),\n",
        "        'architecture': 'ResLSTM'\n",
        "    }\n",
        "}, 'rlstm_pytorch_model.pt')\n",
        "print(\"Saved model as: rlstm_pytorch_model.pt\")\n",
        "\n",
        "print(\"\\n\" + PRINT_LINE)\n",
        "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(PRINT_LINE)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Device used: {device}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "444603650a684445aaba174addd0645d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "615b32600f4e4efcabd2224167925596": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c3bd860039f4d6d92cb4c8aaf72f4f6",
            "max": 40,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76e4adfab9a644bbb02bb7c4ec1a8e31",
            "value": 21
          }
        },
        "63547744ca12426cbd86faed8f28d0ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76e4adfab9a644bbb02bb7c4ec1a8e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "774f7586156545ab86e56f63c05fd54d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c3bd860039f4d6d92cb4c8aaf72f4f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "991e43d25b044f7db120a279f0eec4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adc69da04c95460f8fd3b2886897b5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bee8d116c6f648ad854a81d35dd950b0",
            "placeholder": "​",
            "style": "IPY_MODEL_774f7586156545ab86e56f63c05fd54d",
            "value": " 21/40 [5:44:17&lt;9:36:00, 1818.98s/it]"
          }
        },
        "bee8d116c6f648ad854a81d35dd950b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5667cc234c44ecb9e7479a12b1b74e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0e8d4d62ddf44cdbcbd7b1621999ddd",
              "IPY_MODEL_615b32600f4e4efcabd2224167925596",
              "IPY_MODEL_adc69da04c95460f8fd3b2886897b5bb"
            ],
            "layout": "IPY_MODEL_444603650a684445aaba174addd0645d"
          }
        },
        "e0e8d4d62ddf44cdbcbd7b1621999ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63547744ca12426cbd86faed8f28d0ab",
            "placeholder": "​",
            "style": "IPY_MODEL_991e43d25b044f7db120a279f0eec4b7",
            "value": "Best trial: 12. Best value: 0.896396:  52%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
