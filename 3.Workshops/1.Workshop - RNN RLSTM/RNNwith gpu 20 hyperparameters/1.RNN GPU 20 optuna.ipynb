{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Workshop 3 — RNN: From-Scratch Concept and Text Classification (Kaggle)\n",
        "\n",
        "**Author:** Arman Golbidi  \n",
        "**Course:** BM20A6100 Advanced Data Analysis and Machine Learning  \n",
        "**Period:** 1 Sep 2025 – 12 Dec 2025\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Introduction\n",
        "\n",
        "This notebook implements a compact text classification pipeline with a vanilla **PyTorch RNN**. We (i) review the core mechanics of RNNs using a from-scratch concept, and (ii) train a practical classifier on a small Kaggle news-style dataset. Hyperparameters are tuned with **Optuna** (20 trials). We then retrain with the best settings and report accuracy, learning curves, and a confusion matrix.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Dataset\n",
        "\n",
        "- **Source:** *Text classification documentation* (Kaggle).  \n",
        "- **Size:** 2,225 labeled documents.  \n",
        "- **Labels (5):** `0=Politics`, `1=Sport`, `2=Technology`, `3=Entertainment`, `4=Business`.  \n",
        "- **Split:** Train 80% (1780), Val 10% (222), Test 10% (223).\n",
        "\n",
        "### Preprocessing\n",
        "- Lowercase text and drop empty rows.\n",
        "- Build a frequency-based **vocabulary of 5,000** tokens with special tokens `<PAD>=0` and `<UNK>=1` (≈8.25% unique-token coverage).\n",
        "- Convert each document to a **fixed-length sequence of 50** token IDs via truncation/padding.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Methods\n",
        "\n",
        "### Model\n",
        "`Embedding → nn.RNN (tanh) → Dropout → Linear` using the **final hidden state** as sequence representation.\n",
        "\n",
        "### Training\n",
        "- **Loss:** Cross-Entropy (with label smoothing if supported).\n",
        "- **Optimizer:** Adam + weight decay; **LR scheduler:** `ReduceLROnPlateau`.\n",
        "- **Regularization:** Dropout, gradient clipping (max-norm 5.0), early stopping on validation loss.\n",
        "\n",
        "### Hyperparameter Tuning (Optuna, 20 trials)\n",
        "Search over:\n",
        "- `embedding_dim ∈ {32, 64, 128}`\n",
        "- `hidden_size ∈ {32, 64, 96, 128, 160}`\n",
        "- `num_layers ∈ {1, 2, 3}`\n",
        "- `dropout ∈ [0.1, 0.6]`\n",
        "- `batch_size ∈ {16, 24, 32, 48, 64}`\n",
        "- `learning_rate ∈ [1e-4, 5e-3]`\n",
        "- `weight_decay ∈ [1e-6, 1e-3]`\n",
        "- `label_smoothing ∈ [0.0, 0.2]`\n",
        "- `tune_epochs ∈ [25, 60]`\n",
        "\n",
        "**Best validation accuracy:** `0.6667` (trial 16).  \n",
        "The model is then retrained with the best configuration for up to **150 epochs** with patience.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Results\n",
        "\n",
        "- Final **train/val/test accuracy** are printed after training.\n",
        "- **Learning curves** (loss & accuracy) and **confusion matrix** are saved to:\n",
        "  - `rnn_pytorch_training_results.png`\n",
        "- **Per-class test accuracy** is saved to:\n",
        "  - `rnn_pytorch_per_class_accuracy.png`\n",
        "\n",
        "> Quick interpretation: The model performs best on *Sport* and *Technology*, and is weaker on *Politics*. There is a noticeable train–val gap indicating mild overfitting.\n",
        "\n",
        "**Figures**  \n",
        "(If running in Jupyter, display with `IPython.display.Image` or just open the files)\n",
        "- `2.rnn_pytorch_training_results.png` — training/validation curves and test confusion matrix  \n",
        "- `1.rnn_pytorch_per_class_accuracy.png` — per-class accuracy bars\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Discussion & Next Steps\n",
        "\n",
        "- The vanilla RNN baseline is competitive on this small dataset, but can be improved.\n",
        "- Potential upgrades:\n",
        "  - Replace RNN with **BiLSTM or GRU**.\n",
        "  - Use **pretrained subword embeddings** (e.g., fastText) for higher OOV coverage.\n",
        "  - Mitigate imbalance with **class weighting** or **focal loss**.\n",
        "  - Try hybrid features (e.g., TF-IDF + neural) or shallow ensembling.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Reproducibility\n",
        "\n",
        "- Best hyperparameters saved to `optuna_best_params.json`.\n",
        "- All trials exported to `optuna_trials.csv`.\n",
        "- Trained model + metadata saved as `rnn_pytorch_model.pt`.\n",
        "\n",
        "> To reproduce, place `df_file.csv` (with columns `Text`, `Label`) next to the notebook, then run all cells. Figures will be generated automatically.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Code part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f49dcd2ffb8442a08e4d99ce66566dd7",
            "367eb461c3074766982658d4effd638a",
            "5adcdc9b64774579bac4f08d5e4b4eb3",
            "2148560e571740eaafdd2300b9a69271",
            "ff733b753e7c44ff8bea380d1c23198e",
            "a3681808c5fa4699a65a4e06d58df9e0",
            "c4288fc6c8894db6a571137d5b79f6cc",
            "57d6fd6c2b5d44feac56be0f9f158ead",
            "3ba50848270d4d9e86582fc3d7806b4b",
            "334ffff882564e87bd7f5a24e3985682",
            "9cc0f24fc2f745ecadc1245bc10a0ee9"
          ]
        },
        "id": "tGN2f7UHPuLD",
        "outputId": "bc255750-33cf-48d7-9990-7366c792f76d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA GPU: Tesla T4\n",
            "Device: cuda\n",
            "\n",
            "\n",
            "======================================================================\n",
            "LOADING DATASET\n",
            "======================================================================\n",
            "Dataset shape: (2225, 2)\n",
            "Columns: ['Text', 'Label']\n",
            "Number of classes: 5\n",
            "\n",
            "Label distribution:\n",
            "Label\n",
            "0    417\n",
            "1    511\n",
            "2    401\n",
            "3    386\n",
            "4    510\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class mapping:\n",
            "  0: Politics (417 samples)\n",
            "  1: Sport (511 samples)\n",
            "  2: Technology (401 samples)\n",
            "  3: Entertainment (386 samples)\n",
            "  4: Business (510 samples)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "PREPROCESSING\n",
            "======================================================================\n",
            "[Step] Lowercasing and dropping empty rows ...\n",
            "[Done] Dataset shape after preprocessing: (2225, 2)\n",
            "\n",
            "======================================================================\n",
            "VOCABULARY\n",
            "======================================================================\n",
            "[Step] Counting token frequencies ...\n",
            "[Info] Total unique tokens: 60616\n",
            "[Step] Building vocab size=5000 with <PAD>=0, <UNK>=1 ...\n",
            "[Done] Vocab size: 5000 | Coverage: 8.25%\n",
            "\n",
            "======================================================================\n",
            "SEQUENCE ENCODING\n",
            "======================================================================\n",
            "[Info] Sequence length: 50\n",
            "[Step] Converting texts to index sequences ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-10 12:15:37,834] A new study created in memory with name: no-name-65c89984-5b88-4c82-ba52-c4fb69512272\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Done] X_sequences shape: (2225, 50)\n",
            "[Done] y_labels shape:  (2225,)\n",
            "\n",
            "======================================================================\n",
            "DATA SPLIT\n",
            "======================================================================\n",
            "[Info] Training set:   1780 samples (80.0%)\n",
            "[Info] Validation set: 222 samples (10.0%)\n",
            "[Info] Test set:       223 samples (10.0%)\n",
            "\n",
            "======================================================================\n",
            "OPTUNA STUDY (20 TRIALS)\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f49dcd2ffb8442a08e4d99ce66566dd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 01/20] START\n",
            "[Trial 01/20] Params: embedding_dim=64, hidden_size=160, num_layers=2, dropout=0.58, batch_size=16, lr=0.000779, weight_decay=2.0e-05, label_smoothing=0.06, tune_epochs=47\n",
            "[Trial 01/20] [Train] epochs=47, batch_size=16, lr=0.000779, patience=10\n",
            "[Trial 01/20] Epoch   1/47 | Train Loss: 1.6375 | Train Acc: 0.2112 | Val Loss: 1.6080 | Val Acc: 0.2432 | LR: 0.000779\n",
            "[Trial 01/20] Epoch   2/47 | Train Loss: 1.5669 | Train Acc: 0.2978 | Val Loss: 1.6171 | Val Acc: 0.2748 | LR: 0.000779\n",
            "[Trial 01/20] Epoch   3/47 | Train Loss: 1.4799 | Train Acc: 0.3764 | Val Loss: 1.6572 | Val Acc: 0.2838 | LR: 0.000779\n",
            "[Trial 01/20] Epoch   4/47 | Train Loss: 1.3715 | Train Acc: 0.4562 | Val Loss: 1.5620 | Val Acc: 0.3108 | LR: 0.000779\n",
            "[Trial 01/20] Epoch   5/47 | Train Loss: 1.2385 | Train Acc: 0.5258 | Val Loss: 1.5671 | Val Acc: 0.3919 | LR: 0.000779\n",
            "[Trial 01/20] Epoch   6/47 | Train Loss: 1.1619 | Train Acc: 0.5584 | Val Loss: 1.5600 | Val Acc: 0.3604 | LR: 0.000779\n",
            "[Trial 01/20] Epoch   7/47 | Train Loss: 1.0540 | Train Acc: 0.5983 | Val Loss: 1.7534 | Val Acc: 0.3919 | LR: 0.000779\n",
            "[Trial 01/20] Epoch   8/47 | Train Loss: 0.9694 | Train Acc: 0.6702 | Val Loss: 1.5673 | Val Acc: 0.4189 | LR: 0.000779\n",
            "[Trial 01/20] Epoch   9/47 | Train Loss: 0.9226 | Train Acc: 0.7000 | Val Loss: 1.5634 | Val Acc: 0.3919 | LR: 0.000779\n",
            "[Trial 01/20] Epoch  10/47 | Train Loss: 0.8012 | Train Acc: 0.7511 | Val Loss: 1.6604 | Val Acc: 0.4009 | LR: 0.000779\n",
            "[Trial 01/20] Epoch  11/47 | Train Loss: 0.7348 | Train Acc: 0.7994 | Val Loss: 1.6597 | Val Acc: 0.4279 | LR: 0.000779\n",
            "[Trial 01/20] Epoch  12/47 | Train Loss: 0.6887 | Train Acc: 0.8140 | Val Loss: 1.7764 | Val Acc: 0.4234 | LR: 0.000390 (LR reduced from 0.000779 to 0.000390)\n",
            "[Trial 01/20] Epoch  13/47 | Train Loss: 0.5490 | Train Acc: 0.8910 | Val Loss: 1.7219 | Val Acc: 0.4640 | LR: 0.000390\n",
            "[Trial 01/20] Epoch  14/47 | Train Loss: 0.5099 | Train Acc: 0.9028 | Val Loss: 1.7665 | Val Acc: 0.4414 | LR: 0.000390\n",
            "[Trial 01/20] Epoch  15/47 | Train Loss: 0.4694 | Train Acc: 0.9287 | Val Loss: 1.7803 | Val Acc: 0.4820 | LR: 0.000390\n",
            "[Trial 01/20] Epoch  16/47 | Train Loss: 0.4551 | Train Acc: 0.9360 | Val Loss: 1.7722 | Val Acc: 0.4865 | LR: 0.000390\n",
            "[Trial 01/20] [EarlyStopping] triggered at epoch 16\n",
            "[Trial 01/20] [Info] Loaded best model weights\n",
            "[Trial 01/20] Done | Val Loss: 1.5600 | Val Acc: 0.3604\n",
            "[I 2025-11-10 12:15:47,498] Trial 0 finished with value: 0.36036036036036034 and parameters: {'embedding_dim': 64, 'hidden_size': 160, 'num_layers': 2, 'dropout': 0.5849549260809972, 'batch_size': 16, 'learning_rate': 0.0007790143126276247, 'weight_decay': 1.9762189340280066e-05, 'label_smoothing': 0.058245828039608386, 'tune_epochs': 47}. Best is trial 0 with value: 0.36036036036036034.\n",
            "[Optuna] Completed Trial 1/20 | Value: 0.3604 | Best so far: 0.3604\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 02/20] START\n",
            "[Trial 02/20] Params: embedding_dim=128, hidden_size=64, num_layers=2, dropout=0.13, batch_size=24, lr=0.001454, weight_decay=2.1e-05, label_smoothing=0.02, tune_epochs=42\n",
            "[Trial 02/20] [Train] epochs=42, batch_size=24, lr=0.001454, patience=10\n",
            "[Trial 02/20] Epoch   1/42 | Train Loss: 1.6148 | Train Acc: 0.2219 | Val Loss: 1.5908 | Val Acc: 0.2748 | LR: 0.001454\n",
            "[Trial 02/20] Epoch   2/42 | Train Loss: 1.4894 | Train Acc: 0.3669 | Val Loss: 1.5725 | Val Acc: 0.3063 | LR: 0.001454\n",
            "[Trial 02/20] Epoch   3/42 | Train Loss: 1.3463 | Train Acc: 0.4506 | Val Loss: 1.5842 | Val Acc: 0.3018 | LR: 0.001454\n",
            "[Trial 02/20] Epoch   4/42 | Train Loss: 1.1174 | Train Acc: 0.6011 | Val Loss: 1.5800 | Val Acc: 0.3919 | LR: 0.001454\n",
            "[Trial 02/20] Epoch   5/42 | Train Loss: 0.8344 | Train Acc: 0.7169 | Val Loss: 1.6110 | Val Acc: 0.4279 | LR: 0.001454\n",
            "[Trial 02/20] Epoch   6/42 | Train Loss: 0.6599 | Train Acc: 0.7899 | Val Loss: 1.7291 | Val Acc: 0.4279 | LR: 0.001454\n",
            "[Trial 02/20] Epoch   7/42 | Train Loss: 0.4975 | Train Acc: 0.8556 | Val Loss: 1.7945 | Val Acc: 0.4505 | LR: 0.001454\n",
            "[Trial 02/20] Epoch   8/42 | Train Loss: 0.3541 | Train Acc: 0.9264 | Val Loss: 1.7579 | Val Acc: 0.5000 | LR: 0.000727 (LR reduced from 0.001454 to 0.000727)\n",
            "[Trial 02/20] Epoch   9/42 | Train Loss: 0.2533 | Train Acc: 0.9713 | Val Loss: 1.7951 | Val Acc: 0.5090 | LR: 0.000727\n",
            "[Trial 02/20] Epoch  10/42 | Train Loss: 0.2099 | Train Acc: 0.9848 | Val Loss: 1.8506 | Val Acc: 0.4910 | LR: 0.000727\n",
            "[Trial 02/20] Epoch  11/42 | Train Loss: 0.1968 | Train Acc: 0.9871 | Val Loss: 1.9591 | Val Acc: 0.4369 | LR: 0.000727\n",
            "[Trial 02/20] Epoch  12/42 | Train Loss: 0.1883 | Train Acc: 0.9904 | Val Loss: 1.9668 | Val Acc: 0.5045 | LR: 0.000727\n",
            "[Trial 02/20] [EarlyStopping] triggered at epoch 12\n",
            "[Trial 02/20] [Info] Loaded best model weights\n",
            "[Trial 02/20] Done | Val Loss: 1.5725 | Val Acc: 0.3063\n",
            "[I 2025-11-10 12:15:51,147] Trial 1 finished with value: 0.3063063063063063 and parameters: {'embedding_dim': 128, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.13252579649263976, 'batch_size': 24, 'learning_rate': 0.0014537555576161927, 'weight_decay': 2.091498132903561e-05, 'label_smoothing': 0.024407646968955768, 'tune_epochs': 42}. Best is trial 0 with value: 0.36036036036036034.\n",
            "[Optuna] Completed Trial 2/20 | Value: 0.3063 | Best so far: 0.3604\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 03/20] START\n",
            "[Trial 03/20] Params: embedding_dim=64, hidden_size=32, num_layers=1, dropout=0.55, batch_size=24, lr=0.000357, weight_decay=1.5e-05, label_smoothing=0.05, tune_epochs=54\n",
            "[Trial 03/20] [Train] epochs=54, batch_size=24, lr=0.000357, patience=10\n",
            "[Trial 03/20] Epoch   1/54 | Train Loss: 1.7015 | Train Acc: 0.2230 | Val Loss: 1.6377 | Val Acc: 0.2162 | LR: 0.000357\n",
            "[Trial 03/20] Epoch   2/54 | Train Loss: 1.6692 | Train Acc: 0.2270 | Val Loss: 1.6294 | Val Acc: 0.2252 | LR: 0.000357\n",
            "[Trial 03/20] Epoch   3/54 | Train Loss: 1.6439 | Train Acc: 0.2376 | Val Loss: 1.6245 | Val Acc: 0.2342 | LR: 0.000357\n",
            "[Trial 03/20] Epoch   4/54 | Train Loss: 1.6249 | Train Acc: 0.2444 | Val Loss: 1.6239 | Val Acc: 0.2568 | LR: 0.000357\n",
            "[Trial 03/20] Epoch   5/54 | Train Loss: 1.6249 | Train Acc: 0.2517 | Val Loss: 1.6193 | Val Acc: 0.2523 | LR: 0.000357\n",
            "[Trial 03/20] Epoch   6/54 | Train Loss: 1.6060 | Train Acc: 0.2680 | Val Loss: 1.6189 | Val Acc: 0.2613 | LR: 0.000357\n",
            "[Trial 03/20] Epoch   7/54 | Train Loss: 1.5958 | Train Acc: 0.2652 | Val Loss: 1.6185 | Val Acc: 0.2387 | LR: 0.000357\n",
            "[Trial 03/20] Epoch   8/54 | Train Loss: 1.5764 | Train Acc: 0.2730 | Val Loss: 1.6195 | Val Acc: 0.2477 | LR: 0.000357\n",
            "[Trial 03/20] Epoch   9/54 | Train Loss: 1.5780 | Train Acc: 0.2787 | Val Loss: 1.6196 | Val Acc: 0.2342 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  10/54 | Train Loss: 1.5582 | Train Acc: 0.3039 | Val Loss: 1.6190 | Val Acc: 0.2477 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  11/54 | Train Loss: 1.5596 | Train Acc: 0.3107 | Val Loss: 1.6192 | Val Acc: 0.2252 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  12/54 | Train Loss: 1.5499 | Train Acc: 0.3174 | Val Loss: 1.6172 | Val Acc: 0.2252 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  13/54 | Train Loss: 1.5370 | Train Acc: 0.3197 | Val Loss: 1.6182 | Val Acc: 0.2297 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  14/54 | Train Loss: 1.5377 | Train Acc: 0.3079 | Val Loss: 1.6179 | Val Acc: 0.2207 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  15/54 | Train Loss: 1.5198 | Train Acc: 0.3388 | Val Loss: 1.6155 | Val Acc: 0.2342 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  16/54 | Train Loss: 1.5193 | Train Acc: 0.3247 | Val Loss: 1.6148 | Val Acc: 0.2117 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  17/54 | Train Loss: 1.5028 | Train Acc: 0.3551 | Val Loss: 1.6128 | Val Acc: 0.2477 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  18/54 | Train Loss: 1.4927 | Train Acc: 0.3685 | Val Loss: 1.6063 | Val Acc: 0.2703 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  19/54 | Train Loss: 1.4654 | Train Acc: 0.3848 | Val Loss: 1.5993 | Val Acc: 0.2748 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  20/54 | Train Loss: 1.4475 | Train Acc: 0.4039 | Val Loss: 1.5666 | Val Acc: 0.3378 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  21/54 | Train Loss: 1.4235 | Train Acc: 0.4129 | Val Loss: 1.5251 | Val Acc: 0.3604 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  22/54 | Train Loss: 1.3836 | Train Acc: 0.4287 | Val Loss: 1.4617 | Val Acc: 0.3694 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  23/54 | Train Loss: 1.3333 | Train Acc: 0.4742 | Val Loss: 1.4392 | Val Acc: 0.3874 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  24/54 | Train Loss: 1.2765 | Train Acc: 0.4955 | Val Loss: 1.4339 | Val Acc: 0.4099 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  25/54 | Train Loss: 1.2469 | Train Acc: 0.5101 | Val Loss: 1.4173 | Val Acc: 0.4189 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  26/54 | Train Loss: 1.2295 | Train Acc: 0.5163 | Val Loss: 1.4100 | Val Acc: 0.4234 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  27/54 | Train Loss: 1.1935 | Train Acc: 0.5444 | Val Loss: 1.4293 | Val Acc: 0.4189 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  28/54 | Train Loss: 1.1822 | Train Acc: 0.5298 | Val Loss: 1.3856 | Val Acc: 0.4324 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  29/54 | Train Loss: 1.1485 | Train Acc: 0.5584 | Val Loss: 1.3506 | Val Acc: 0.4505 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  30/54 | Train Loss: 1.1304 | Train Acc: 0.5601 | Val Loss: 1.3433 | Val Acc: 0.4414 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  31/54 | Train Loss: 1.1102 | Train Acc: 0.5657 | Val Loss: 1.3430 | Val Acc: 0.4595 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  32/54 | Train Loss: 1.0810 | Train Acc: 0.5916 | Val Loss: 1.3480 | Val Acc: 0.4324 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  33/54 | Train Loss: 1.0762 | Train Acc: 0.5764 | Val Loss: 1.3541 | Val Acc: 0.4414 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  34/54 | Train Loss: 1.0407 | Train Acc: 0.6084 | Val Loss: 1.3610 | Val Acc: 0.4595 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  35/54 | Train Loss: 1.0318 | Train Acc: 0.6067 | Val Loss: 1.3506 | Val Acc: 0.4550 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  36/54 | Train Loss: 1.0196 | Train Acc: 0.6174 | Val Loss: 1.3397 | Val Acc: 0.4595 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  37/54 | Train Loss: 1.0068 | Train Acc: 0.6258 | Val Loss: 1.3527 | Val Acc: 0.4640 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  38/54 | Train Loss: 0.9886 | Train Acc: 0.6444 | Val Loss: 1.3593 | Val Acc: 0.4595 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  39/54 | Train Loss: 0.9686 | Train Acc: 0.6410 | Val Loss: 1.3665 | Val Acc: 0.4730 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  40/54 | Train Loss: 0.9538 | Train Acc: 0.6579 | Val Loss: 1.3490 | Val Acc: 0.5000 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  41/54 | Train Loss: 0.9297 | Train Acc: 0.6652 | Val Loss: 1.3458 | Val Acc: 0.4955 | LR: 0.000357\n",
            "[Trial 03/20] Epoch  42/54 | Train Loss: 0.9158 | Train Acc: 0.6685 | Val Loss: 1.3418 | Val Acc: 0.5045 | LR: 0.000179 (LR reduced from 0.000357 to 0.000179)\n",
            "[Trial 03/20] Epoch  43/54 | Train Loss: 0.8830 | Train Acc: 0.6871 | Val Loss: 1.3511 | Val Acc: 0.4955 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  44/54 | Train Loss: 0.8850 | Train Acc: 0.6938 | Val Loss: 1.3394 | Val Acc: 0.4955 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  45/54 | Train Loss: 0.8727 | Train Acc: 0.7006 | Val Loss: 1.3294 | Val Acc: 0.5090 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  46/54 | Train Loss: 0.8697 | Train Acc: 0.7022 | Val Loss: 1.3234 | Val Acc: 0.4820 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  47/54 | Train Loss: 0.8462 | Train Acc: 0.7146 | Val Loss: 1.3353 | Val Acc: 0.4955 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  48/54 | Train Loss: 0.8482 | Train Acc: 0.7169 | Val Loss: 1.3413 | Val Acc: 0.5180 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  49/54 | Train Loss: 0.8490 | Train Acc: 0.6978 | Val Loss: 1.3143 | Val Acc: 0.5135 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  50/54 | Train Loss: 0.8338 | Train Acc: 0.7140 | Val Loss: 1.3234 | Val Acc: 0.4955 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  51/54 | Train Loss: 0.8393 | Train Acc: 0.7191 | Val Loss: 1.3171 | Val Acc: 0.4820 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  52/54 | Train Loss: 0.8260 | Train Acc: 0.7197 | Val Loss: 1.2985 | Val Acc: 0.4955 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  53/54 | Train Loss: 0.8178 | Train Acc: 0.7331 | Val Loss: 1.3219 | Val Acc: 0.5180 | LR: 0.000179\n",
            "[Trial 03/20] Epoch  54/54 | Train Loss: 0.8138 | Train Acc: 0.7208 | Val Loss: 1.3399 | Val Acc: 0.5000 | LR: 0.000179\n",
            "[Trial 03/20] [Info] Loaded best model weights\n",
            "[Trial 03/20] Done | Val Loss: 1.2985 | Val Acc: 0.4955\n",
            "[I 2025-11-10 12:16:04,344] Trial 2 finished with value: 0.4954954954954955 and parameters: {'embedding_dim': 64, 'hidden_size': 32, 'num_layers': 1, 'dropout': 0.5474136752138244, 'batch_size': 24, 'learning_rate': 0.0003570478920909981, 'weight_decay': 1.4656553886225336e-05, 'label_smoothing': 0.05426980635477918, 'tune_epochs': 54}. Best is trial 2 with value: 0.4954954954954955.\n",
            "[Optuna] Completed Trial 3/20 | Value: 0.4955 | Best so far: 0.4955\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 04/20] START\n",
            "[Trial 04/20] Params: embedding_dim=128, hidden_size=128, num_layers=3, dropout=0.45, batch_size=24, lr=0.002927, weight_decay=7.4e-05, label_smoothing=0.07, tune_epochs=27\n",
            "[Trial 04/20] [Train] epochs=27, batch_size=24, lr=0.002927, patience=10\n",
            "[Trial 04/20] Epoch   1/27 | Train Loss: 1.6702 | Train Acc: 0.2219 | Val Loss: 1.6307 | Val Acc: 0.2117 | LR: 0.002927\n",
            "[Trial 04/20] Epoch   2/27 | Train Loss: 1.6084 | Train Acc: 0.2640 | Val Loss: 1.6002 | Val Acc: 0.2523 | LR: 0.002927\n",
            "[Trial 04/20] Epoch   3/27 | Train Loss: 1.5390 | Train Acc: 0.3275 | Val Loss: 1.6111 | Val Acc: 0.2568 | LR: 0.002927\n",
            "[Trial 04/20] Epoch   4/27 | Train Loss: 1.5015 | Train Acc: 0.3742 | Val Loss: 1.6571 | Val Acc: 0.2523 | LR: 0.002927\n",
            "[Trial 04/20] Epoch   5/27 | Train Loss: 1.4054 | Train Acc: 0.4416 | Val Loss: 1.6915 | Val Acc: 0.2658 | LR: 0.002927\n",
            "[Trial 04/20] Epoch   6/27 | Train Loss: 1.3571 | Train Acc: 0.4601 | Val Loss: 1.7484 | Val Acc: 0.3018 | LR: 0.002927\n",
            "[Trial 04/20] Epoch   7/27 | Train Loss: 1.1247 | Train Acc: 0.6017 | Val Loss: 1.7186 | Val Acc: 0.3784 | LR: 0.002927\n",
            "[Trial 04/20] Epoch   8/27 | Train Loss: 0.9341 | Train Acc: 0.7079 | Val Loss: 1.7773 | Val Acc: 0.3829 | LR: 0.001463 (LR reduced from 0.002927 to 0.001463)\n",
            "[Trial 04/20] Epoch   9/27 | Train Loss: 0.6822 | Train Acc: 0.8275 | Val Loss: 1.7925 | Val Acc: 0.4550 | LR: 0.001463\n",
            "[Trial 04/20] Epoch  10/27 | Train Loss: 0.5635 | Train Acc: 0.8871 | Val Loss: 1.7057 | Val Acc: 0.4865 | LR: 0.001463\n",
            "[Trial 04/20] Epoch  11/27 | Train Loss: 0.4925 | Train Acc: 0.9208 | Val Loss: 1.8134 | Val Acc: 0.5000 | LR: 0.001463\n",
            "[Trial 04/20] Epoch  12/27 | Train Loss: 0.4379 | Train Acc: 0.9472 | Val Loss: 1.9559 | Val Acc: 0.4595 | LR: 0.001463\n",
            "[Trial 04/20] [EarlyStopping] triggered at epoch 12\n",
            "[Trial 04/20] [Info] Loaded best model weights\n",
            "[Trial 04/20] Done | Val Loss: 1.6002 | Val Acc: 0.2523\n",
            "[I 2025-11-10 12:16:07,577] Trial 3 finished with value: 0.25225225225225223 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 3, 'dropout': 0.45342867192380854, 'batch_size': 24, 'learning_rate': 0.002926758115062129, 'weight_decay': 7.411299781083242e-05, 'label_smoothing': 0.06617960497052984, 'tune_epochs': 27}. Best is trial 2 with value: 0.4954954954954955.\n",
            "[Optuna] Completed Trial 4/20 | Value: 0.2523 | Best so far: 0.4955\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 05/20] START\n",
            "[Trial 05/20] Params: embedding_dim=128, hidden_size=64, num_layers=3, dropout=0.35, batch_size=16, lr=0.001206, weight_decay=8.8e-06, label_smoothing=0.10, tune_epochs=57\n",
            "[Trial 05/20] [Train] epochs=57, batch_size=16, lr=0.001206, patience=10\n",
            "[Trial 05/20] Epoch   1/57 | Train Loss: 1.6205 | Train Acc: 0.2185 | Val Loss: 1.5955 | Val Acc: 0.2252 | LR: 0.001206\n",
            "[Trial 05/20] Epoch   2/57 | Train Loss: 1.5650 | Train Acc: 0.3022 | Val Loss: 1.6045 | Val Acc: 0.2523 | LR: 0.001206\n",
            "[Trial 05/20] Epoch   3/57 | Train Loss: 1.4981 | Train Acc: 0.3775 | Val Loss: 1.5740 | Val Acc: 0.3198 | LR: 0.001206\n",
            "[Trial 05/20] Epoch   4/57 | Train Loss: 1.3508 | Train Acc: 0.4848 | Val Loss: 1.5338 | Val Acc: 0.3964 | LR: 0.001206\n",
            "[Trial 05/20] Epoch   5/57 | Train Loss: 1.1761 | Train Acc: 0.5820 | Val Loss: 1.4901 | Val Acc: 0.4414 | LR: 0.001206\n",
            "[Trial 05/20] Epoch   6/57 | Train Loss: 1.0569 | Train Acc: 0.6511 | Val Loss: 1.5048 | Val Acc: 0.4820 | LR: 0.001206\n",
            "[Trial 05/20] Epoch   7/57 | Train Loss: 0.9081 | Train Acc: 0.7461 | Val Loss: 1.5194 | Val Acc: 0.4595 | LR: 0.001206\n",
            "[Trial 05/20] Epoch   8/57 | Train Loss: 0.8119 | Train Acc: 0.8079 | Val Loss: 1.5424 | Val Acc: 0.4820 | LR: 0.001206\n",
            "[Trial 05/20] Epoch   9/57 | Train Loss: 0.7556 | Train Acc: 0.8348 | Val Loss: 1.5043 | Val Acc: 0.4955 | LR: 0.001206\n",
            "[Trial 05/20] Epoch  10/57 | Train Loss: 0.6578 | Train Acc: 0.8933 | Val Loss: 1.5230 | Val Acc: 0.5090 | LR: 0.001206\n",
            "[Trial 05/20] Epoch  11/57 | Train Loss: 0.6170 | Train Acc: 0.9169 | Val Loss: 1.5223 | Val Acc: 0.5135 | LR: 0.000603 (LR reduced from 0.001206 to 0.000603)\n",
            "[Trial 05/20] Epoch  12/57 | Train Loss: 0.5408 | Train Acc: 0.9601 | Val Loss: 1.4384 | Val Acc: 0.5495 | LR: 0.000603\n",
            "[Trial 05/20] Epoch  13/57 | Train Loss: 0.5188 | Train Acc: 0.9691 | Val Loss: 1.4441 | Val Acc: 0.5360 | LR: 0.000603\n",
            "[Trial 05/20] Epoch  14/57 | Train Loss: 0.5005 | Train Acc: 0.9820 | Val Loss: 1.5001 | Val Acc: 0.5450 | LR: 0.000603\n",
            "[Trial 05/20] Epoch  15/57 | Train Loss: 0.4925 | Train Acc: 0.9837 | Val Loss: 1.5154 | Val Acc: 0.5405 | LR: 0.000603\n",
            "[Trial 05/20] Epoch  16/57 | Train Loss: 0.4819 | Train Acc: 0.9848 | Val Loss: 1.4950 | Val Acc: 0.5676 | LR: 0.000603\n",
            "[Trial 05/20] Epoch  17/57 | Train Loss: 0.4774 | Train Acc: 0.9854 | Val Loss: 1.5065 | Val Acc: 0.5405 | LR: 0.000603\n",
            "[Trial 05/20] Epoch  18/57 | Train Loss: 0.4743 | Train Acc: 0.9860 | Val Loss: 1.4504 | Val Acc: 0.5495 | LR: 0.000301 (LR reduced from 0.000603 to 0.000301)\n",
            "[Trial 05/20] Epoch  19/57 | Train Loss: 0.4623 | Train Acc: 0.9921 | Val Loss: 1.4644 | Val Acc: 0.5450 | LR: 0.000301\n",
            "[Trial 05/20] Epoch  20/57 | Train Loss: 0.4561 | Train Acc: 0.9949 | Val Loss: 1.4452 | Val Acc: 0.5676 | LR: 0.000301\n",
            "[Trial 05/20] Epoch  21/57 | Train Loss: 0.4495 | Train Acc: 0.9972 | Val Loss: 1.4449 | Val Acc: 0.5586 | LR: 0.000301\n",
            "[Trial 05/20] Epoch  22/57 | Train Loss: 0.4501 | Train Acc: 0.9961 | Val Loss: 1.4808 | Val Acc: 0.5405 | LR: 0.000301\n",
            "[Trial 05/20] [EarlyStopping] triggered at epoch 22\n",
            "[Trial 05/20] [Info] Loaded best model weights\n",
            "[Trial 05/20] Done | Val Loss: 1.4384 | Val Acc: 0.5495\n",
            "[I 2025-11-10 12:16:16,796] Trial 4 finished with value: 0.5495495495495496 and parameters: {'embedding_dim': 128, 'hidden_size': 64, 'num_layers': 3, 'dropout': 0.34689779818219535, 'batch_size': 16, 'learning_rate': 0.0012057081573389123, 'weight_decay': 8.771380343280564e-06, 'label_smoothing': 0.10171413823294057, 'tune_epochs': 57}. Best is trial 4 with value: 0.5495495495495496.\n",
            "[Optuna] Completed Trial 5/20 | Value: 0.5495 | Best so far: 0.5495\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 06/20] START\n",
            "[Trial 06/20] Params: embedding_dim=128, hidden_size=160, num_layers=3, dropout=0.50, batch_size=64, lr=0.000347, weight_decay=2.1e-06, label_smoothing=0.05, tune_epochs=40\n",
            "[Trial 06/20] [Train] epochs=40, batch_size=64, lr=0.000347, patience=10\n",
            "[Trial 06/20] Epoch   1/40 | Train Loss: 1.6347 | Train Acc: 0.2056 | Val Loss: 1.6085 | Val Acc: 0.2162 | LR: 0.000347\n",
            "[Trial 06/20] Epoch   2/40 | Train Loss: 1.6123 | Train Acc: 0.2236 | Val Loss: 1.6003 | Val Acc: 0.2297 | LR: 0.000347\n",
            "[Trial 06/20] Epoch   3/40 | Train Loss: 1.5926 | Train Acc: 0.2590 | Val Loss: 1.5928 | Val Acc: 0.2568 | LR: 0.000347\n",
            "[Trial 06/20] Epoch   4/40 | Train Loss: 1.5548 | Train Acc: 0.3140 | Val Loss: 1.5826 | Val Acc: 0.2703 | LR: 0.000347\n",
            "[Trial 06/20] Epoch   5/40 | Train Loss: 1.5281 | Train Acc: 0.3275 | Val Loss: 1.5773 | Val Acc: 0.2883 | LR: 0.000347\n",
            "[Trial 06/20] Epoch   6/40 | Train Loss: 1.5014 | Train Acc: 0.3579 | Val Loss: 1.5753 | Val Acc: 0.2838 | LR: 0.000347\n",
            "[Trial 06/20] Epoch   7/40 | Train Loss: 1.4549 | Train Acc: 0.3831 | Val Loss: 1.5704 | Val Acc: 0.3288 | LR: 0.000347\n",
            "[Trial 06/20] Epoch   8/40 | Train Loss: 1.3883 | Train Acc: 0.4236 | Val Loss: 1.5918 | Val Acc: 0.3153 | LR: 0.000347\n",
            "[Trial 06/20] Epoch   9/40 | Train Loss: 1.3072 | Train Acc: 0.4961 | Val Loss: 1.6112 | Val Acc: 0.3378 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  10/40 | Train Loss: 1.2312 | Train Acc: 0.5315 | Val Loss: 1.5226 | Val Acc: 0.4234 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  11/40 | Train Loss: 1.1476 | Train Acc: 0.5815 | Val Loss: 1.4454 | Val Acc: 0.4279 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  12/40 | Train Loss: 1.0514 | Train Acc: 0.6253 | Val Loss: 1.4067 | Val Acc: 0.4910 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  13/40 | Train Loss: 0.9899 | Train Acc: 0.6348 | Val Loss: 1.3724 | Val Acc: 0.5045 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  14/40 | Train Loss: 0.8760 | Train Acc: 0.7180 | Val Loss: 1.4377 | Val Acc: 0.4865 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  15/40 | Train Loss: 0.8408 | Train Acc: 0.7258 | Val Loss: 1.3718 | Val Acc: 0.5180 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  16/40 | Train Loss: 0.7986 | Train Acc: 0.7551 | Val Loss: 1.3860 | Val Acc: 0.5450 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  17/40 | Train Loss: 0.7301 | Train Acc: 0.7837 | Val Loss: 1.4408 | Val Acc: 0.5225 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  18/40 | Train Loss: 0.6947 | Train Acc: 0.7921 | Val Loss: 1.4050 | Val Acc: 0.6036 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  19/40 | Train Loss: 0.6589 | Train Acc: 0.8247 | Val Loss: 1.4290 | Val Acc: 0.5586 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  20/40 | Train Loss: 0.6377 | Train Acc: 0.8298 | Val Loss: 1.4179 | Val Acc: 0.5721 | LR: 0.000347\n",
            "[Trial 06/20] Epoch  21/40 | Train Loss: 0.5743 | Train Acc: 0.8584 | Val Loss: 1.5234 | Val Acc: 0.5811 | LR: 0.000173 (LR reduced from 0.000347 to 0.000173)\n",
            "[Trial 06/20] Epoch  22/40 | Train Loss: 0.5222 | Train Acc: 0.8809 | Val Loss: 1.4529 | Val Acc: 0.6081 | LR: 0.000173\n",
            "[Trial 06/20] Epoch  23/40 | Train Loss: 0.4777 | Train Acc: 0.9112 | Val Loss: 1.4505 | Val Acc: 0.6216 | LR: 0.000173\n",
            "[Trial 06/20] Epoch  24/40 | Train Loss: 0.4734 | Train Acc: 0.9101 | Val Loss: 1.4800 | Val Acc: 0.6036 | LR: 0.000173\n",
            "[Trial 06/20] Epoch  25/40 | Train Loss: 0.4506 | Train Acc: 0.9140 | Val Loss: 1.5271 | Val Acc: 0.5946 | LR: 0.000173\n",
            "[Trial 06/20] [EarlyStopping] triggered at epoch 25\n",
            "[Trial 06/20] [Info] Loaded best model weights\n",
            "[Trial 06/20] Done | Val Loss: 1.3718 | Val Acc: 0.5180\n",
            "[I 2025-11-10 12:16:19,820] Trial 5 finished with value: 0.5180180180180181 and parameters: {'embedding_dim': 128, 'hidden_size': 160, 'num_layers': 3, 'dropout': 0.5018360384495573, 'batch_size': 64, 'learning_rate': 0.00034695916603302916, 'weight_decay': 2.138729075414893e-06, 'label_smoothing': 0.04558703250838834, 'tune_epochs': 40}. Best is trial 4 with value: 0.5495495495495496.\n",
            "[Optuna] Completed Trial 6/20 | Value: 0.5180 | Best so far: 0.5495\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 07/20] START\n",
            "[Trial 07/20] Params: embedding_dim=64, hidden_size=32, num_layers=1, dropout=0.45, batch_size=24, lr=0.000324, weight_decay=7.2e-06, label_smoothing=0.01, tune_epochs=46\n",
            "[Trial 07/20] [Train] epochs=46, batch_size=24, lr=0.000324, patience=10\n",
            "[Trial 07/20] Epoch   1/46 | Train Loss: 1.6849 | Train Acc: 0.2135 | Val Loss: 1.6251 | Val Acc: 0.2117 | LR: 0.000324\n",
            "[Trial 07/20] Epoch   2/46 | Train Loss: 1.6567 | Train Acc: 0.2180 | Val Loss: 1.6191 | Val Acc: 0.2162 | LR: 0.000324\n",
            "[Trial 07/20] Epoch   3/46 | Train Loss: 1.6403 | Train Acc: 0.2315 | Val Loss: 1.6158 | Val Acc: 0.1982 | LR: 0.000324\n",
            "[Trial 07/20] Epoch   4/46 | Train Loss: 1.6255 | Train Acc: 0.2292 | Val Loss: 1.6132 | Val Acc: 0.2207 | LR: 0.000324\n",
            "[Trial 07/20] Epoch   5/46 | Train Loss: 1.6113 | Train Acc: 0.2455 | Val Loss: 1.6120 | Val Acc: 0.2117 | LR: 0.000324\n",
            "[Trial 07/20] Epoch   6/46 | Train Loss: 1.6030 | Train Acc: 0.2556 | Val Loss: 1.6093 | Val Acc: 0.2117 | LR: 0.000324\n",
            "[Trial 07/20] Epoch   7/46 | Train Loss: 1.5977 | Train Acc: 0.2669 | Val Loss: 1.6074 | Val Acc: 0.1982 | LR: 0.000324\n",
            "[Trial 07/20] Epoch   8/46 | Train Loss: 1.5779 | Train Acc: 0.2702 | Val Loss: 1.6069 | Val Acc: 0.2117 | LR: 0.000324\n",
            "[Trial 07/20] Epoch   9/46 | Train Loss: 1.5595 | Train Acc: 0.3017 | Val Loss: 1.6078 | Val Acc: 0.2207 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  10/46 | Train Loss: 1.5627 | Train Acc: 0.2949 | Val Loss: 1.6063 | Val Acc: 0.2117 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  11/46 | Train Loss: 1.5485 | Train Acc: 0.3174 | Val Loss: 1.6051 | Val Acc: 0.2297 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  12/46 | Train Loss: 1.5400 | Train Acc: 0.3146 | Val Loss: 1.6039 | Val Acc: 0.2297 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  13/46 | Train Loss: 1.5307 | Train Acc: 0.3287 | Val Loss: 1.6031 | Val Acc: 0.2162 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  14/46 | Train Loss: 1.5203 | Train Acc: 0.3213 | Val Loss: 1.6040 | Val Acc: 0.2207 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  15/46 | Train Loss: 1.5091 | Train Acc: 0.3438 | Val Loss: 1.6044 | Val Acc: 0.2207 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  16/46 | Train Loss: 1.4994 | Train Acc: 0.3567 | Val Loss: 1.6016 | Val Acc: 0.2342 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  17/46 | Train Loss: 1.4867 | Train Acc: 0.3674 | Val Loss: 1.6010 | Val Acc: 0.2523 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  18/46 | Train Loss: 1.4656 | Train Acc: 0.3955 | Val Loss: 1.5976 | Val Acc: 0.2342 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  19/46 | Train Loss: 1.4572 | Train Acc: 0.3820 | Val Loss: 1.5962 | Val Acc: 0.2162 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  20/46 | Train Loss: 1.4460 | Train Acc: 0.4034 | Val Loss: 1.5915 | Val Acc: 0.2477 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  21/46 | Train Loss: 1.4130 | Train Acc: 0.4185 | Val Loss: 1.5878 | Val Acc: 0.2523 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  22/46 | Train Loss: 1.4109 | Train Acc: 0.4146 | Val Loss: 1.5835 | Val Acc: 0.2523 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  23/46 | Train Loss: 1.3723 | Train Acc: 0.4455 | Val Loss: 1.5813 | Val Acc: 0.2748 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  24/46 | Train Loss: 1.3619 | Train Acc: 0.4404 | Val Loss: 1.5732 | Val Acc: 0.2973 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  25/46 | Train Loss: 1.3120 | Train Acc: 0.4713 | Val Loss: 1.5688 | Val Acc: 0.3333 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  26/46 | Train Loss: 1.2673 | Train Acc: 0.4933 | Val Loss: 1.5483 | Val Acc: 0.3559 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  27/46 | Train Loss: 1.2143 | Train Acc: 0.5404 | Val Loss: 1.5096 | Val Acc: 0.3784 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  28/46 | Train Loss: 1.1528 | Train Acc: 0.5483 | Val Loss: 1.4889 | Val Acc: 0.3784 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  29/46 | Train Loss: 1.0928 | Train Acc: 0.5848 | Val Loss: 1.4562 | Val Acc: 0.4054 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  30/46 | Train Loss: 1.0634 | Train Acc: 0.5910 | Val Loss: 1.4626 | Val Acc: 0.3829 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  31/46 | Train Loss: 1.0043 | Train Acc: 0.6096 | Val Loss: 1.4649 | Val Acc: 0.4279 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  32/46 | Train Loss: 0.9825 | Train Acc: 0.6090 | Val Loss: 1.4614 | Val Acc: 0.4369 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  33/46 | Train Loss: 0.9616 | Train Acc: 0.6399 | Val Loss: 1.4339 | Val Acc: 0.4459 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  34/46 | Train Loss: 0.8994 | Train Acc: 0.6702 | Val Loss: 1.4508 | Val Acc: 0.4595 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  35/46 | Train Loss: 0.8562 | Train Acc: 0.6764 | Val Loss: 1.4263 | Val Acc: 0.4640 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  36/46 | Train Loss: 0.8338 | Train Acc: 0.6994 | Val Loss: 1.4140 | Val Acc: 0.4550 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  37/46 | Train Loss: 0.7997 | Train Acc: 0.7163 | Val Loss: 1.4304 | Val Acc: 0.4730 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  38/46 | Train Loss: 0.7765 | Train Acc: 0.7219 | Val Loss: 1.4399 | Val Acc: 0.4775 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  39/46 | Train Loss: 0.7530 | Train Acc: 0.7331 | Val Loss: 1.3832 | Val Acc: 0.5090 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  40/46 | Train Loss: 0.7011 | Train Acc: 0.7697 | Val Loss: 1.4028 | Val Acc: 0.4955 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  41/46 | Train Loss: 0.6879 | Train Acc: 0.7680 | Val Loss: 1.3672 | Val Acc: 0.5090 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  42/46 | Train Loss: 0.6602 | Train Acc: 0.7798 | Val Loss: 1.4036 | Val Acc: 0.5315 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  43/46 | Train Loss: 0.6298 | Train Acc: 0.8000 | Val Loss: 1.3560 | Val Acc: 0.5360 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  44/46 | Train Loss: 0.5814 | Train Acc: 0.8213 | Val Loss: 1.3794 | Val Acc: 0.5450 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  45/46 | Train Loss: 0.5646 | Train Acc: 0.8309 | Val Loss: 1.3613 | Val Acc: 0.5586 | LR: 0.000324\n",
            "[Trial 07/20] Epoch  46/46 | Train Loss: 0.5507 | Train Acc: 0.8360 | Val Loss: 1.3408 | Val Acc: 0.5856 | LR: 0.000324\n",
            "[Trial 07/20] [Info] Loaded best model weights\n",
            "[Trial 07/20] Done | Val Loss: 1.3408 | Val Acc: 0.5856\n",
            "[I 2025-11-10 12:16:30,937] Trial 6 finished with value: 0.5855855855855856 and parameters: {'embedding_dim': 64, 'hidden_size': 32, 'num_layers': 1, 'dropout': 0.4515094794475889, 'batch_size': 24, 'learning_rate': 0.000324476480988986, 'weight_decay': 7.153547794693157e-06, 'label_smoothing': 0.007377389470906559, 'tune_epochs': 46}. Best is trial 6 with value: 0.5855855855855856.\n",
            "[Optuna] Completed Trial 7/20 | Value: 0.5856 | Best so far: 0.5856\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 08/20] START\n",
            "[Trial 08/20] Params: embedding_dim=32, hidden_size=160, num_layers=3, dropout=0.22, batch_size=16, lr=0.000142, weight_decay=3.2e-04, label_smoothing=0.06, tune_epochs=31\n",
            "[Trial 08/20] [Train] epochs=31, batch_size=16, lr=0.000142, patience=10\n",
            "[Trial 08/20] Epoch   1/31 | Train Loss: 1.6087 | Train Acc: 0.2247 | Val Loss: 1.6045 | Val Acc: 0.2568 | LR: 0.000142\n",
            "[Trial 08/20] Epoch   2/31 | Train Loss: 1.5821 | Train Acc: 0.2747 | Val Loss: 1.6063 | Val Acc: 0.2387 | LR: 0.000142\n",
            "[Trial 08/20] Epoch   3/31 | Train Loss: 1.5513 | Train Acc: 0.3163 | Val Loss: 1.6052 | Val Acc: 0.2658 | LR: 0.000142\n",
            "[Trial 08/20] Epoch   4/31 | Train Loss: 1.5184 | Train Acc: 0.3500 | Val Loss: 1.6150 | Val Acc: 0.2613 | LR: 0.000142\n",
            "[Trial 08/20] Epoch   5/31 | Train Loss: 1.4777 | Train Acc: 0.3691 | Val Loss: 1.5968 | Val Acc: 0.3018 | LR: 0.000142\n",
            "[Trial 08/20] Epoch   6/31 | Train Loss: 1.4378 | Train Acc: 0.4051 | Val Loss: 1.6049 | Val Acc: 0.2973 | LR: 0.000142\n",
            "[Trial 08/20] Epoch   7/31 | Train Loss: 1.4153 | Train Acc: 0.4118 | Val Loss: 1.5949 | Val Acc: 0.3333 | LR: 0.000142\n",
            "[Trial 08/20] Epoch   8/31 | Train Loss: 1.3587 | Train Acc: 0.4629 | Val Loss: 1.6083 | Val Acc: 0.3378 | LR: 0.000142\n",
            "[Trial 08/20] Epoch   9/31 | Train Loss: 1.3272 | Train Acc: 0.4770 | Val Loss: 1.6456 | Val Acc: 0.3243 | LR: 0.000142\n",
            "[Trial 08/20] Epoch  10/31 | Train Loss: 1.2953 | Train Acc: 0.4882 | Val Loss: 1.6245 | Val Acc: 0.3423 | LR: 0.000142\n",
            "[Trial 08/20] Epoch  11/31 | Train Loss: 1.2741 | Train Acc: 0.5051 | Val Loss: 1.6614 | Val Acc: 0.3153 | LR: 0.000142\n",
            "[Trial 08/20] Epoch  12/31 | Train Loss: 1.2525 | Train Acc: 0.5225 | Val Loss: 1.6501 | Val Acc: 0.3288 | LR: 0.000142\n",
            "[Trial 08/20] Epoch  13/31 | Train Loss: 1.2107 | Train Acc: 0.5522 | Val Loss: 1.6331 | Val Acc: 0.3378 | LR: 0.000071 (LR reduced from 0.000142 to 0.000071)\n",
            "[Trial 08/20] Epoch  14/31 | Train Loss: 1.1614 | Train Acc: 0.5747 | Val Loss: 1.6407 | Val Acc: 0.3514 | LR: 0.000071\n",
            "[Trial 08/20] Epoch  15/31 | Train Loss: 1.1438 | Train Acc: 0.5848 | Val Loss: 1.6231 | Val Acc: 0.3604 | LR: 0.000071\n",
            "[Trial 08/20] Epoch  16/31 | Train Loss: 1.1191 | Train Acc: 0.6067 | Val Loss: 1.6473 | Val Acc: 0.3514 | LR: 0.000071\n",
            "[Trial 08/20] Epoch  17/31 | Train Loss: 1.1071 | Train Acc: 0.6045 | Val Loss: 1.6853 | Val Acc: 0.3514 | LR: 0.000071\n",
            "[Trial 08/20] [EarlyStopping] triggered at epoch 17\n",
            "[Trial 08/20] [Info] Loaded best model weights\n",
            "[Trial 08/20] Done | Val Loss: 1.5949 | Val Acc: 0.3333\n",
            "[I 2025-11-10 12:16:38,058] Trial 7 finished with value: 0.3333333333333333 and parameters: {'embedding_dim': 32, 'hidden_size': 160, 'num_layers': 3, 'dropout': 0.21881877199619984, 'batch_size': 16, 'learning_rate': 0.0001423638128723575, 'weight_decay': 0.00032055863990707473, 'label_smoothing': 0.06415601299434717, 'tune_epochs': 31}. Best is trial 6 with value: 0.5855855855855856.\n",
            "[Optuna] Completed Trial 8/20 | Value: 0.3333 | Best so far: 0.5856\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 09/20] START\n",
            "[Trial 09/20] Params: embedding_dim=128, hidden_size=128, num_layers=3, dropout=0.17, batch_size=32, lr=0.001322, weight_decay=2.8e-04, label_smoothing=0.11, tune_epochs=44\n",
            "[Trial 09/20] [Train] epochs=44, batch_size=32, lr=0.001322, patience=10\n",
            "[Trial 09/20] Epoch   1/44 | Train Loss: 1.6222 | Train Acc: 0.2208 | Val Loss: 1.6023 | Val Acc: 0.2387 | LR: 0.001322\n",
            "[Trial 09/20] Epoch   2/44 | Train Loss: 1.5117 | Train Acc: 0.3640 | Val Loss: 1.6424 | Val Acc: 0.2703 | LR: 0.001322\n",
            "[Trial 09/20] Epoch   3/44 | Train Loss: 1.4009 | Train Acc: 0.4365 | Val Loss: 1.6717 | Val Acc: 0.2793 | LR: 0.001322\n",
            "[Trial 09/20] Epoch   4/44 | Train Loss: 1.2494 | Train Acc: 0.5573 | Val Loss: 1.6783 | Val Acc: 0.3514 | LR: 0.001322\n",
            "[Trial 09/20] Epoch   5/44 | Train Loss: 1.0857 | Train Acc: 0.6489 | Val Loss: 1.8325 | Val Acc: 0.3153 | LR: 0.001322\n",
            "[Trial 09/20] Epoch   6/44 | Train Loss: 0.9801 | Train Acc: 0.7163 | Val Loss: 1.8392 | Val Acc: 0.3333 | LR: 0.001322\n",
            "[Trial 09/20] Epoch   7/44 | Train Loss: 0.8346 | Train Acc: 0.8062 | Val Loss: 2.0416 | Val Acc: 0.3514 | LR: 0.000661 (LR reduced from 0.001322 to 0.000661)\n",
            "[Trial 09/20] Epoch   8/44 | Train Loss: 0.6718 | Train Acc: 0.9006 | Val Loss: 1.8665 | Val Acc: 0.4234 | LR: 0.000661\n",
            "[Trial 09/20] Epoch   9/44 | Train Loss: 0.6041 | Train Acc: 0.9360 | Val Loss: 1.8704 | Val Acc: 0.4099 | LR: 0.000661\n",
            "[Trial 09/20] Epoch  10/44 | Train Loss: 0.5573 | Train Acc: 0.9657 | Val Loss: 1.8645 | Val Acc: 0.4414 | LR: 0.000661\n",
            "[Trial 09/20] Epoch  11/44 | Train Loss: 0.5378 | Train Acc: 0.9803 | Val Loss: 1.8803 | Val Acc: 0.4505 | LR: 0.000661\n",
            "[Trial 09/20] [EarlyStopping] triggered at epoch 11\n",
            "[Trial 09/20] [Info] Loaded best model weights\n",
            "[Trial 09/20] Done | Val Loss: 1.6023 | Val Acc: 0.2387\n",
            "[I 2025-11-10 12:16:40,315] Trial 8 finished with value: 0.23873873873873874 and parameters: {'embedding_dim': 128, 'hidden_size': 128, 'num_layers': 3, 'dropout': 0.16876047207299663, 'batch_size': 32, 'learning_rate': 0.0013221876538695291, 'weight_decay': 0.0002829219225536188, 'label_smoothing': 0.11104016231989247, 'tune_epochs': 44}. Best is trial 6 with value: 0.5855855855855856.\n",
            "[Optuna] Completed Trial 9/20 | Value: 0.2387 | Best so far: 0.5856\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 10/20] START\n",
            "[Trial 10/20] Params: embedding_dim=128, hidden_size=32, num_layers=1, dropout=0.42, batch_size=32, lr=0.000149, weight_decay=9.8e-05, label_smoothing=0.00, tune_epochs=30\n",
            "[Trial 10/20] [Train] epochs=30, batch_size=32, lr=0.000149, patience=10\n",
            "[Trial 10/20] Epoch   1/30 | Train Loss: 1.6968 | Train Acc: 0.2034 | Val Loss: 1.6769 | Val Acc: 0.2162 | LR: 0.000149\n",
            "[Trial 10/20] Epoch   2/30 | Train Loss: 1.6927 | Train Acc: 0.1927 | Val Loss: 1.6717 | Val Acc: 0.2117 | LR: 0.000149\n",
            "[Trial 10/20] Epoch   3/30 | Train Loss: 1.6668 | Train Acc: 0.2169 | Val Loss: 1.6664 | Val Acc: 0.1982 | LR: 0.000149\n",
            "[Trial 10/20] Epoch   4/30 | Train Loss: 1.6504 | Train Acc: 0.2376 | Val Loss: 1.6626 | Val Acc: 0.1892 | LR: 0.000149\n",
            "[Trial 10/20] Epoch   5/30 | Train Loss: 1.6541 | Train Acc: 0.2320 | Val Loss: 1.6585 | Val Acc: 0.2072 | LR: 0.000149\n",
            "[Trial 10/20] Epoch   6/30 | Train Loss: 1.6500 | Train Acc: 0.2208 | Val Loss: 1.6557 | Val Acc: 0.2117 | LR: 0.000149\n",
            "[Trial 10/20] Epoch   7/30 | Train Loss: 1.6294 | Train Acc: 0.2275 | Val Loss: 1.6530 | Val Acc: 0.2072 | LR: 0.000149\n",
            "[Trial 10/20] Epoch   8/30 | Train Loss: 1.6204 | Train Acc: 0.2483 | Val Loss: 1.6498 | Val Acc: 0.2162 | LR: 0.000149\n",
            "[Trial 10/20] Epoch   9/30 | Train Loss: 1.6251 | Train Acc: 0.2298 | Val Loss: 1.6468 | Val Acc: 0.2027 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  10/30 | Train Loss: 1.6223 | Train Acc: 0.2511 | Val Loss: 1.6443 | Val Acc: 0.2072 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  11/30 | Train Loss: 1.6112 | Train Acc: 0.2612 | Val Loss: 1.6430 | Val Acc: 0.2027 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  12/30 | Train Loss: 1.6021 | Train Acc: 0.2596 | Val Loss: 1.6399 | Val Acc: 0.1892 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  13/30 | Train Loss: 1.5843 | Train Acc: 0.2770 | Val Loss: 1.6378 | Val Acc: 0.1892 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  14/30 | Train Loss: 1.5949 | Train Acc: 0.2680 | Val Loss: 1.6352 | Val Acc: 0.1982 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  15/30 | Train Loss: 1.5818 | Train Acc: 0.2730 | Val Loss: 1.6335 | Val Acc: 0.1892 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  16/30 | Train Loss: 1.5764 | Train Acc: 0.2674 | Val Loss: 1.6322 | Val Acc: 0.2072 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  17/30 | Train Loss: 1.5687 | Train Acc: 0.2860 | Val Loss: 1.6314 | Val Acc: 0.2252 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  18/30 | Train Loss: 1.5686 | Train Acc: 0.2770 | Val Loss: 1.6296 | Val Acc: 0.2162 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  19/30 | Train Loss: 1.5598 | Train Acc: 0.3067 | Val Loss: 1.6275 | Val Acc: 0.2252 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  20/30 | Train Loss: 1.5546 | Train Acc: 0.3073 | Val Loss: 1.6264 | Val Acc: 0.2342 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  21/30 | Train Loss: 1.5419 | Train Acc: 0.3022 | Val Loss: 1.6240 | Val Acc: 0.2297 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  22/30 | Train Loss: 1.5477 | Train Acc: 0.3073 | Val Loss: 1.6234 | Val Acc: 0.2252 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  23/30 | Train Loss: 1.5331 | Train Acc: 0.3129 | Val Loss: 1.6220 | Val Acc: 0.2207 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  24/30 | Train Loss: 1.5297 | Train Acc: 0.3197 | Val Loss: 1.6205 | Val Acc: 0.2207 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  25/30 | Train Loss: 1.5347 | Train Acc: 0.3197 | Val Loss: 1.6204 | Val Acc: 0.2117 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  26/30 | Train Loss: 1.5196 | Train Acc: 0.3354 | Val Loss: 1.6195 | Val Acc: 0.2342 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  27/30 | Train Loss: 1.5161 | Train Acc: 0.3360 | Val Loss: 1.6185 | Val Acc: 0.2297 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  28/30 | Train Loss: 1.5045 | Train Acc: 0.3562 | Val Loss: 1.6193 | Val Acc: 0.2342 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  29/30 | Train Loss: 1.5007 | Train Acc: 0.3489 | Val Loss: 1.6179 | Val Acc: 0.2477 | LR: 0.000149\n",
            "[Trial 10/20] Epoch  30/30 | Train Loss: 1.4894 | Train Acc: 0.3803 | Val Loss: 1.6185 | Val Acc: 0.2387 | LR: 0.000149\n",
            "[Trial 10/20] [Info] Loaded best model weights\n",
            "[Trial 10/20] Done | Val Loss: 1.6179 | Val Acc: 0.2477\n",
            "[I 2025-11-10 12:16:45,653] Trial 9 finished with value: 0.24774774774774774 and parameters: {'embedding_dim': 128, 'hidden_size': 32, 'num_layers': 1, 'dropout': 0.4210158230771439, 'batch_size': 32, 'learning_rate': 0.00014872949647124203, 'weight_decay': 9.783749110062347e-05, 'label_smoothing': 0.0010123167692437374, 'tune_epochs': 30}. Best is trial 6 with value: 0.5855855855855856.\n",
            "[Optuna] Completed Trial 10/20 | Value: 0.2477 | Best so far: 0.5856\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 11/20] START\n",
            "[Trial 11/20] Params: embedding_dim=64, hidden_size=96, num_layers=1, dropout=0.31, batch_size=48, lr=0.000324, weight_decay=1.1e-06, label_smoothing=0.17, tune_epochs=51\n",
            "[Trial 11/20] [Train] epochs=51, batch_size=48, lr=0.000324, patience=10\n",
            "[Trial 11/20] Epoch   1/51 | Train Loss: 1.6396 | Train Acc: 0.2090 | Val Loss: 1.6232 | Val Acc: 0.1982 | LR: 0.000324\n",
            "[Trial 11/20] Epoch   2/51 | Train Loss: 1.6104 | Train Acc: 0.2500 | Val Loss: 1.6192 | Val Acc: 0.2342 | LR: 0.000324\n",
            "[Trial 11/20] Epoch   3/51 | Train Loss: 1.5904 | Train Acc: 0.2663 | Val Loss: 1.6181 | Val Acc: 0.2162 | LR: 0.000324\n",
            "[Trial 11/20] Epoch   4/51 | Train Loss: 1.5783 | Train Acc: 0.2927 | Val Loss: 1.6166 | Val Acc: 0.2027 | LR: 0.000324\n",
            "[Trial 11/20] Epoch   5/51 | Train Loss: 1.5668 | Train Acc: 0.3039 | Val Loss: 1.6202 | Val Acc: 0.2207 | LR: 0.000324\n",
            "[Trial 11/20] Epoch   6/51 | Train Loss: 1.5511 | Train Acc: 0.3225 | Val Loss: 1.6192 | Val Acc: 0.2207 | LR: 0.000324\n",
            "[Trial 11/20] Epoch   7/51 | Train Loss: 1.5411 | Train Acc: 0.3433 | Val Loss: 1.6196 | Val Acc: 0.2477 | LR: 0.000324\n",
            "[Trial 11/20] Epoch   8/51 | Train Loss: 1.5224 | Train Acc: 0.3702 | Val Loss: 1.6207 | Val Acc: 0.2432 | LR: 0.000324\n",
            "[Trial 11/20] Epoch   9/51 | Train Loss: 1.5057 | Train Acc: 0.3848 | Val Loss: 1.6215 | Val Acc: 0.2523 | LR: 0.000324\n",
            "[Trial 11/20] Epoch  10/51 | Train Loss: 1.4940 | Train Acc: 0.3944 | Val Loss: 1.6274 | Val Acc: 0.2523 | LR: 0.000162 (LR reduced from 0.000324 to 0.000162)\n",
            "[Trial 11/20] Epoch  11/51 | Train Loss: 1.4677 | Train Acc: 0.4388 | Val Loss: 1.6281 | Val Acc: 0.2477 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  12/51 | Train Loss: 1.4577 | Train Acc: 0.4433 | Val Loss: 1.6310 | Val Acc: 0.2568 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  13/51 | Train Loss: 1.4387 | Train Acc: 0.4680 | Val Loss: 1.6365 | Val Acc: 0.2748 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  14/51 | Train Loss: 1.4101 | Train Acc: 0.4792 | Val Loss: 1.6104 | Val Acc: 0.2838 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  15/51 | Train Loss: 1.3713 | Train Acc: 0.5079 | Val Loss: 1.5884 | Val Acc: 0.2973 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  16/51 | Train Loss: 1.3380 | Train Acc: 0.5376 | Val Loss: 1.5784 | Val Acc: 0.3423 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  17/51 | Train Loss: 1.3175 | Train Acc: 0.5607 | Val Loss: 1.5771 | Val Acc: 0.3559 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  18/51 | Train Loss: 1.2972 | Train Acc: 0.5758 | Val Loss: 1.5614 | Val Acc: 0.3649 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  19/51 | Train Loss: 1.2712 | Train Acc: 0.5854 | Val Loss: 1.5633 | Val Acc: 0.3694 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  20/51 | Train Loss: 1.2530 | Train Acc: 0.5904 | Val Loss: 1.5509 | Val Acc: 0.3829 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  21/51 | Train Loss: 1.2331 | Train Acc: 0.6101 | Val Loss: 1.5569 | Val Acc: 0.3874 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  22/51 | Train Loss: 1.2154 | Train Acc: 0.6309 | Val Loss: 1.5411 | Val Acc: 0.3964 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  23/51 | Train Loss: 1.1899 | Train Acc: 0.6410 | Val Loss: 1.5271 | Val Acc: 0.4054 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  24/51 | Train Loss: 1.1721 | Train Acc: 0.6562 | Val Loss: 1.5345 | Val Acc: 0.3964 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  25/51 | Train Loss: 1.1608 | Train Acc: 0.6556 | Val Loss: 1.5162 | Val Acc: 0.4189 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  26/51 | Train Loss: 1.1473 | Train Acc: 0.6848 | Val Loss: 1.5228 | Val Acc: 0.3964 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  27/51 | Train Loss: 1.1325 | Train Acc: 0.6826 | Val Loss: 1.5194 | Val Acc: 0.4189 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  28/51 | Train Loss: 1.1168 | Train Acc: 0.6944 | Val Loss: 1.5151 | Val Acc: 0.4279 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  29/51 | Train Loss: 1.0999 | Train Acc: 0.7180 | Val Loss: 1.5146 | Val Acc: 0.4234 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  30/51 | Train Loss: 1.0924 | Train Acc: 0.7202 | Val Loss: 1.5167 | Val Acc: 0.4054 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  31/51 | Train Loss: 1.0701 | Train Acc: 0.7466 | Val Loss: 1.5126 | Val Acc: 0.4189 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  32/51 | Train Loss: 1.0694 | Train Acc: 0.7404 | Val Loss: 1.5144 | Val Acc: 0.4324 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  33/51 | Train Loss: 1.0635 | Train Acc: 0.7567 | Val Loss: 1.5170 | Val Acc: 0.4324 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  34/51 | Train Loss: 1.0456 | Train Acc: 0.7573 | Val Loss: 1.5135 | Val Acc: 0.4414 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  35/51 | Train Loss: 1.0379 | Train Acc: 0.7601 | Val Loss: 1.5029 | Val Acc: 0.4505 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  36/51 | Train Loss: 1.0188 | Train Acc: 0.7725 | Val Loss: 1.4946 | Val Acc: 0.4550 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  37/51 | Train Loss: 1.0070 | Train Acc: 0.7820 | Val Loss: 1.5001 | Val Acc: 0.4595 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  38/51 | Train Loss: 1.0037 | Train Acc: 0.7854 | Val Loss: 1.5006 | Val Acc: 0.4640 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  39/51 | Train Loss: 0.9905 | Train Acc: 0.8022 | Val Loss: 1.4794 | Val Acc: 0.4730 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  40/51 | Train Loss: 0.9850 | Train Acc: 0.8107 | Val Loss: 1.4888 | Val Acc: 0.4775 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  41/51 | Train Loss: 0.9708 | Train Acc: 0.8135 | Val Loss: 1.4723 | Val Acc: 0.4955 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  42/51 | Train Loss: 0.9562 | Train Acc: 0.8242 | Val Loss: 1.4684 | Val Acc: 0.4820 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  43/51 | Train Loss: 0.9470 | Train Acc: 0.8242 | Val Loss: 1.4578 | Val Acc: 0.4775 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  44/51 | Train Loss: 0.9331 | Train Acc: 0.8416 | Val Loss: 1.4627 | Val Acc: 0.5135 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  45/51 | Train Loss: 0.9284 | Train Acc: 0.8376 | Val Loss: 1.4754 | Val Acc: 0.5000 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  46/51 | Train Loss: 0.9173 | Train Acc: 0.8466 | Val Loss: 1.4478 | Val Acc: 0.5000 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  47/51 | Train Loss: 0.9081 | Train Acc: 0.8376 | Val Loss: 1.4562 | Val Acc: 0.5000 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  48/51 | Train Loss: 0.8970 | Train Acc: 0.8528 | Val Loss: 1.4461 | Val Acc: 0.5225 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  49/51 | Train Loss: 0.8893 | Train Acc: 0.8635 | Val Loss: 1.4280 | Val Acc: 0.5090 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  50/51 | Train Loss: 0.8825 | Train Acc: 0.8674 | Val Loss: 1.4454 | Val Acc: 0.5405 | LR: 0.000162\n",
            "[Trial 11/20] Epoch  51/51 | Train Loss: 0.8797 | Train Acc: 0.8646 | Val Loss: 1.4643 | Val Acc: 0.5135 | LR: 0.000162\n",
            "[Trial 11/20] [Info] Loaded best model weights\n",
            "[Trial 11/20] Done | Val Loss: 1.4280 | Val Acc: 0.5090\n",
            "[I 2025-11-10 12:16:52,436] Trial 10 finished with value: 0.509009009009009 and parameters: {'embedding_dim': 64, 'hidden_size': 96, 'num_layers': 1, 'dropout': 0.3099083496682127, 'batch_size': 48, 'learning_rate': 0.000324078797498575, 'weight_decay': 1.113741490829352e-06, 'label_smoothing': 0.174126004655771, 'tune_epochs': 51}. Best is trial 6 with value: 0.5855855855855856.\n",
            "[Optuna] Completed Trial 11/20 | Value: 0.5090 | Best so far: 0.5856\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 12/20] START\n",
            "[Trial 12/20] Params: embedding_dim=32, hidden_size=64, num_layers=1, dropout=0.34, batch_size=16, lr=0.004698, weight_decay=5.3e-06, label_smoothing=0.12, tune_epochs=60\n",
            "[Trial 12/20] [Train] epochs=60, batch_size=16, lr=0.004698, patience=10\n",
            "[Trial 12/20] Epoch   1/60 | Train Loss: 1.6272 | Train Acc: 0.2483 | Val Loss: 1.6488 | Val Acc: 0.2117 | LR: 0.004698\n",
            "[Trial 12/20] Epoch   2/60 | Train Loss: 1.5652 | Train Acc: 0.3096 | Val Loss: 1.6095 | Val Acc: 0.2477 | LR: 0.004698\n",
            "[Trial 12/20] Epoch   3/60 | Train Loss: 1.4617 | Train Acc: 0.4096 | Val Loss: 1.6387 | Val Acc: 0.2252 | LR: 0.004698\n",
            "[Trial 12/20] Epoch   4/60 | Train Loss: 1.3307 | Train Acc: 0.5000 | Val Loss: 1.5793 | Val Acc: 0.4054 | LR: 0.004698\n",
            "[Trial 12/20] Epoch   5/60 | Train Loss: 1.1866 | Train Acc: 0.6011 | Val Loss: 1.5995 | Val Acc: 0.3604 | LR: 0.004698\n",
            "[Trial 12/20] Epoch   6/60 | Train Loss: 1.0066 | Train Acc: 0.7146 | Val Loss: 1.7606 | Val Acc: 0.3559 | LR: 0.004698\n",
            "[Trial 12/20] Epoch   7/60 | Train Loss: 0.8898 | Train Acc: 0.7899 | Val Loss: 1.6816 | Val Acc: 0.3874 | LR: 0.004698\n",
            "[Trial 12/20] Epoch   8/60 | Train Loss: 0.7871 | Train Acc: 0.8489 | Val Loss: 1.7078 | Val Acc: 0.4144 | LR: 0.004698\n",
            "[Trial 12/20] Epoch   9/60 | Train Loss: 0.7364 | Train Acc: 0.8781 | Val Loss: 1.8251 | Val Acc: 0.3784 | LR: 0.004698\n",
            "[Trial 12/20] Epoch  10/60 | Train Loss: 0.7050 | Train Acc: 0.8961 | Val Loss: 1.8197 | Val Acc: 0.4189 | LR: 0.002349 (LR reduced from 0.004698 to 0.002349)\n",
            "[Trial 12/20] Epoch  11/60 | Train Loss: 0.6002 | Train Acc: 0.9612 | Val Loss: 1.7125 | Val Acc: 0.4099 | LR: 0.002349\n",
            "[Trial 12/20] Epoch  12/60 | Train Loss: 0.5641 | Train Acc: 0.9781 | Val Loss: 1.7152 | Val Acc: 0.4189 | LR: 0.002349\n",
            "[Trial 12/20] Epoch  13/60 | Train Loss: 0.5482 | Train Acc: 0.9865 | Val Loss: 1.6905 | Val Acc: 0.4459 | LR: 0.002349\n",
            "[Trial 12/20] Epoch  14/60 | Train Loss: 0.5375 | Train Acc: 0.9904 | Val Loss: 1.7077 | Val Acc: 0.4324 | LR: 0.002349\n",
            "[Trial 12/20] [EarlyStopping] triggered at epoch 14\n",
            "[Trial 12/20] [Info] Loaded best model weights\n",
            "[Trial 12/20] Done | Val Loss: 1.5793 | Val Acc: 0.4054\n",
            "[I 2025-11-10 12:16:57,239] Trial 11 finished with value: 0.40540540540540543 and parameters: {'embedding_dim': 32, 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.33900798222619904, 'batch_size': 16, 'learning_rate': 0.0046976510456392916, 'weight_decay': 5.250488957261494e-06, 'label_smoothing': 0.12143685480664514, 'tune_epochs': 60}. Best is trial 6 with value: 0.5855855855855856.\n",
            "[Optuna] Completed Trial 12/20 | Value: 0.4054 | Best so far: 0.5856\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 13/20] START\n",
            "[Trial 13/20] Params: embedding_dim=64, hidden_size=32, num_layers=3, dropout=0.39, batch_size=48, lr=0.000696, weight_decay=6.0e-06, label_smoothing=0.15, tune_epochs=60\n",
            "[Trial 13/20] [Train] epochs=60, batch_size=48, lr=0.000696, patience=10\n",
            "[Trial 13/20] Epoch   1/60 | Train Loss: 1.6238 | Train Acc: 0.2045 | Val Loss: 1.6042 | Val Acc: 0.2477 | LR: 0.000696\n",
            "[Trial 13/20] Epoch   2/60 | Train Loss: 1.6060 | Train Acc: 0.2382 | Val Loss: 1.6055 | Val Acc: 0.2297 | LR: 0.000696\n",
            "[Trial 13/20] Epoch   3/60 | Train Loss: 1.6044 | Train Acc: 0.2382 | Val Loss: 1.6034 | Val Acc: 0.2342 | LR: 0.000696\n",
            "[Trial 13/20] Epoch   4/60 | Train Loss: 1.6093 | Train Acc: 0.2404 | Val Loss: 1.6030 | Val Acc: 0.2342 | LR: 0.000696\n",
            "[Trial 13/20] Epoch   5/60 | Train Loss: 1.5958 | Train Acc: 0.2500 | Val Loss: 1.6026 | Val Acc: 0.2117 | LR: 0.000696\n",
            "[Trial 13/20] Epoch   6/60 | Train Loss: 1.5882 | Train Acc: 0.2730 | Val Loss: 1.6013 | Val Acc: 0.2207 | LR: 0.000696\n",
            "[Trial 13/20] Epoch   7/60 | Train Loss: 1.5858 | Train Acc: 0.2736 | Val Loss: 1.5970 | Val Acc: 0.2613 | LR: 0.000696\n",
            "[Trial 13/20] Epoch   8/60 | Train Loss: 1.5762 | Train Acc: 0.2904 | Val Loss: 1.5987 | Val Acc: 0.2613 | LR: 0.000696\n",
            "[Trial 13/20] Epoch   9/60 | Train Loss: 1.5645 | Train Acc: 0.3174 | Val Loss: 1.6033 | Val Acc: 0.2523 | LR: 0.000696\n",
            "[Trial 13/20] Epoch  10/60 | Train Loss: 1.5550 | Train Acc: 0.3489 | Val Loss: 1.6027 | Val Acc: 0.2523 | LR: 0.000696\n",
            "[Trial 13/20] Epoch  11/60 | Train Loss: 1.5454 | Train Acc: 0.3388 | Val Loss: 1.6073 | Val Acc: 0.2432 | LR: 0.000696\n",
            "[Trial 13/20] Epoch  12/60 | Train Loss: 1.5186 | Train Acc: 0.3607 | Val Loss: 1.6105 | Val Acc: 0.2658 | LR: 0.000696\n",
            "[Trial 13/20] Epoch  13/60 | Train Loss: 1.5119 | Train Acc: 0.3730 | Val Loss: 1.6045 | Val Acc: 0.2658 | LR: 0.000348 (LR reduced from 0.000696 to 0.000348)\n",
            "[Trial 13/20] Epoch  14/60 | Train Loss: 1.4756 | Train Acc: 0.4096 | Val Loss: 1.5903 | Val Acc: 0.3198 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  15/60 | Train Loss: 1.4637 | Train Acc: 0.4236 | Val Loss: 1.5901 | Val Acc: 0.3198 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  16/60 | Train Loss: 1.4325 | Train Acc: 0.4360 | Val Loss: 1.5629 | Val Acc: 0.3649 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  17/60 | Train Loss: 1.4066 | Train Acc: 0.4612 | Val Loss: 1.5555 | Val Acc: 0.3423 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  18/60 | Train Loss: 1.4067 | Train Acc: 0.4601 | Val Loss: 1.5315 | Val Acc: 0.3468 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  19/60 | Train Loss: 1.3788 | Train Acc: 0.4921 | Val Loss: 1.5074 | Val Acc: 0.3739 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  20/60 | Train Loss: 1.3609 | Train Acc: 0.4978 | Val Loss: 1.5197 | Val Acc: 0.3829 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  21/60 | Train Loss: 1.3417 | Train Acc: 0.4994 | Val Loss: 1.5386 | Val Acc: 0.3649 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  22/60 | Train Loss: 1.3303 | Train Acc: 0.5146 | Val Loss: 1.5042 | Val Acc: 0.3919 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  23/60 | Train Loss: 1.3137 | Train Acc: 0.5331 | Val Loss: 1.5246 | Val Acc: 0.3874 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  24/60 | Train Loss: 1.2989 | Train Acc: 0.5472 | Val Loss: 1.5337 | Val Acc: 0.3694 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  25/60 | Train Loss: 1.2801 | Train Acc: 0.5506 | Val Loss: 1.5270 | Val Acc: 0.3739 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  26/60 | Train Loss: 1.2640 | Train Acc: 0.5596 | Val Loss: 1.5394 | Val Acc: 0.3964 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  27/60 | Train Loss: 1.2583 | Train Acc: 0.5730 | Val Loss: 1.5080 | Val Acc: 0.4144 | LR: 0.000348\n",
            "[Trial 13/20] Epoch  28/60 | Train Loss: 1.2386 | Train Acc: 0.5691 | Val Loss: 1.5334 | Val Acc: 0.3964 | LR: 0.000174 (LR reduced from 0.000348 to 0.000174)\n",
            "[Trial 13/20] Epoch  29/60 | Train Loss: 1.2158 | Train Acc: 0.5921 | Val Loss: 1.5151 | Val Acc: 0.4369 | LR: 0.000174\n",
            "[Trial 13/20] Epoch  30/60 | Train Loss: 1.2232 | Train Acc: 0.5961 | Val Loss: 1.5244 | Val Acc: 0.4054 | LR: 0.000174\n",
            "[Trial 13/20] Epoch  31/60 | Train Loss: 1.2200 | Train Acc: 0.6000 | Val Loss: 1.5217 | Val Acc: 0.4144 | LR: 0.000174\n",
            "[Trial 13/20] Epoch  32/60 | Train Loss: 1.2042 | Train Acc: 0.6062 | Val Loss: 1.5070 | Val Acc: 0.4099 | LR: 0.000174\n",
            "[Trial 13/20] [EarlyStopping] triggered at epoch 32\n",
            "[Trial 13/20] [Info] Loaded best model weights\n",
            "[Trial 13/20] Done | Val Loss: 1.5042 | Val Acc: 0.3919\n",
            "[I 2025-11-10 12:17:02,272] Trial 12 finished with value: 0.3918918918918919 and parameters: {'embedding_dim': 64, 'hidden_size': 32, 'num_layers': 3, 'dropout': 0.38992652659230687, 'batch_size': 48, 'learning_rate': 0.0006962405447088134, 'weight_decay': 6.0019809714236876e-06, 'label_smoothing': 0.14887198698629364, 'tune_epochs': 60}. Best is trial 6 with value: 0.5855855855855856.\n",
            "[Optuna] Completed Trial 13/20 | Value: 0.3919 | Best so far: 0.5856\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 14/20] START\n",
            "[Trial 14/20] Params: embedding_dim=64, hidden_size=64, num_layers=1, dropout=0.30, batch_size=64, lr=0.000699, weight_decay=5.3e-06, label_smoothing=0.10, tune_epochs=36\n",
            "[Trial 14/20] [Train] epochs=36, batch_size=64, lr=0.000699, patience=10\n",
            "[Trial 14/20] Epoch   1/36 | Train Loss: 1.6434 | Train Acc: 0.2062 | Val Loss: 1.6200 | Val Acc: 0.2342 | LR: 0.000699\n",
            "[Trial 14/20] Epoch   2/36 | Train Loss: 1.5930 | Train Acc: 0.2438 | Val Loss: 1.6117 | Val Acc: 0.2207 | LR: 0.000699\n",
            "[Trial 14/20] Epoch   3/36 | Train Loss: 1.5829 | Train Acc: 0.2590 | Val Loss: 1.6124 | Val Acc: 0.2297 | LR: 0.000699\n",
            "[Trial 14/20] Epoch   4/36 | Train Loss: 1.5613 | Train Acc: 0.3112 | Val Loss: 1.6049 | Val Acc: 0.2432 | LR: 0.000699\n",
            "[Trial 14/20] Epoch   5/36 | Train Loss: 1.5338 | Train Acc: 0.3404 | Val Loss: 1.6079 | Val Acc: 0.2207 | LR: 0.000699\n",
            "[Trial 14/20] Epoch   6/36 | Train Loss: 1.5110 | Train Acc: 0.3719 | Val Loss: 1.6087 | Val Acc: 0.2162 | LR: 0.000699\n",
            "[Trial 14/20] Epoch   7/36 | Train Loss: 1.4933 | Train Acc: 0.3758 | Val Loss: 1.6097 | Val Acc: 0.2297 | LR: 0.000699\n",
            "[Trial 14/20] Epoch   8/36 | Train Loss: 1.4575 | Train Acc: 0.4107 | Val Loss: 1.6041 | Val Acc: 0.2297 | LR: 0.000699\n",
            "[Trial 14/20] Epoch   9/36 | Train Loss: 1.4268 | Train Acc: 0.4292 | Val Loss: 1.5848 | Val Acc: 0.2928 | LR: 0.000699\n",
            "[Trial 14/20] Epoch  10/36 | Train Loss: 1.3747 | Train Acc: 0.4725 | Val Loss: 1.5783 | Val Acc: 0.2613 | LR: 0.000699\n",
            "[Trial 14/20] Epoch  11/36 | Train Loss: 1.3205 | Train Acc: 0.5056 | Val Loss: 1.5421 | Val Acc: 0.3018 | LR: 0.000699\n",
            "[Trial 14/20] Epoch  12/36 | Train Loss: 1.2600 | Train Acc: 0.5511 | Val Loss: 1.5740 | Val Acc: 0.3108 | LR: 0.000699\n",
            "[Trial 14/20] Epoch  13/36 | Train Loss: 1.2113 | Train Acc: 0.5640 | Val Loss: 1.5682 | Val Acc: 0.3423 | LR: 0.000699\n",
            "[Trial 14/20] Epoch  14/36 | Train Loss: 1.1829 | Train Acc: 0.5888 | Val Loss: 1.5724 | Val Acc: 0.3333 | LR: 0.000699\n",
            "[Trial 14/20] Epoch  15/36 | Train Loss: 1.1491 | Train Acc: 0.6129 | Val Loss: 1.5574 | Val Acc: 0.3468 | LR: 0.000699\n",
            "[Trial 14/20] Epoch  16/36 | Train Loss: 1.0958 | Train Acc: 0.6506 | Val Loss: 1.5665 | Val Acc: 0.3559 | LR: 0.000699\n",
            "[Trial 14/20] Epoch  17/36 | Train Loss: 1.0298 | Train Acc: 0.6843 | Val Loss: 1.5633 | Val Acc: 0.3649 | LR: 0.000349 (LR reduced from 0.000699 to 0.000349)\n",
            "[Trial 14/20] Epoch  18/36 | Train Loss: 0.9682 | Train Acc: 0.7157 | Val Loss: 1.5539 | Val Acc: 0.4009 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  19/36 | Train Loss: 0.9417 | Train Acc: 0.7433 | Val Loss: 1.5377 | Val Acc: 0.4189 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  20/36 | Train Loss: 0.9078 | Train Acc: 0.7506 | Val Loss: 1.5433 | Val Acc: 0.4369 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  21/36 | Train Loss: 0.8813 | Train Acc: 0.7708 | Val Loss: 1.5360 | Val Acc: 0.4505 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  22/36 | Train Loss: 0.8454 | Train Acc: 0.7803 | Val Loss: 1.4982 | Val Acc: 0.4414 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  23/36 | Train Loss: 0.8318 | Train Acc: 0.7978 | Val Loss: 1.4909 | Val Acc: 0.4640 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  24/36 | Train Loss: 0.8035 | Train Acc: 0.8197 | Val Loss: 1.4891 | Val Acc: 0.4595 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  25/36 | Train Loss: 0.7912 | Train Acc: 0.8230 | Val Loss: 1.4870 | Val Acc: 0.4820 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  26/36 | Train Loss: 0.7587 | Train Acc: 0.8320 | Val Loss: 1.4885 | Val Acc: 0.4955 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  27/36 | Train Loss: 0.7426 | Train Acc: 0.8461 | Val Loss: 1.4892 | Val Acc: 0.4955 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  28/36 | Train Loss: 0.7272 | Train Acc: 0.8629 | Val Loss: 1.4777 | Val Acc: 0.5000 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  29/36 | Train Loss: 0.6992 | Train Acc: 0.8702 | Val Loss: 1.4967 | Val Acc: 0.4730 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  30/36 | Train Loss: 0.6922 | Train Acc: 0.8803 | Val Loss: 1.5095 | Val Acc: 0.4955 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  31/36 | Train Loss: 0.6700 | Train Acc: 0.8949 | Val Loss: 1.4994 | Val Acc: 0.5135 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  32/36 | Train Loss: 0.6548 | Train Acc: 0.8978 | Val Loss: 1.4842 | Val Acc: 0.5090 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  33/36 | Train Loss: 0.6465 | Train Acc: 0.8944 | Val Loss: 1.4599 | Val Acc: 0.5045 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  34/36 | Train Loss: 0.6311 | Train Acc: 0.9124 | Val Loss: 1.4638 | Val Acc: 0.5045 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  35/36 | Train Loss: 0.6151 | Train Acc: 0.9135 | Val Loss: 1.4749 | Val Acc: 0.5045 | LR: 0.000349\n",
            "[Trial 14/20] Epoch  36/36 | Train Loss: 0.6088 | Train Acc: 0.9180 | Val Loss: 1.4910 | Val Acc: 0.5135 | LR: 0.000349\n",
            "[Trial 14/20] [Info] Loaded best model weights\n",
            "[Trial 14/20] Done | Val Loss: 1.4599 | Val Acc: 0.5045\n",
            "[I 2025-11-10 12:17:05,803] Trial 13 finished with value: 0.5045045045045045 and parameters: {'embedding_dim': 64, 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.30392941597747736, 'batch_size': 64, 'learning_rate': 0.0006985741738644921, 'weight_decay': 5.3047485033023735e-06, 'label_smoothing': 0.09514793689854993, 'tune_epochs': 36}. Best is trial 6 with value: 0.5855855855855856.\n",
            "[Optuna] Completed Trial 14/20 | Value: 0.5045 | Best so far: 0.5856\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 15/20] START\n",
            "[Trial 15/20] Params: embedding_dim=32, hidden_size=96, num_layers=2, dropout=0.48, batch_size=24, lr=0.001892, weight_decay=5.8e-05, label_smoothing=0.14, tune_epochs=53\n",
            "[Trial 15/20] [Train] epochs=53, batch_size=24, lr=0.001892, patience=10\n",
            "[Trial 15/20] Epoch   1/53 | Train Loss: 1.6302 | Train Acc: 0.2118 | Val Loss: 1.6097 | Val Acc: 0.2072 | LR: 0.001892\n",
            "[Trial 15/20] Epoch   2/53 | Train Loss: 1.5903 | Train Acc: 0.2545 | Val Loss: 1.5967 | Val Acc: 0.2613 | LR: 0.001892\n",
            "[Trial 15/20] Epoch   3/53 | Train Loss: 1.5627 | Train Acc: 0.3039 | Val Loss: 1.5886 | Val Acc: 0.2748 | LR: 0.001892\n",
            "[Trial 15/20] Epoch   4/53 | Train Loss: 1.5275 | Train Acc: 0.3478 | Val Loss: 1.6108 | Val Acc: 0.2703 | LR: 0.001892\n",
            "[Trial 15/20] Epoch   5/53 | Train Loss: 1.4813 | Train Acc: 0.3893 | Val Loss: 1.5921 | Val Acc: 0.3018 | LR: 0.001892\n",
            "[Trial 15/20] Epoch   6/53 | Train Loss: 1.4708 | Train Acc: 0.4073 | Val Loss: 1.5980 | Val Acc: 0.3063 | LR: 0.001892\n",
            "[Trial 15/20] Epoch   7/53 | Train Loss: 1.3299 | Train Acc: 0.5157 | Val Loss: 1.5807 | Val Acc: 0.3153 | LR: 0.001892\n",
            "[Trial 15/20] Epoch   8/53 | Train Loss: 1.2097 | Train Acc: 0.5787 | Val Loss: 1.6254 | Val Acc: 0.3604 | LR: 0.001892\n",
            "[Trial 15/20] Epoch   9/53 | Train Loss: 1.1575 | Train Acc: 0.6281 | Val Loss: 1.5750 | Val Acc: 0.4144 | LR: 0.001892\n",
            "[Trial 15/20] Epoch  10/53 | Train Loss: 1.0840 | Train Acc: 0.6747 | Val Loss: 1.6660 | Val Acc: 0.3919 | LR: 0.001892\n",
            "[Trial 15/20] Epoch  11/53 | Train Loss: 1.0018 | Train Acc: 0.7298 | Val Loss: 1.6182 | Val Acc: 0.4550 | LR: 0.001892\n",
            "[Trial 15/20] Epoch  12/53 | Train Loss: 0.9539 | Train Acc: 0.7702 | Val Loss: 1.6521 | Val Acc: 0.4640 | LR: 0.001892\n",
            "[Trial 15/20] Epoch  13/53 | Train Loss: 0.8801 | Train Acc: 0.8084 | Val Loss: 1.6523 | Val Acc: 0.4640 | LR: 0.001892\n",
            "[Trial 15/20] Epoch  14/53 | Train Loss: 0.8066 | Train Acc: 0.8590 | Val Loss: 1.8111 | Val Acc: 0.4324 | LR: 0.001892\n",
            "[Trial 15/20] Epoch  15/53 | Train Loss: 0.7720 | Train Acc: 0.8815 | Val Loss: 1.7215 | Val Acc: 0.4459 | LR: 0.000946 (LR reduced from 0.001892 to 0.000946)\n",
            "[Trial 15/20] Epoch  16/53 | Train Loss: 0.7034 | Train Acc: 0.9275 | Val Loss: 1.7170 | Val Acc: 0.4775 | LR: 0.000946\n",
            "[Trial 15/20] Epoch  17/53 | Train Loss: 0.6690 | Train Acc: 0.9556 | Val Loss: 1.7080 | Val Acc: 0.4685 | LR: 0.000946\n",
            "[Trial 15/20] Epoch  18/53 | Train Loss: 0.6462 | Train Acc: 0.9640 | Val Loss: 1.7107 | Val Acc: 0.4550 | LR: 0.000946\n",
            "[Trial 15/20] Epoch  19/53 | Train Loss: 0.6278 | Train Acc: 0.9713 | Val Loss: 1.7005 | Val Acc: 0.5045 | LR: 0.000946\n",
            "[Trial 15/20] [EarlyStopping] triggered at epoch 19\n",
            "[Trial 15/20] [Info] Loaded best model weights\n",
            "[Trial 15/20] Done | Val Loss: 1.5750 | Val Acc: 0.4144\n",
            "[I 2025-11-10 12:17:10,557] Trial 14 finished with value: 0.4144144144144144 and parameters: {'embedding_dim': 32, 'hidden_size': 96, 'num_layers': 2, 'dropout': 0.48240385425910964, 'batch_size': 24, 'learning_rate': 0.0018922070821124548, 'weight_decay': 5.8023146443001724e-05, 'label_smoothing': 0.13999174992263252, 'tune_epochs': 53}. Best is trial 6 with value: 0.5855855855855856.\n",
            "[Optuna] Completed Trial 15/20 | Value: 0.4144 | Best so far: 0.5856\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 16/20] START\n",
            "[Trial 16/20] Params: embedding_dim=128, hidden_size=32, num_layers=3, dropout=0.25, batch_size=16, lr=0.000234, weight_decay=8.7e-04, label_smoothing=0.20, tune_epochs=49\n",
            "[Trial 16/20] [Train] epochs=49, batch_size=16, lr=0.000234, patience=10\n",
            "[Trial 16/20] Epoch   1/49 | Train Loss: 1.6277 | Train Acc: 0.2006 | Val Loss: 1.6233 | Val Acc: 0.2162 | LR: 0.000234\n",
            "[Trial 16/20] Epoch   2/49 | Train Loss: 1.6167 | Train Acc: 0.2084 | Val Loss: 1.6167 | Val Acc: 0.2342 | LR: 0.000234\n",
            "[Trial 16/20] Epoch   3/49 | Train Loss: 1.6052 | Train Acc: 0.2388 | Val Loss: 1.6125 | Val Acc: 0.2387 | LR: 0.000234\n",
            "[Trial 16/20] Epoch   4/49 | Train Loss: 1.6017 | Train Acc: 0.2478 | Val Loss: 1.6100 | Val Acc: 0.2477 | LR: 0.000234\n",
            "[Trial 16/20] Epoch   5/49 | Train Loss: 1.5947 | Train Acc: 0.2596 | Val Loss: 1.6088 | Val Acc: 0.2297 | LR: 0.000234\n",
            "[Trial 16/20] Epoch   6/49 | Train Loss: 1.5895 | Train Acc: 0.2708 | Val Loss: 1.6080 | Val Acc: 0.2252 | LR: 0.000234\n",
            "[Trial 16/20] Epoch   7/49 | Train Loss: 1.5839 | Train Acc: 0.2854 | Val Loss: 1.6069 | Val Acc: 0.2432 | LR: 0.000234\n",
            "[Trial 16/20] Epoch   8/49 | Train Loss: 1.5778 | Train Acc: 0.3051 | Val Loss: 1.6067 | Val Acc: 0.2387 | LR: 0.000234\n",
            "[Trial 16/20] Epoch   9/49 | Train Loss: 1.5715 | Train Acc: 0.2848 | Val Loss: 1.6046 | Val Acc: 0.2523 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  10/49 | Train Loss: 1.5587 | Train Acc: 0.3191 | Val Loss: 1.6018 | Val Acc: 0.2568 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  11/49 | Train Loss: 1.5478 | Train Acc: 0.3376 | Val Loss: 1.6075 | Val Acc: 0.2703 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  12/49 | Train Loss: 1.5345 | Train Acc: 0.3494 | Val Loss: 1.5779 | Val Acc: 0.2928 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  13/49 | Train Loss: 1.5040 | Train Acc: 0.3826 | Val Loss: 1.5383 | Val Acc: 0.3423 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  14/49 | Train Loss: 1.4604 | Train Acc: 0.4090 | Val Loss: 1.5327 | Val Acc: 0.3874 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  15/49 | Train Loss: 1.4360 | Train Acc: 0.4371 | Val Loss: 1.5155 | Val Acc: 0.3468 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  16/49 | Train Loss: 1.4212 | Train Acc: 0.4388 | Val Loss: 1.5238 | Val Acc: 0.3694 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  17/49 | Train Loss: 1.4009 | Train Acc: 0.4719 | Val Loss: 1.5160 | Val Acc: 0.3829 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  18/49 | Train Loss: 1.3830 | Train Acc: 0.4758 | Val Loss: 1.4805 | Val Acc: 0.4009 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  19/49 | Train Loss: 1.3572 | Train Acc: 0.5051 | Val Loss: 1.4964 | Val Acc: 0.3739 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  20/49 | Train Loss: 1.3543 | Train Acc: 0.4966 | Val Loss: 1.4854 | Val Acc: 0.3964 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  21/49 | Train Loss: 1.3310 | Train Acc: 0.5124 | Val Loss: 1.4926 | Val Acc: 0.4054 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  22/49 | Train Loss: 1.3067 | Train Acc: 0.5303 | Val Loss: 1.4902 | Val Acc: 0.4009 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  23/49 | Train Loss: 1.3101 | Train Acc: 0.5219 | Val Loss: 1.4738 | Val Acc: 0.4234 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  24/49 | Train Loss: 1.2836 | Train Acc: 0.5612 | Val Loss: 1.4762 | Val Acc: 0.4324 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  25/49 | Train Loss: 1.2781 | Train Acc: 0.5500 | Val Loss: 1.4964 | Val Acc: 0.4144 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  26/49 | Train Loss: 1.2591 | Train Acc: 0.5803 | Val Loss: 1.4901 | Val Acc: 0.4414 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  27/49 | Train Loss: 1.2437 | Train Acc: 0.5826 | Val Loss: 1.4781 | Val Acc: 0.4324 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  28/49 | Train Loss: 1.2405 | Train Acc: 0.6028 | Val Loss: 1.5038 | Val Acc: 0.4054 | LR: 0.000234\n",
            "[Trial 16/20] Epoch  29/49 | Train Loss: 1.2174 | Train Acc: 0.6208 | Val Loss: 1.4874 | Val Acc: 0.4369 | LR: 0.000117 (LR reduced from 0.000234 to 0.000117)\n",
            "[Trial 16/20] Epoch  30/49 | Train Loss: 1.2062 | Train Acc: 0.6264 | Val Loss: 1.4837 | Val Acc: 0.4414 | LR: 0.000117\n",
            "[Trial 16/20] Epoch  31/49 | Train Loss: 1.1959 | Train Acc: 0.6348 | Val Loss: 1.4835 | Val Acc: 0.4550 | LR: 0.000117\n",
            "[Trial 16/20] Epoch  32/49 | Train Loss: 1.1829 | Train Acc: 0.6410 | Val Loss: 1.4817 | Val Acc: 0.4505 | LR: 0.000117\n",
            "[Trial 16/20] Epoch  33/49 | Train Loss: 1.1706 | Train Acc: 0.6466 | Val Loss: 1.4791 | Val Acc: 0.4685 | LR: 0.000117\n",
            "[Trial 16/20] [EarlyStopping] triggered at epoch 33\n",
            "[Trial 16/20] [Info] Loaded best model weights\n",
            "[Trial 16/20] Done | Val Loss: 1.4738 | Val Acc: 0.4234\n",
            "[I 2025-11-10 12:17:26,920] Trial 15 finished with value: 0.42342342342342343 and parameters: {'embedding_dim': 128, 'hidden_size': 32, 'num_layers': 3, 'dropout': 0.25156577312734085, 'batch_size': 16, 'learning_rate': 0.0002339763520473826, 'weight_decay': 0.0008675145655490937, 'label_smoothing': 0.19826104033608824, 'tune_epochs': 49}. Best is trial 6 with value: 0.5855855855855856.\n",
            "[Optuna] Completed Trial 16/20 | Value: 0.4234 | Best so far: 0.5856\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 17/20] START\n",
            "[Trial 17/20] Params: embedding_dim=64, hidden_size=64, num_layers=1, dropout=0.41, batch_size=24, lr=0.000546, weight_decay=8.6e-06, label_smoothing=0.09, tune_epochs=55\n",
            "[Trial 17/20] [Train] epochs=55, batch_size=24, lr=0.000546, patience=10\n",
            "[Trial 17/20] Epoch   1/55 | Train Loss: 1.6524 | Train Acc: 0.2129 | Val Loss: 1.6266 | Val Acc: 0.1802 | LR: 0.000546\n",
            "[Trial 17/20] Epoch   2/55 | Train Loss: 1.6018 | Train Acc: 0.2567 | Val Loss: 1.6184 | Val Acc: 0.1802 | LR: 0.000546\n",
            "[Trial 17/20] Epoch   3/55 | Train Loss: 1.5857 | Train Acc: 0.2837 | Val Loss: 1.6115 | Val Acc: 0.2252 | LR: 0.000546\n",
            "[Trial 17/20] Epoch   4/55 | Train Loss: 1.5506 | Train Acc: 0.3112 | Val Loss: 1.6141 | Val Acc: 0.2342 | LR: 0.000546\n",
            "[Trial 17/20] Epoch   5/55 | Train Loss: 1.5377 | Train Acc: 0.3230 | Val Loss: 1.6129 | Val Acc: 0.2387 | LR: 0.000546\n",
            "[Trial 17/20] Epoch   6/55 | Train Loss: 1.4924 | Train Acc: 0.3798 | Val Loss: 1.6142 | Val Acc: 0.2703 | LR: 0.000546\n",
            "[Trial 17/20] Epoch   7/55 | Train Loss: 1.4160 | Train Acc: 0.4270 | Val Loss: 1.6073 | Val Acc: 0.3378 | LR: 0.000546\n",
            "[Trial 17/20] Epoch   8/55 | Train Loss: 1.3190 | Train Acc: 0.5017 | Val Loss: 1.5155 | Val Acc: 0.3423 | LR: 0.000546\n",
            "[Trial 17/20] Epoch   9/55 | Train Loss: 1.2719 | Train Acc: 0.5022 | Val Loss: 1.4790 | Val Acc: 0.4054 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  10/55 | Train Loss: 1.1944 | Train Acc: 0.5539 | Val Loss: 1.5052 | Val Acc: 0.3468 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  11/55 | Train Loss: 1.1462 | Train Acc: 0.5865 | Val Loss: 1.4976 | Val Acc: 0.3964 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  12/55 | Train Loss: 1.1094 | Train Acc: 0.6140 | Val Loss: 1.4654 | Val Acc: 0.4279 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  13/55 | Train Loss: 1.0463 | Train Acc: 0.6382 | Val Loss: 1.4473 | Val Acc: 0.4234 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  14/55 | Train Loss: 1.0242 | Train Acc: 0.6607 | Val Loss: 1.5257 | Val Acc: 0.4009 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  15/55 | Train Loss: 0.9872 | Train Acc: 0.6921 | Val Loss: 1.4544 | Val Acc: 0.4369 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  16/55 | Train Loss: 0.9172 | Train Acc: 0.7309 | Val Loss: 1.4560 | Val Acc: 0.4595 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  17/55 | Train Loss: 0.8850 | Train Acc: 0.7438 | Val Loss: 1.3808 | Val Acc: 0.4910 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  18/55 | Train Loss: 0.8464 | Train Acc: 0.7787 | Val Loss: 1.4194 | Val Acc: 0.5270 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  19/55 | Train Loss: 0.8009 | Train Acc: 0.8062 | Val Loss: 1.4367 | Val Acc: 0.5495 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  20/55 | Train Loss: 0.7657 | Train Acc: 0.8275 | Val Loss: 1.3778 | Val Acc: 0.5631 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  21/55 | Train Loss: 0.7116 | Train Acc: 0.8691 | Val Loss: 1.3904 | Val Acc: 0.5586 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  22/55 | Train Loss: 0.7008 | Train Acc: 0.8579 | Val Loss: 1.4243 | Val Acc: 0.5811 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  23/55 | Train Loss: 0.6607 | Train Acc: 0.8820 | Val Loss: 1.3705 | Val Acc: 0.6216 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  24/55 | Train Loss: 0.6332 | Train Acc: 0.8921 | Val Loss: 1.3862 | Val Acc: 0.5721 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  25/55 | Train Loss: 0.5936 | Train Acc: 0.9298 | Val Loss: 1.4330 | Val Acc: 0.6081 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  26/55 | Train Loss: 0.5805 | Train Acc: 0.9315 | Val Loss: 1.3357 | Val Acc: 0.5946 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  27/55 | Train Loss: 0.5693 | Train Acc: 0.9343 | Val Loss: 1.3477 | Val Acc: 0.6036 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  28/55 | Train Loss: 0.5466 | Train Acc: 0.9433 | Val Loss: 1.3406 | Val Acc: 0.6306 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  29/55 | Train Loss: 0.5270 | Train Acc: 0.9618 | Val Loss: 1.2826 | Val Acc: 0.6306 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  30/55 | Train Loss: 0.5187 | Train Acc: 0.9596 | Val Loss: 1.3659 | Val Acc: 0.5991 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  31/55 | Train Loss: 0.5101 | Train Acc: 0.9663 | Val Loss: 1.3067 | Val Acc: 0.6396 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  32/55 | Train Loss: 0.4880 | Train Acc: 0.9787 | Val Loss: 1.3263 | Val Acc: 0.6351 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  33/55 | Train Loss: 0.4797 | Train Acc: 0.9803 | Val Loss: 1.3549 | Val Acc: 0.6261 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  34/55 | Train Loss: 0.4801 | Train Acc: 0.9798 | Val Loss: 1.3149 | Val Acc: 0.6532 | LR: 0.000546\n",
            "[Trial 17/20] Epoch  35/55 | Train Loss: 0.4675 | Train Acc: 0.9837 | Val Loss: 1.3012 | Val Acc: 0.6396 | LR: 0.000273 (LR reduced from 0.000546 to 0.000273)\n",
            "[Trial 17/20] Epoch  36/55 | Train Loss: 0.4633 | Train Acc: 0.9843 | Val Loss: 1.2952 | Val Acc: 0.6532 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  37/55 | Train Loss: 0.4547 | Train Acc: 0.9888 | Val Loss: 1.2583 | Val Acc: 0.6486 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  38/55 | Train Loss: 0.4511 | Train Acc: 0.9938 | Val Loss: 1.2880 | Val Acc: 0.6441 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  39/55 | Train Loss: 0.4491 | Train Acc: 0.9916 | Val Loss: 1.2676 | Val Acc: 0.6622 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  40/55 | Train Loss: 0.4464 | Train Acc: 0.9933 | Val Loss: 1.2740 | Val Acc: 0.6577 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  41/55 | Train Loss: 0.4434 | Train Acc: 0.9921 | Val Loss: 1.2551 | Val Acc: 0.6577 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  42/55 | Train Loss: 0.4442 | Train Acc: 0.9933 | Val Loss: 1.2414 | Val Acc: 0.6441 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  43/55 | Train Loss: 0.4418 | Train Acc: 0.9944 | Val Loss: 1.2682 | Val Acc: 0.6757 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  44/55 | Train Loss: 0.4375 | Train Acc: 0.9949 | Val Loss: 1.2547 | Val Acc: 0.6577 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  45/55 | Train Loss: 0.4395 | Train Acc: 0.9933 | Val Loss: 1.2556 | Val Acc: 0.6532 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  46/55 | Train Loss: 0.4371 | Train Acc: 0.9944 | Val Loss: 1.2274 | Val Acc: 0.6667 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  47/55 | Train Loss: 0.4314 | Train Acc: 0.9966 | Val Loss: 1.2436 | Val Acc: 0.6667 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  48/55 | Train Loss: 0.4290 | Train Acc: 0.9966 | Val Loss: 1.2610 | Val Acc: 0.6667 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  49/55 | Train Loss: 0.4330 | Train Acc: 0.9949 | Val Loss: 1.2665 | Val Acc: 0.6667 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  50/55 | Train Loss: 0.4340 | Train Acc: 0.9927 | Val Loss: 1.2841 | Val Acc: 0.6667 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  51/55 | Train Loss: 0.4307 | Train Acc: 0.9938 | Val Loss: 1.2774 | Val Acc: 0.6577 | LR: 0.000273\n",
            "[Trial 17/20] Epoch  52/55 | Train Loss: 0.4274 | Train Acc: 0.9961 | Val Loss: 1.2520 | Val Acc: 0.6532 | LR: 0.000137 (LR reduced from 0.000273 to 0.000137)\n",
            "[Trial 17/20] Epoch  53/55 | Train Loss: 0.4251 | Train Acc: 0.9961 | Val Loss: 1.2618 | Val Acc: 0.6577 | LR: 0.000137\n",
            "[Trial 17/20] Epoch  54/55 | Train Loss: 0.4235 | Train Acc: 0.9978 | Val Loss: 1.2407 | Val Acc: 0.6757 | LR: 0.000137\n",
            "[Trial 17/20] Epoch  55/55 | Train Loss: 0.4217 | Train Acc: 0.9983 | Val Loss: 1.2546 | Val Acc: 0.6757 | LR: 0.000137\n",
            "[Trial 17/20] [Info] Loaded best model weights\n",
            "[Trial 17/20] Done | Val Loss: 1.2274 | Val Acc: 0.6667\n",
            "[I 2025-11-10 12:17:40,278] Trial 16 finished with value: 0.6666666666666666 and parameters: {'embedding_dim': 64, 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.4129989624132342, 'batch_size': 24, 'learning_rate': 0.0005460598289100347, 'weight_decay': 8.552011163103825e-06, 'label_smoothing': 0.09378345235787833, 'tune_epochs': 55}. Best is trial 16 with value: 0.6666666666666666.\n",
            "[Optuna] Completed Trial 17/20 | Value: 0.6667 | Best so far: 0.6667\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 18/20] START\n",
            "[Trial 18/20] Params: embedding_dim=64, hidden_size=32, num_layers=1, dropout=0.40, batch_size=24, lr=0.000551, weight_decay=3.0e-06, label_smoothing=0.01, tune_epochs=46\n",
            "[Trial 18/20] [Train] epochs=46, batch_size=24, lr=0.000551, patience=10\n",
            "[Trial 18/20] Epoch   1/46 | Train Loss: 1.6718 | Train Acc: 0.2022 | Val Loss: 1.6249 | Val Acc: 0.1892 | LR: 0.000551\n",
            "[Trial 18/20] Epoch   2/46 | Train Loss: 1.6433 | Train Acc: 0.2174 | Val Loss: 1.6150 | Val Acc: 0.2027 | LR: 0.000551\n",
            "[Trial 18/20] Epoch   3/46 | Train Loss: 1.6153 | Train Acc: 0.2354 | Val Loss: 1.6105 | Val Acc: 0.2072 | LR: 0.000551\n",
            "[Trial 18/20] Epoch   4/46 | Train Loss: 1.5887 | Train Acc: 0.2427 | Val Loss: 1.6035 | Val Acc: 0.2387 | LR: 0.000551\n",
            "[Trial 18/20] Epoch   5/46 | Train Loss: 1.5783 | Train Acc: 0.2680 | Val Loss: 1.6027 | Val Acc: 0.2477 | LR: 0.000551\n",
            "[Trial 18/20] Epoch   6/46 | Train Loss: 1.5683 | Train Acc: 0.2747 | Val Loss: 1.6010 | Val Acc: 0.2658 | LR: 0.000551\n",
            "[Trial 18/20] Epoch   7/46 | Train Loss: 1.5579 | Train Acc: 0.2792 | Val Loss: 1.5972 | Val Acc: 0.2793 | LR: 0.000551\n",
            "[Trial 18/20] Epoch   8/46 | Train Loss: 1.5436 | Train Acc: 0.2983 | Val Loss: 1.5959 | Val Acc: 0.2838 | LR: 0.000551\n",
            "[Trial 18/20] Epoch   9/46 | Train Loss: 1.5155 | Train Acc: 0.3354 | Val Loss: 1.5947 | Val Acc: 0.2838 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  10/46 | Train Loss: 1.5103 | Train Acc: 0.3404 | Val Loss: 1.5920 | Val Acc: 0.2973 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  11/46 | Train Loss: 1.4992 | Train Acc: 0.3399 | Val Loss: 1.5948 | Val Acc: 0.2973 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  12/46 | Train Loss: 1.4689 | Train Acc: 0.3680 | Val Loss: 1.5903 | Val Acc: 0.3018 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  13/46 | Train Loss: 1.4493 | Train Acc: 0.3809 | Val Loss: 1.5895 | Val Acc: 0.2703 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  14/46 | Train Loss: 1.4210 | Train Acc: 0.3893 | Val Loss: 1.5853 | Val Acc: 0.2883 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  15/46 | Train Loss: 1.3858 | Train Acc: 0.4275 | Val Loss: 1.5758 | Val Acc: 0.2793 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  16/46 | Train Loss: 1.3455 | Train Acc: 0.4455 | Val Loss: 1.5529 | Val Acc: 0.3063 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  17/46 | Train Loss: 1.2965 | Train Acc: 0.4893 | Val Loss: 1.5455 | Val Acc: 0.3108 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  18/46 | Train Loss: 1.2333 | Train Acc: 0.5169 | Val Loss: 1.5261 | Val Acc: 0.3333 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  19/46 | Train Loss: 1.1555 | Train Acc: 0.5713 | Val Loss: 1.6286 | Val Acc: 0.2838 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  20/46 | Train Loss: 1.1095 | Train Acc: 0.5860 | Val Loss: 1.4461 | Val Acc: 0.4009 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  21/46 | Train Loss: 1.0423 | Train Acc: 0.6079 | Val Loss: 1.4845 | Val Acc: 0.4009 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  22/46 | Train Loss: 0.9855 | Train Acc: 0.6590 | Val Loss: 1.4837 | Val Acc: 0.3919 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  23/46 | Train Loss: 0.9278 | Train Acc: 0.6764 | Val Loss: 1.4123 | Val Acc: 0.4144 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  24/46 | Train Loss: 0.8646 | Train Acc: 0.6989 | Val Loss: 1.3768 | Val Acc: 0.4279 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  25/46 | Train Loss: 0.8117 | Train Acc: 0.7258 | Val Loss: 1.3536 | Val Acc: 0.4595 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  26/46 | Train Loss: 0.7467 | Train Acc: 0.7365 | Val Loss: 1.3683 | Val Acc: 0.4279 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  27/46 | Train Loss: 0.7123 | Train Acc: 0.7590 | Val Loss: 1.3860 | Val Acc: 0.4459 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  28/46 | Train Loss: 0.6559 | Train Acc: 0.7882 | Val Loss: 1.3634 | Val Acc: 0.4640 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  29/46 | Train Loss: 0.6126 | Train Acc: 0.8039 | Val Loss: 1.3634 | Val Acc: 0.4820 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  30/46 | Train Loss: 0.5663 | Train Acc: 0.8348 | Val Loss: 1.3820 | Val Acc: 0.4820 | LR: 0.000551\n",
            "[Trial 18/20] Epoch  31/46 | Train Loss: 0.5250 | Train Acc: 0.8466 | Val Loss: 1.4331 | Val Acc: 0.4955 | LR: 0.000275 (LR reduced from 0.000551 to 0.000275)\n",
            "[Trial 18/20] Epoch  32/46 | Train Loss: 0.4772 | Train Acc: 0.8680 | Val Loss: 1.4779 | Val Acc: 0.4955 | LR: 0.000275\n",
            "[Trial 18/20] Epoch  33/46 | Train Loss: 0.4604 | Train Acc: 0.8719 | Val Loss: 1.4317 | Val Acc: 0.4910 | LR: 0.000275\n",
            "[Trial 18/20] Epoch  34/46 | Train Loss: 0.4451 | Train Acc: 0.8815 | Val Loss: 1.4178 | Val Acc: 0.5000 | LR: 0.000275\n",
            "[Trial 18/20] Epoch  35/46 | Train Loss: 0.4086 | Train Acc: 0.8949 | Val Loss: 1.4667 | Val Acc: 0.5180 | LR: 0.000275\n",
            "[Trial 18/20] [EarlyStopping] triggered at epoch 35\n",
            "[Trial 18/20] [Info] Loaded best model weights\n",
            "[Trial 18/20] Done | Val Loss: 1.3536 | Val Acc: 0.4595\n",
            "[I 2025-11-10 12:17:48,587] Trial 17 finished with value: 0.4594594594594595 and parameters: {'embedding_dim': 64, 'hidden_size': 32, 'num_layers': 1, 'dropout': 0.4027047150290285, 'batch_size': 24, 'learning_rate': 0.0005507069230682298, 'weight_decay': 3.034284338418056e-06, 'label_smoothing': 0.00726096264642366, 'tune_epochs': 46}. Best is trial 16 with value: 0.6666666666666666.\n",
            "[Optuna] Completed Trial 18/20 | Value: 0.4595 | Best so far: 0.6667\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 19/20] START\n",
            "[Trial 19/20] Params: embedding_dim=64, hidden_size=64, num_layers=1, dropout=0.52, batch_size=24, lr=0.000212, weight_decay=4.0e-05, label_smoothing=0.03, tune_epochs=38\n",
            "[Trial 19/20] [Train] epochs=38, batch_size=24, lr=0.000212, patience=10\n",
            "[Trial 19/20] Epoch   1/38 | Train Loss: 1.6681 | Train Acc: 0.2084 | Val Loss: 1.6240 | Val Acc: 0.2072 | LR: 0.000212\n",
            "[Trial 19/20] Epoch   2/38 | Train Loss: 1.6463 | Train Acc: 0.2107 | Val Loss: 1.6166 | Val Acc: 0.2072 | LR: 0.000212\n",
            "[Trial 19/20] Epoch   3/38 | Train Loss: 1.6260 | Train Acc: 0.2326 | Val Loss: 1.6127 | Val Acc: 0.2117 | LR: 0.000212\n",
            "[Trial 19/20] Epoch   4/38 | Train Loss: 1.6248 | Train Acc: 0.2337 | Val Loss: 1.6107 | Val Acc: 0.2207 | LR: 0.000212\n",
            "[Trial 19/20] Epoch   5/38 | Train Loss: 1.6015 | Train Acc: 0.2444 | Val Loss: 1.6109 | Val Acc: 0.2117 | LR: 0.000212\n",
            "[Trial 19/20] Epoch   6/38 | Train Loss: 1.5956 | Train Acc: 0.2455 | Val Loss: 1.6088 | Val Acc: 0.2072 | LR: 0.000212\n",
            "[Trial 19/20] Epoch   7/38 | Train Loss: 1.5786 | Train Acc: 0.2758 | Val Loss: 1.6079 | Val Acc: 0.2207 | LR: 0.000212\n",
            "[Trial 19/20] Epoch   8/38 | Train Loss: 1.5537 | Train Acc: 0.3045 | Val Loss: 1.6076 | Val Acc: 0.2252 | LR: 0.000212\n",
            "[Trial 19/20] Epoch   9/38 | Train Loss: 1.5503 | Train Acc: 0.3028 | Val Loss: 1.6071 | Val Acc: 0.2207 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  10/38 | Train Loss: 1.5570 | Train Acc: 0.3034 | Val Loss: 1.6062 | Val Acc: 0.2207 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  11/38 | Train Loss: 1.5460 | Train Acc: 0.3202 | Val Loss: 1.6067 | Val Acc: 0.2117 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  12/38 | Train Loss: 1.5327 | Train Acc: 0.3101 | Val Loss: 1.6046 | Val Acc: 0.2252 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  13/38 | Train Loss: 1.5188 | Train Acc: 0.3320 | Val Loss: 1.6058 | Val Acc: 0.2162 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  14/38 | Train Loss: 1.5023 | Train Acc: 0.3522 | Val Loss: 1.6052 | Val Acc: 0.2072 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  15/38 | Train Loss: 1.4851 | Train Acc: 0.3798 | Val Loss: 1.6074 | Val Acc: 0.2162 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  16/38 | Train Loss: 1.4850 | Train Acc: 0.3646 | Val Loss: 1.6064 | Val Acc: 0.2477 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  17/38 | Train Loss: 1.4625 | Train Acc: 0.4011 | Val Loss: 1.5965 | Val Acc: 0.2613 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  18/38 | Train Loss: 1.4339 | Train Acc: 0.4118 | Val Loss: 1.5822 | Val Acc: 0.2793 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  19/38 | Train Loss: 1.3812 | Train Acc: 0.4360 | Val Loss: 1.5338 | Val Acc: 0.3108 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  20/38 | Train Loss: 1.3497 | Train Acc: 0.4466 | Val Loss: 1.5316 | Val Acc: 0.3378 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  21/38 | Train Loss: 1.3042 | Train Acc: 0.4831 | Val Loss: 1.4883 | Val Acc: 0.3153 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  22/38 | Train Loss: 1.2681 | Train Acc: 0.4938 | Val Loss: 1.4839 | Val Acc: 0.3333 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  23/38 | Train Loss: 1.2301 | Train Acc: 0.5124 | Val Loss: 1.4755 | Val Acc: 0.3604 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  24/38 | Train Loss: 1.2031 | Train Acc: 0.5247 | Val Loss: 1.4549 | Val Acc: 0.3423 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  25/38 | Train Loss: 1.1600 | Train Acc: 0.5472 | Val Loss: 1.4355 | Val Acc: 0.3604 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  26/38 | Train Loss: 1.1210 | Train Acc: 0.5713 | Val Loss: 1.4244 | Val Acc: 0.3694 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  27/38 | Train Loss: 1.1068 | Train Acc: 0.5831 | Val Loss: 1.4175 | Val Acc: 0.3874 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  28/38 | Train Loss: 1.0889 | Train Acc: 0.5770 | Val Loss: 1.4043 | Val Acc: 0.3784 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  29/38 | Train Loss: 1.0336 | Train Acc: 0.6191 | Val Loss: 1.4254 | Val Acc: 0.4189 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  30/38 | Train Loss: 1.0094 | Train Acc: 0.6140 | Val Loss: 1.4106 | Val Acc: 0.4144 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  31/38 | Train Loss: 0.9808 | Train Acc: 0.6399 | Val Loss: 1.3995 | Val Acc: 0.4054 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  32/38 | Train Loss: 0.9610 | Train Acc: 0.6646 | Val Loss: 1.3740 | Val Acc: 0.4324 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  33/38 | Train Loss: 0.9280 | Train Acc: 0.6697 | Val Loss: 1.3505 | Val Acc: 0.4505 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  34/38 | Train Loss: 0.9070 | Train Acc: 0.6893 | Val Loss: 1.4016 | Val Acc: 0.4459 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  35/38 | Train Loss: 0.8683 | Train Acc: 0.7028 | Val Loss: 1.3360 | Val Acc: 0.5000 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  36/38 | Train Loss: 0.8476 | Train Acc: 0.7185 | Val Loss: 1.4391 | Val Acc: 0.4144 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  37/38 | Train Loss: 0.8334 | Train Acc: 0.7219 | Val Loss: 1.3749 | Val Acc: 0.4505 | LR: 0.000212\n",
            "[Trial 19/20] Epoch  38/38 | Train Loss: 0.7864 | Train Acc: 0.7528 | Val Loss: 1.3822 | Val Acc: 0.4865 | LR: 0.000212\n",
            "[Trial 19/20] [Info] Loaded best model weights\n",
            "[Trial 19/20] Done | Val Loss: 1.3360 | Val Acc: 0.5000\n",
            "[I 2025-11-10 12:17:57,903] Trial 18 finished with value: 0.5 and parameters: {'embedding_dim': 64, 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.5170701692767817, 'batch_size': 24, 'learning_rate': 0.00021227790786730025, 'weight_decay': 4.042495548026012e-05, 'label_smoothing': 0.029387478307783196, 'tune_epochs': 38}. Best is trial 16 with value: 0.6666666666666666.\n",
            "[Optuna] Completed Trial 19/20 | Value: 0.5000 | Best so far: 0.6667\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "[Trial 20/20] START\n",
            "[Trial 20/20] Params: embedding_dim=64, hidden_size=128, num_layers=1, dropout=0.44, batch_size=24, lr=0.000106, weight_decay=2.0e-06, label_smoothing=0.08, tune_epochs=56\n",
            "[Trial 20/20] [Train] epochs=56, batch_size=24, lr=0.000106, patience=10\n",
            "[Trial 20/20] Epoch   1/56 | Train Loss: 1.6421 | Train Acc: 0.1927 | Val Loss: 1.6071 | Val Acc: 0.2297 | LR: 0.000106\n",
            "[Trial 20/20] Epoch   2/56 | Train Loss: 1.6199 | Train Acc: 0.2258 | Val Loss: 1.6063 | Val Acc: 0.2387 | LR: 0.000106\n",
            "[Trial 20/20] Epoch   3/56 | Train Loss: 1.5999 | Train Acc: 0.2517 | Val Loss: 1.6046 | Val Acc: 0.2523 | LR: 0.000106\n",
            "[Trial 20/20] Epoch   4/56 | Train Loss: 1.5976 | Train Acc: 0.2545 | Val Loss: 1.6045 | Val Acc: 0.2523 | LR: 0.000106\n",
            "[Trial 20/20] Epoch   5/56 | Train Loss: 1.5929 | Train Acc: 0.2433 | Val Loss: 1.6045 | Val Acc: 0.2613 | LR: 0.000106\n",
            "[Trial 20/20] Epoch   6/56 | Train Loss: 1.5788 | Train Acc: 0.2787 | Val Loss: 1.6040 | Val Acc: 0.2432 | LR: 0.000106\n",
            "[Trial 20/20] Epoch   7/56 | Train Loss: 1.5800 | Train Acc: 0.2781 | Val Loss: 1.6036 | Val Acc: 0.2523 | LR: 0.000106\n",
            "[Trial 20/20] Epoch   8/56 | Train Loss: 1.5594 | Train Acc: 0.3118 | Val Loss: 1.6038 | Val Acc: 0.2523 | LR: 0.000106\n",
            "[Trial 20/20] Epoch   9/56 | Train Loss: 1.5600 | Train Acc: 0.2871 | Val Loss: 1.6046 | Val Acc: 0.2387 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  10/56 | Train Loss: 1.5492 | Train Acc: 0.3169 | Val Loss: 1.6050 | Val Acc: 0.2477 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  11/56 | Train Loss: 1.5344 | Train Acc: 0.3281 | Val Loss: 1.6061 | Val Acc: 0.2432 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  12/56 | Train Loss: 1.5279 | Train Acc: 0.3382 | Val Loss: 1.6049 | Val Acc: 0.2613 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  13/56 | Train Loss: 1.5133 | Train Acc: 0.3539 | Val Loss: 1.6024 | Val Acc: 0.2523 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  14/56 | Train Loss: 1.4979 | Train Acc: 0.3697 | Val Loss: 1.5971 | Val Acc: 0.2432 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  15/56 | Train Loss: 1.4768 | Train Acc: 0.3910 | Val Loss: 1.5669 | Val Acc: 0.2793 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  16/56 | Train Loss: 1.4392 | Train Acc: 0.4180 | Val Loss: 1.5527 | Val Acc: 0.2973 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  17/56 | Train Loss: 1.4053 | Train Acc: 0.4444 | Val Loss: 1.5315 | Val Acc: 0.3153 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  18/56 | Train Loss: 1.3780 | Train Acc: 0.4517 | Val Loss: 1.5579 | Val Acc: 0.3243 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  19/56 | Train Loss: 1.3494 | Train Acc: 0.4674 | Val Loss: 1.5136 | Val Acc: 0.3243 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  20/56 | Train Loss: 1.3275 | Train Acc: 0.4831 | Val Loss: 1.4884 | Val Acc: 0.3559 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  21/56 | Train Loss: 1.2974 | Train Acc: 0.5017 | Val Loss: 1.4974 | Val Acc: 0.3468 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  22/56 | Train Loss: 1.2877 | Train Acc: 0.5090 | Val Loss: 1.4698 | Val Acc: 0.3559 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  23/56 | Train Loss: 1.2491 | Train Acc: 0.5455 | Val Loss: 1.4645 | Val Acc: 0.3739 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  24/56 | Train Loss: 1.2320 | Train Acc: 0.5427 | Val Loss: 1.4581 | Val Acc: 0.3874 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  25/56 | Train Loss: 1.2161 | Train Acc: 0.5663 | Val Loss: 1.4726 | Val Acc: 0.3694 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  26/56 | Train Loss: 1.1914 | Train Acc: 0.5764 | Val Loss: 1.4479 | Val Acc: 0.3829 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  27/56 | Train Loss: 1.1837 | Train Acc: 0.5787 | Val Loss: 1.4373 | Val Acc: 0.3739 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  28/56 | Train Loss: 1.1567 | Train Acc: 0.5972 | Val Loss: 1.4132 | Val Acc: 0.3649 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  29/56 | Train Loss: 1.1270 | Train Acc: 0.6011 | Val Loss: 1.4039 | Val Acc: 0.4234 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  30/56 | Train Loss: 1.1081 | Train Acc: 0.6230 | Val Loss: 1.4032 | Val Acc: 0.4459 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  31/56 | Train Loss: 1.0865 | Train Acc: 0.6449 | Val Loss: 1.3869 | Val Acc: 0.4189 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  32/56 | Train Loss: 1.0603 | Train Acc: 0.6551 | Val Loss: 1.3659 | Val Acc: 0.4459 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  33/56 | Train Loss: 1.0467 | Train Acc: 0.6607 | Val Loss: 1.3788 | Val Acc: 0.4505 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  34/56 | Train Loss: 1.0352 | Train Acc: 0.6685 | Val Loss: 1.3711 | Val Acc: 0.4234 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  35/56 | Train Loss: 1.0102 | Train Acc: 0.6674 | Val Loss: 1.3737 | Val Acc: 0.4459 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  36/56 | Train Loss: 0.9924 | Train Acc: 0.6770 | Val Loss: 1.3305 | Val Acc: 0.4910 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  37/56 | Train Loss: 0.9699 | Train Acc: 0.6933 | Val Loss: 1.3579 | Val Acc: 0.4775 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  38/56 | Train Loss: 0.9454 | Train Acc: 0.7169 | Val Loss: 1.3310 | Val Acc: 0.5000 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  39/56 | Train Loss: 0.9374 | Train Acc: 0.7230 | Val Loss: 1.3306 | Val Acc: 0.5180 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  40/56 | Train Loss: 0.9152 | Train Acc: 0.7331 | Val Loss: 1.3591 | Val Acc: 0.5225 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  41/56 | Train Loss: 0.9056 | Train Acc: 0.7388 | Val Loss: 1.3279 | Val Acc: 0.5270 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  42/56 | Train Loss: 0.8804 | Train Acc: 0.7489 | Val Loss: 1.3190 | Val Acc: 0.5135 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  43/56 | Train Loss: 0.8805 | Train Acc: 0.7399 | Val Loss: 1.3110 | Val Acc: 0.5360 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  44/56 | Train Loss: 0.8577 | Train Acc: 0.7708 | Val Loss: 1.3145 | Val Acc: 0.5180 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  45/56 | Train Loss: 0.8460 | Train Acc: 0.7697 | Val Loss: 1.3314 | Val Acc: 0.5180 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  46/56 | Train Loss: 0.8442 | Train Acc: 0.7629 | Val Loss: 1.3112 | Val Acc: 0.5360 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  47/56 | Train Loss: 0.8253 | Train Acc: 0.7775 | Val Loss: 1.3161 | Val Acc: 0.5586 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  48/56 | Train Loss: 0.8032 | Train Acc: 0.8006 | Val Loss: 1.3016 | Val Acc: 0.5541 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  49/56 | Train Loss: 0.7929 | Train Acc: 0.7955 | Val Loss: 1.3013 | Val Acc: 0.5631 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  50/56 | Train Loss: 0.7832 | Train Acc: 0.8051 | Val Loss: 1.2924 | Val Acc: 0.5405 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  51/56 | Train Loss: 0.7708 | Train Acc: 0.8118 | Val Loss: 1.3031 | Val Acc: 0.5450 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  52/56 | Train Loss: 0.7607 | Train Acc: 0.8129 | Val Loss: 1.3039 | Val Acc: 0.5721 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  53/56 | Train Loss: 0.7512 | Train Acc: 0.8096 | Val Loss: 1.3051 | Val Acc: 0.5676 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  54/56 | Train Loss: 0.7483 | Train Acc: 0.8213 | Val Loss: 1.2732 | Val Acc: 0.5721 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  55/56 | Train Loss: 0.7318 | Train Acc: 0.8303 | Val Loss: 1.2665 | Val Acc: 0.5676 | LR: 0.000106\n",
            "[Trial 20/20] Epoch  56/56 | Train Loss: 0.7150 | Train Acc: 0.8427 | Val Loss: 1.2855 | Val Acc: 0.5811 | LR: 0.000106\n",
            "[Trial 20/20] [Info] Loaded best model weights\n",
            "[Trial 20/20] Done | Val Loss: 1.2665 | Val Acc: 0.5676\n",
            "[I 2025-11-10 12:18:11,605] Trial 19 finished with value: 0.5675675675675675 and parameters: {'embedding_dim': 64, 'hidden_size': 128, 'num_layers': 1, 'dropout': 0.4380549877235325, 'batch_size': 24, 'learning_rate': 0.00010572608511308002, 'weight_decay': 2.0382190546405095e-06, 'label_smoothing': 0.08363081645641483, 'tune_epochs': 56}. Best is trial 16 with value: 0.6666666666666666.\n",
            "[Optuna] Completed Trial 20/20 | Value: 0.5676 | Best so far: 0.6667\n",
            "\n",
            "[Optuna] Best trial:\n",
            "  Value (Val Acc): 0.6667\n",
            "  Params:\n",
            "    embedding_dim: 64\n",
            "    hidden_size: 64\n",
            "    num_layers: 1\n",
            "    dropout: 0.4129989624132342\n",
            "    batch_size: 24\n",
            "    learning_rate: 0.0005460598289100347\n",
            "    weight_decay: 8.552011163103825e-06\n",
            "    label_smoothing: 0.09378345235787833\n",
            "    tune_epochs: 55\n",
            "[Output] Saved optuna_trials.csv\n",
            "[Output] Saved optuna_best_params.json\n",
            "\n",
            "======================================================================\n",
            "FINAL TRAINING WITH BEST HYPERPARAMETERS\n",
            "======================================================================\n",
            "[Final Config]\n",
            "  Vocab size:       5000\n",
            "  Embedding dim:    64\n",
            "  Hidden size:      64\n",
            "  Num layers:       1\n",
            "  Dropout:          0.4129989624132342\n",
            "  Output size:      5\n",
            "  Batch size:       24\n",
            "  Learning rate:    0.0005460598289100347\n",
            "  Weight decay:     8.552011163103825e-06\n",
            "  Label smoothing:  0.09378345235787833\n",
            "  Max epochs:       150\n",
            "  Sequence length:  50\n",
            "\n",
            "Model architecture:\n",
            "RNNClassifier(\n",
            "  (embedding): Embedding(5000, 64, padding_idx=0)\n",
            "  (rnn): RNN(64, 64, batch_first=True)\n",
            "  (dropout): Dropout(p=0.4129989624132342, inplace=False)\n",
            "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 328,645\n",
            "Trainable parameters: 328,645\n",
            "[Final] [Train] epochs=150, batch_size=24, lr=0.000546, patience=20\n",
            "[Final] Epoch   1/150 | Train Loss: 1.6533 | Train Acc: 0.1955 | Val Loss: 1.6230 | Val Acc: 0.2207 | LR: 0.000546\n",
            "[Final] Epoch   2/150 | Train Loss: 1.6011 | Train Acc: 0.2483 | Val Loss: 1.6143 | Val Acc: 0.2207 | LR: 0.000546\n",
            "[Final] Epoch   3/150 | Train Loss: 1.5744 | Train Acc: 0.2792 | Val Loss: 1.6062 | Val Acc: 0.2207 | LR: 0.000546\n",
            "[Final] Epoch   4/150 | Train Loss: 1.5562 | Train Acc: 0.3152 | Val Loss: 1.6035 | Val Acc: 0.2523 | LR: 0.000546\n",
            "[Final] Epoch   5/150 | Train Loss: 1.5326 | Train Acc: 0.3287 | Val Loss: 1.6048 | Val Acc: 0.2342 | LR: 0.000546\n",
            "[Final] Epoch   6/150 | Train Loss: 1.5109 | Train Acc: 0.3528 | Val Loss: 1.6040 | Val Acc: 0.2387 | LR: 0.000546\n",
            "[Final] Epoch   7/150 | Train Loss: 1.4714 | Train Acc: 0.3893 | Val Loss: 1.6091 | Val Acc: 0.2568 | LR: 0.000546\n",
            "[Final] Epoch   8/150 | Train Loss: 1.4470 | Train Acc: 0.4045 | Val Loss: 1.6123 | Val Acc: 0.2568 | LR: 0.000546\n",
            "[Final] Epoch   9/150 | Train Loss: 1.4038 | Train Acc: 0.4416 | Val Loss: 1.6278 | Val Acc: 0.2432 | LR: 0.000546\n",
            "[Final] Epoch  10/150 | Train Loss: 1.3640 | Train Acc: 0.4742 | Val Loss: 1.6269 | Val Acc: 0.2838 | LR: 0.000273 (LR reduced from 0.000546 to 0.000273)\n",
            "[Final] Epoch  11/150 | Train Loss: 1.3063 | Train Acc: 0.5287 | Val Loss: 1.6288 | Val Acc: 0.2838 | LR: 0.000273\n",
            "[Final] Epoch  12/150 | Train Loss: 1.2761 | Train Acc: 0.5292 | Val Loss: 1.6212 | Val Acc: 0.3153 | LR: 0.000273\n",
            "[Final] Epoch  13/150 | Train Loss: 1.2243 | Train Acc: 0.5753 | Val Loss: 1.6043 | Val Acc: 0.3378 | LR: 0.000273\n",
            "[Final] Epoch  14/150 | Train Loss: 1.1626 | Train Acc: 0.6163 | Val Loss: 1.5656 | Val Acc: 0.3649 | LR: 0.000273\n",
            "[Final] Epoch  15/150 | Train Loss: 1.1142 | Train Acc: 0.6438 | Val Loss: 1.5082 | Val Acc: 0.3829 | LR: 0.000273\n",
            "[Final] Epoch  16/150 | Train Loss: 1.0645 | Train Acc: 0.6742 | Val Loss: 1.5456 | Val Acc: 0.3964 | LR: 0.000273\n",
            "[Final] Epoch  17/150 | Train Loss: 1.0267 | Train Acc: 0.6972 | Val Loss: 1.5017 | Val Acc: 0.4144 | LR: 0.000273\n",
            "[Final] Epoch  18/150 | Train Loss: 0.9876 | Train Acc: 0.7152 | Val Loss: 1.4955 | Val Acc: 0.4189 | LR: 0.000273\n",
            "[Final] Epoch  19/150 | Train Loss: 0.9409 | Train Acc: 0.7393 | Val Loss: 1.4670 | Val Acc: 0.4279 | LR: 0.000273\n",
            "[Final] Epoch  20/150 | Train Loss: 0.9084 | Train Acc: 0.7551 | Val Loss: 1.5128 | Val Acc: 0.4144 | LR: 0.000273\n",
            "[Final] Epoch  21/150 | Train Loss: 0.8798 | Train Acc: 0.7742 | Val Loss: 1.4634 | Val Acc: 0.4369 | LR: 0.000273\n",
            "[Final] Epoch  22/150 | Train Loss: 0.8458 | Train Acc: 0.7871 | Val Loss: 1.4694 | Val Acc: 0.4459 | LR: 0.000273\n",
            "[Final] Epoch  23/150 | Train Loss: 0.8210 | Train Acc: 0.8090 | Val Loss: 1.4870 | Val Acc: 0.4414 | LR: 0.000273\n",
            "[Final] Epoch  24/150 | Train Loss: 0.7856 | Train Acc: 0.8343 | Val Loss: 1.4839 | Val Acc: 0.4685 | LR: 0.000273\n",
            "[Final] Epoch  25/150 | Train Loss: 0.7637 | Train Acc: 0.8371 | Val Loss: 1.4768 | Val Acc: 0.4820 | LR: 0.000273\n",
            "[Final] Epoch  26/150 | Train Loss: 0.7411 | Train Acc: 0.8511 | Val Loss: 1.4665 | Val Acc: 0.4685 | LR: 0.000273\n",
            "[Final] Epoch  27/150 | Train Loss: 0.7175 | Train Acc: 0.8573 | Val Loss: 1.4939 | Val Acc: 0.5180 | LR: 0.000137 (LR reduced from 0.000273 to 0.000137)\n",
            "[Final] Epoch  28/150 | Train Loss: 0.6953 | Train Acc: 0.8860 | Val Loss: 1.4491 | Val Acc: 0.5270 | LR: 0.000137\n",
            "[Final] Epoch  29/150 | Train Loss: 0.6909 | Train Acc: 0.8865 | Val Loss: 1.4776 | Val Acc: 0.5135 | LR: 0.000137\n",
            "[Final] Epoch  30/150 | Train Loss: 0.6749 | Train Acc: 0.8949 | Val Loss: 1.4614 | Val Acc: 0.5000 | LR: 0.000137\n",
            "[Final] Epoch  31/150 | Train Loss: 0.6633 | Train Acc: 0.9011 | Val Loss: 1.4656 | Val Acc: 0.5405 | LR: 0.000137\n",
            "[Final] Epoch  32/150 | Train Loss: 0.6614 | Train Acc: 0.8910 | Val Loss: 1.4597 | Val Acc: 0.5135 | LR: 0.000137\n",
            "[Final] Epoch  33/150 | Train Loss: 0.6520 | Train Acc: 0.8938 | Val Loss: 1.4677 | Val Acc: 0.5090 | LR: 0.000137\n",
            "[Final] Epoch  34/150 | Train Loss: 0.6502 | Train Acc: 0.8983 | Val Loss: 1.4631 | Val Acc: 0.5315 | LR: 0.000068 (LR reduced from 0.000137 to 0.000068)\n",
            "[Final] Epoch  35/150 | Train Loss: 0.6361 | Train Acc: 0.9101 | Val Loss: 1.4609 | Val Acc: 0.5090 | LR: 0.000068\n",
            "[Final] Epoch  36/150 | Train Loss: 0.6355 | Train Acc: 0.9140 | Val Loss: 1.4625 | Val Acc: 0.5360 | LR: 0.000068\n",
            "[Final] Epoch  37/150 | Train Loss: 0.6268 | Train Acc: 0.9112 | Val Loss: 1.4555 | Val Acc: 0.5270 | LR: 0.000068\n",
            "[Final] Epoch  38/150 | Train Loss: 0.6328 | Train Acc: 0.9152 | Val Loss: 1.4679 | Val Acc: 0.5315 | LR: 0.000068\n",
            "[Final] Epoch  39/150 | Train Loss: 0.6213 | Train Acc: 0.9174 | Val Loss: 1.4826 | Val Acc: 0.5225 | LR: 0.000068\n",
            "[Final] Epoch  40/150 | Train Loss: 0.6182 | Train Acc: 0.9197 | Val Loss: 1.4794 | Val Acc: 0.5225 | LR: 0.000034 (LR reduced from 0.000068 to 0.000034)\n",
            "[Final] Epoch  41/150 | Train Loss: 0.6146 | Train Acc: 0.9197 | Val Loss: 1.4697 | Val Acc: 0.5225 | LR: 0.000034\n",
            "[Final] Epoch  42/150 | Train Loss: 0.6124 | Train Acc: 0.9242 | Val Loss: 1.4700 | Val Acc: 0.5270 | LR: 0.000034\n",
            "[Final] Epoch  43/150 | Train Loss: 0.6152 | Train Acc: 0.9281 | Val Loss: 1.4779 | Val Acc: 0.5225 | LR: 0.000034\n",
            "[Final] Epoch  44/150 | Train Loss: 0.6000 | Train Acc: 0.9264 | Val Loss: 1.4767 | Val Acc: 0.5315 | LR: 0.000034\n",
            "[Final] Epoch  45/150 | Train Loss: 0.6029 | Train Acc: 0.9320 | Val Loss: 1.4784 | Val Acc: 0.5360 | LR: 0.000034\n",
            "[Final] Epoch  46/150 | Train Loss: 0.6078 | Train Acc: 0.9208 | Val Loss: 1.4719 | Val Acc: 0.5315 | LR: 0.000017 (LR reduced from 0.000034 to 0.000017)\n",
            "[Final] Epoch  47/150 | Train Loss: 0.5957 | Train Acc: 0.9315 | Val Loss: 1.4781 | Val Acc: 0.5315 | LR: 0.000017\n",
            "[Final] Epoch  48/150 | Train Loss: 0.6099 | Train Acc: 0.9208 | Val Loss: 1.4813 | Val Acc: 0.5225 | LR: 0.000017\n",
            "[Final] [EarlyStopping] triggered at epoch 48\n",
            "[Final] [Info] Loaded best model weights\n",
            "\n",
            "======================================================================\n",
            "MODEL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Final Accuracy Scores:\n",
            "  Training Accuracy:   0.9090 (90.90%)\n",
            "  Validation Accuracy: 0.5270 (52.70%)\n",
            "  Test Accuracy:       0.5426 (54.26%)\n",
            "\n",
            "======================================================================\n",
            "DETAILED CLASSIFICATION REPORT (Test Set)\n",
            "======================================================================\n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Politics       0.50      0.43      0.46        42\n",
            "        Sport       0.68      0.63      0.65        51\n",
            "   Technology       0.46      0.57      0.51        40\n",
            "Entertainment       0.68      0.54      0.60        39\n",
            "     Business       0.46      0.53      0.49        51\n",
            "\n",
            "     accuracy                           0.54       223\n",
            "    macro avg       0.56      0.54      0.54       223\n",
            " weighted avg       0.56      0.54      0.55       223\n",
            "\n",
            "\n",
            "======================================================================\n",
            "PLOTTING & SAVING FIGURES\n",
            "======================================================================\n",
            "[Plot] Training/Validation curves + Confusion Matrix -> rnn_pytorch_training_results.png\n",
            "[Plot] Per-class accuracy -> rnn_pytorch_per_class_accuracy.png\n",
            "[Output] Saved: rnn_pytorch_training_results.png, rnn_pytorch_per_class_accuracy.png\n",
            "\n",
            "======================================================================\n",
            "SAMPLE PREDICTIONS\n",
            "======================================================================\n",
            "Text preview: <UNK> anger at <UNK> stars sale <UNK> director of rugby <UNK> <UNK> has <UNK> <UNK> <UNK> <UNK> sale host bath...\n",
            "True Label: Sport\n",
            "Predicted:  Sport\n",
            "Confidence: 83.45%\n",
            "Result: Correct\n",
            "----------------------------------------------------------------------\n",
            "Text preview: <UNK> <UNK> deal for <UNK> a <UNK> aid made from a southern african <UNK> is set to be developed by...\n",
            "True Label: Business\n",
            "Predicted:  Business\n",
            "Confidence: 76.57%\n",
            "Result: Correct\n",
            "----------------------------------------------------------------------\n",
            "Text preview: broadband <UNK> ahead in the us more and more americans are joining the <UNK> fast <UNK> according to official figures....\n",
            "True Label: Technology\n",
            "Predicted:  Politics\n",
            "Confidence: 43.42%\n",
            "Result: Incorrect\n",
            "----------------------------------------------------------------------\n",
            "Text preview: ireland <UNK> england ireland <UNK> england to their third straight six nations defeat with a <UNK> victory at lansdowne <UNK>...\n",
            "True Label: Sport\n",
            "Predicted:  Sport\n",
            "Confidence: 51.59%\n",
            "Result: Correct\n",
            "----------------------------------------------------------------------\n",
            "Text preview: little britain two top comic list little britain stars matt <UNK> and david <UNK> have been named the most powerful...\n",
            "True Label: Entertainment\n",
            "Predicted:  Entertainment\n",
            "Confidence: 81.59%\n",
            "Result: Correct\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Saving model and metadata ...\n",
            "Saved model as: rnn_pytorch_model.pt\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "Final Test Accuracy: 54.26%\n",
            "Device used: cuda\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "PyTorch RNN Text Classification + Optuna (20 trials) — Verbose, compatible with older torch\n",
        "Dataset: df_file.csv with columns ['Text', 'Label'] and 5 classes\n",
        "\n",
        "Pipeline:\n",
        "- Preprocess -> vocab (5000) -> indexify (seq_len=50)\n",
        "- PyTorch RNN with Embedding + nn.RNN\n",
        "- Optuna tunes: embedding_dim, hidden_size, num_layers, dropout, batch_size,\n",
        "                learning_rate, weight_decay, label_smoothing, tune_epochs\n",
        "- Retrain final model with best hyperparameters (max 150 epochs)\n",
        "- Saves:\n",
        "    * optuna_trials.csv\n",
        "    * optuna_best_params.json\n",
        "    * rnn_pytorch_training_results.png\n",
        "    * rnn_pytorch_per_class_accuracy.png\n",
        "    * rnn_pytorch_model.pt\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------\n",
        "# Standard library imports\n",
        "# -----------------------------\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # silence non-critical warnings for cleaner logs\n",
        "\n",
        "# -----------------------------\n",
        "# Third-party imports\n",
        "# -----------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib; matplotlib.use(\"Agg\")  # use non-interactive backend for saving figures\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# -----------------------------\n",
        "# Optuna import / auto-install\n",
        "# -----------------------------\n",
        "try:\n",
        "    import optuna  # hyperparameter optimization framework\n",
        "except ImportError:\n",
        "    print(\"[Setup] Optuna not found. Installing optuna ...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"optuna\"])\n",
        "    import optuna  # import again after installation\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.INFO)  # reduce Optuna log noise\n",
        "\n",
        "# -----------------------------\n",
        "# Config / reproducibility\n",
        "# -----------------------------\n",
        "GLOBAL_SEED = 42  # global random seed\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "torch.manual_seed(GLOBAL_SEED)\n",
        "\n",
        "N_TRIALS = 20           # number of Optuna trials\n",
        "SEQ_LEN = 50            # fixed sequence length (tokens per sample)\n",
        "VOCAB_SIZE = 5000       # vocabulary cap (top-k most frequent tokens + PAD/UNK)\n",
        "FINAL_MAX_EPOCHS = 150  # upper bound for final training epochs\n",
        "\n",
        "PRINT_LINE = \"=\" * 70  # pretty print separator\n",
        "def hr(msg: str):\n",
        "    \"\"\"Print a big header line for readability.\"\"\"\n",
        "    print(\"\\n\" + PRINT_LINE)\n",
        "    print(msg)\n",
        "    print(PRINT_LINE)\n",
        "\n",
        "# -----------------------------\n",
        "# Device selection (no emojis)\n",
        "# -----------------------------\n",
        "def get_device():\n",
        "    \"\"\"Select best available device: MPS (Apple GPU) > CUDA > CPU.\"\"\"\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        print(\"Using Mac GPU (MPS)\")\n",
        "        return torch.device(\"mps\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        return torch.device(\"cuda\")\n",
        "    print(\"Using CPU\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()  # resolve device once and reuse\n",
        "print(f\"Device: {device}\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load data\n",
        "# -----------------------------\n",
        "hr(\"LOADING DATASET\")\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# df = pd.read_csv(list(uploaded.keys())[0])\n",
        "\n",
        "df = pd.read_csv('df_file.csv')  # expects columns: ['Text', 'Label']\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"Number of classes: {df['Label'].nunique()}\")\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(df['Label'].value_counts().sort_index())\n",
        "\n",
        "# fixed label to class-name mapping\n",
        "class_names = {0: 'Politics', 1: 'Sport', 2: 'Technology', 3: 'Entertainment', 4: 'Business'}\n",
        "print(\"\\nClass mapping:\")\n",
        "for label, name in class_names.items():\n",
        "    print(f\"  {label}: {name} ({len(df[df['Label'] == label])} samples)\")\n",
        "print(PRINT_LINE)\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocess\n",
        "# -----------------------------\n",
        "hr(\"PREPROCESSING\")\n",
        "print(\"[Step] Lowercasing and dropping empty rows ...\")\n",
        "df['Text'] = df['Text'].astype(str).str.lower()     # normalize to lowercase strings\n",
        "df = df[df['Text'].str.len() > 0].reset_index(drop=True)  # drop empties and reindex\n",
        "print(f\"[Done] Dataset shape after preprocessing: {df.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Vocabulary\n",
        "# -----------------------------\n",
        "hr(\"VOCABULARY\")\n",
        "print(\"[Step] Counting token frequencies ...\")\n",
        "all_words = []\n",
        "for text in df['Text'].values:\n",
        "    all_words.extend(text.split())  # whitespace tokenization\n",
        "\n",
        "word_counts = Counter(all_words)  # frequency of each token\n",
        "print(f\"[Info] Total unique tokens: {len(word_counts)}\")\n",
        "\n",
        "print(f\"[Step] Building vocab size={VOCAB_SIZE} with <PAD>=0, <UNK>=1 ...\")\n",
        "vocab = {'<PAD>': 0, '<UNK>': 1}  # reserve indices for padding and unknowns\n",
        "for w, _ in word_counts.most_common(VOCAB_SIZE - 2):\n",
        "    vocab[w] = len(vocab)  # assign next available index\n",
        "coverage = (len(vocab) / max(1, len(word_counts))) * 100  # proportion of unique tokens covered\n",
        "print(f\"[Done] Vocab size: {len(vocab)} | Coverage: {coverage:.2f}%\")\n",
        "\n",
        "inverse_vocab = {idx: word for word, idx in vocab.items()}  # for human-readable previews\n",
        "\n",
        "# -----------------------------\n",
        "# Sequences\n",
        "# -----------------------------\n",
        "hr(\"SEQUENCE ENCODING\")\n",
        "print(f\"[Info] Sequence length: {SEQ_LEN}\")\n",
        "print(\"[Step] Converting texts to index sequences ...\")\n",
        "\n",
        "X_sequences = []\n",
        "for text in df['Text'].values:\n",
        "    words = text.split()[:SEQ_LEN]  # truncate to max length\n",
        "    seq = [vocab.get(w, vocab['<UNK>']) for w in words]  # map to ids w/ UNK fallback\n",
        "    if len(seq) < SEQ_LEN:\n",
        "        seq += [vocab['<PAD>']] * (SEQ_LEN - len(seq))  # right-pad with PAD tokens\n",
        "    X_sequences.append(seq)\n",
        "\n",
        "X_sequences = np.array(X_sequences, dtype=np.int64)  # shape: (N, T)\n",
        "y_labels = df['Label'].astype(int).values            # shape: (N,)\n",
        "\n",
        "print(f\"[Done] X_sequences shape: {X_sequences.shape}\")\n",
        "print(f\"[Done] y_labels shape:  {y_labels.shape}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Split\n",
        "# -----------------------------\n",
        "hr(\"DATA SPLIT\")\n",
        "# 80/10/10 stratified split: first hold out 20%, then split evenly into val/test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X_sequences, y_labels, test_size=0.2, random_state=GLOBAL_SEED, stratify=y_labels\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=GLOBAL_SEED, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"[Info] Training set:   {X_train.shape[0]} samples ({X_train.shape[0]/len(X_sequences)*100:.1f}%)\")\n",
        "print(f\"[Info] Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X_sequences)*100:.1f}%)\")\n",
        "print(f\"[Info] Test set:       {X_test.shape[0]} samples ({X_test.shape[0]/len(X_sequences)*100:.1f}%)\")\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset class\n",
        "# -----------------------------\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"Thin torch Dataset wrapper around (sequences, labels).\"\"\"\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = torch.LongTensor(sequences)  # (N, T)\n",
        "        self.labels = torch.LongTensor(labels)        # (N,)\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "# Instantiate datasets for each split\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "val_dataset   = TextDataset(X_val,   y_val)\n",
        "test_dataset  = TextDataset(X_test,  y_test)\n",
        "\n",
        "# -----------------------------\n",
        "# Model\n",
        "# -----------------------------\n",
        "class RNNClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Embedding -> RNN -> Dropout -> Linear\n",
        "    A simple classifier using the last hidden state of an nn.RNN.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size,\n",
        "                 num_layers=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # PAD ignored in gradients\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,                     # input/output as (B, T, *)\n",
        "            dropout=dropout if num_layers > 1 else 0.0,  # internal dropout only if L>1\n",
        "            nonlinearity='tanh'\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)        # regularization on last hidden state\n",
        "        self.fc = nn.Linear(hidden_size, output_size)  # maps to class logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)           # (B, T, E)\n",
        "        out, h_n = self.rnn(embedded)          # out: (B, T, H); h_n: (L, B, H)\n",
        "        last_hidden = h_n[-1]                  # take the top-layer final state: (B, H)\n",
        "        dropped = self.dropout(last_hidden)    # apply dropout before classifier\n",
        "        logits = self.fc(dropped)              # (B, C)\n",
        "        return logits\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval helpers\n",
        "# -----------------------------\n",
        "def current_lr(optimizer):\n",
        "    \"\"\"Read current learning rate from optimizer.\"\"\"\n",
        "    return float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"One training epoch over a dataloader; returns (loss, accuracy).\"\"\"\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for sequences, labels in dataloader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(sequences)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # stabilize training\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * sequences.size(0)\n",
        "        _, pred = torch.max(logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (pred == labels).sum().item()\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    \"\"\"Evaluation loop without gradient tracking; returns (loss, accuracy).\"\"\"\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in dataloader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            logits = model(sequences)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item() * sequences.size(0)\n",
        "            _, pred = torch.max(logits, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum().item()\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "def make_criterion(label_smoothing_value: float):\n",
        "    \"\"\"Create CrossEntropyLoss; falls back if older torch lacks label_smoothing.\"\"\"\n",
        "    try:\n",
        "        return nn.CrossEntropyLoss(label_smoothing=float(label_smoothing_value))\n",
        "    except TypeError:\n",
        "        return nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                scheduler, num_epochs, device, patience=10, log_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Train with early stopping on validation loss and ReduceLROnPlateau scheduler.\n",
        "    Returns history dict with curves.\n",
        "    \"\"\"\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_state = None\n",
        "\n",
        "    print(f\"{log_prefix}[Train] epochs={num_epochs}, batch_size={train_loader.batch_size}, \"\n",
        "          f\"lr={current_lr(optimizer):.6f}, patience={patience}\")\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Scheduler step with explicit LR-change log (no 'verbose' arg)\n",
        "        lr_before = current_lr(optimizer)\n",
        "        scheduler.step(val_loss)\n",
        "        lr_after = current_lr(optimizer)\n",
        "        lr_note = \"\"\n",
        "        if lr_after < lr_before:\n",
        "            lr_note = f\" (LR reduced from {lr_before:.6f} to {lr_after:.6f})\"\n",
        "\n",
        "        history['train_loss'].append(tr_loss)\n",
        "        history['train_acc'].append(tr_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"{log_prefix}Epoch {epoch:3d}/{num_epochs} | \"\n",
        "              f\"Train Loss: {tr_loss:.4f} | Train Acc: {tr_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n",
        "              f\"LR: {current_lr(optimizer):.6f}{lr_note}\")\n",
        "\n",
        "        # Track best weights by validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"{log_prefix}[EarlyStopping] triggered at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)  # restore best weights\n",
        "        print(f\"{log_prefix}[Info] Loaded best model weights\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# -----------------------------\n",
        "# Optuna Objective\n",
        "# -----------------------------\n",
        "def objective(trial: optuna.trial.Trial) -> float:\n",
        "    \"\"\"Define search space, train/validate, and return validation accuracy.\"\"\"\n",
        "    # Hyperparameter search space\n",
        "    embedding_dim = trial.suggest_categorical(\"embedding_dim\", [32, 64, 128])\n",
        "    hidden_size   = trial.suggest_categorical(\"hidden_size\",   [32, 64, 96, 128, 160])\n",
        "    num_layers    = trial.suggest_categorical(\"num_layers\",    [1, 2, 3])\n",
        "    dropout       = trial.suggest_float(\"dropout\", 0.1, 0.6)\n",
        "    batch_size    = trial.suggest_categorical(\"batch_size\",    [16, 24, 32, 48, 64])\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 5e-3, log=True)\n",
        "    weight_decay  = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "    label_smooth  = trial.suggest_float(\"label_smoothing\", 0.0, 0.2)\n",
        "    tune_epochs   = trial.suggest_int(\"tune_epochs\", 25, 60)\n",
        "    patience      = 10  # early-stopping patience during tuning\n",
        "\n",
        "    tnum = trial.number + 1\n",
        "    header = f\"[Trial {tnum:02d}/{N_TRIALS}] \"\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"{header}START\")\n",
        "    print(f\"{header}Params: embedding_dim={embedding_dim}, hidden_size={hidden_size}, \"\n",
        "          f\"num_layers={num_layers}, dropout={dropout:.2f}, batch_size={batch_size}, \"\n",
        "          f\"lr={learning_rate:.6f}, weight_decay={weight_decay:.1e}, \"\n",
        "          f\"label_smoothing={label_smooth:.2f}, tune_epochs={tune_epochs}\")\n",
        "\n",
        "    # DataLoaders for this trial\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model instantiation per trial\n",
        "    model = RNNClassifier(\n",
        "        vocab_size=len(vocab),\n",
        "        embedding_dim=embedding_dim,\n",
        "        hidden_size=hidden_size,\n",
        "        output_size=len(class_names),\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout\n",
        "    ).to(device)\n",
        "\n",
        "    # Loss/optimizer/scheduler\n",
        "    criterion = make_criterion(label_smooth)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    # Important: no 'verbose' argument for compatibility\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    # Train (verbose per-epoch with prefix)\n",
        "    _ = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "        num_epochs=int(tune_epochs), device=device, patience=patience, log_prefix=header\n",
        "    )\n",
        "\n",
        "    # Validation accuracy (objective to maximize)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "    print(f\"{header}Done | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    trial.set_user_attr(\"val_acc\", float(val_acc))  # store for analysis\n",
        "    return float(val_acc)\n",
        "\n",
        "def trial_callback(total_trials: int):\n",
        "    \"\"\"Progress callback to print per-trial summary.\"\"\"\n",
        "    def _cb(study: optuna.study.Study, trial: optuna.trial.FrozenTrial):\n",
        "        best = study.best_value if study.best_trial is not None else None\n",
        "        print(f\"[Optuna] Completed Trial {trial.number + 1}/{total_trials} | \"\n",
        "              f\"Value: {trial.value:.4f} | Best so far: {best:.4f}\")\n",
        "    return _cb\n",
        "\n",
        "# -----------------------------\n",
        "# Run Optuna\n",
        "# -----------------------------\n",
        "hr(\"OPTUNA STUDY (20 TRIALS)\")\n",
        "sampler = optuna.samplers.TPESampler(seed=GLOBAL_SEED)  # TPE sampler with fixed seed\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler)  # maximize val accuracy\n",
        "study.optimize(objective, n_trials=N_TRIALS, callbacks=[trial_callback(N_TRIALS)], show_progress_bar=True)\n",
        "\n",
        "print(\"\\n[Optuna] Best trial:\")\n",
        "best_trial = study.best_trial\n",
        "print(f\"  Value (Val Acc): {best_trial.value:.4f}\")\n",
        "print(\"  Params:\")\n",
        "for k, v in best_trial.params.items():\n",
        "    print(f\"    {k}: {v}\")\n",
        "\n",
        "# Save trials dataframe\n",
        "try:\n",
        "    df_trials = study.trials_dataframe()\n",
        "    df_trials.to_csv(\"optuna_trials.csv\", index=False)\n",
        "    print(\"[Output] Saved optuna_trials.csv\")\n",
        "except Exception as e:\n",
        "    print(f\"[Warn] Could not save trials dataframe: {e}\")\n",
        "\n",
        "# Save best params for reproducibility\n",
        "with open(\"optuna_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(best_trial.params, f, indent=2)\n",
        "print(\"[Output] Saved optuna_best_params.json\")\n",
        "\n",
        "# -----------------------------\n",
        "# Final training with best params\n",
        "# -----------------------------\n",
        "hr(\"FINAL TRAINING WITH BEST HYPERPARAMETERS\")\n",
        "\n",
        "bp = best_trial.params  # shorthand\n",
        "embedding_dim = int(bp[\"embedding_dim\"])\n",
        "hidden_size   = int(bp[\"hidden_size\"])\n",
        "num_layers    = int(bp[\"num_layers\"])\n",
        "dropout       = float(bp[\"dropout\"])\n",
        "batch_size    = int(bp[\"batch_size\"])\n",
        "learning_rate = float(bp[\"learning_rate\"])\n",
        "weight_decay  = float(bp[\"weight_decay\"])\n",
        "label_smooth  = float(bp[\"label_smoothing\"])\n",
        "num_epochs    = FINAL_MAX_EPOCHS\n",
        "patience      = 20  # longer patience for final fit\n",
        "\n",
        "print(\"[Final Config]\")\n",
        "print(f\"  Vocab size:       {len(vocab)}\")\n",
        "print(f\"  Embedding dim:    {embedding_dim}\")\n",
        "print(f\"  Hidden size:      {hidden_size}\")\n",
        "print(f\"  Num layers:       {num_layers}\")\n",
        "print(f\"  Dropout:          {dropout}\")\n",
        "print(f\"  Output size:      {len(class_names)}\")\n",
        "print(f\"  Batch size:       {batch_size}\")\n",
        "print(f\"  Learning rate:    {learning_rate}\")\n",
        "print(f\"  Weight decay:     {weight_decay}\")\n",
        "print(f\"  Label smoothing:  {label_smooth}\")\n",
        "print(f\"  Max epochs:       {num_epochs}\")\n",
        "print(f\"  Sequence length:  {SEQ_LEN}\")\n",
        "\n",
        "# Build final DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Final model with best hyperparameters\n",
        "model = RNNClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    output_size=len(class_names),\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "print(\"\\nModel architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "criterion = make_criterion(label_smooth)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "# Important: no 'verbose' argument for compatibility\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "# Train and collect history for plotting\n",
        "history = train_model(\n",
        "    model, train_loader, val_loader, criterion, optimizer,\n",
        "    scheduler, num_epochs, device, patience=patience, log_prefix=\"[Final] \"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation\n",
        "# -----------------------------\n",
        "hr(\"MODEL EVALUATION\")\n",
        "train_loss, train_acc = evaluate(model, train_loader, criterion, device)\n",
        "val_loss, val_acc     = evaluate(model, val_loader,   criterion, device)\n",
        "test_loss, test_acc   = evaluate(model, test_loader,  criterion, device)\n",
        "\n",
        "print(\"\\nFinal Accuracy Scores:\")\n",
        "print(f\"  Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "print(f\"  Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "print(f\"  Test Accuracy:       {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "# Collect test predictions (for report/plots)\n",
        "model.eval()\n",
        "all_predictions, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in test_loader:\n",
        "        sequences = sequences.to(device)\n",
        "        logits = model(sequences)\n",
        "        _, pred = torch.max(logits, 1)\n",
        "        all_predictions.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "y_test_pred = np.array(all_predictions)\n",
        "y_test_true = np.array(all_labels)\n",
        "\n",
        "print(\"\\n\" + PRINT_LINE)\n",
        "print(\"DETAILED CLASSIFICATION REPORT (Test Set)\")\n",
        "print(PRINT_LINE + \"\\n\")\n",
        "target_names = [class_names[i] for i in range(len(class_names))]\n",
        "print(classification_report(y_test_true, y_test_pred, target_names=target_names))\n",
        "\n",
        "# -----------------------------\n",
        "# Visualizations\n",
        "# -----------------------------\n",
        "hr(\"PLOTTING & SAVING FIGURES\")\n",
        "print(\"[Plot] Training/Validation curves + Confusion Matrix -> rnn_pytorch_training_results.png\")\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "# Loss curves\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
        "plt.plot(history['val_loss'],   label='Validation Loss', linewidth=2)\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Training and Validation Loss')\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history['train_acc'], label='Training Accuracy', linewidth=2)\n",
        "plt.plot(history['val_acc'],   label='Validation Accuracy', linewidth=2)\n",
        "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Training and Validation Accuracy')\n",
        "plt.legend(); plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion matrix on test set\n",
        "plt.subplot(1, 3, 3)\n",
        "cm = confusion_matrix(y_test_true, y_test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=target_names, yticklabels=target_names)\n",
        "plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix (Test Set)')\n",
        "plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.savefig('rnn_pytorch_training_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"[Plot] Per-class accuracy -> rnn_pytorch_per_class_accuracy.png\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "per_class_acc = []\n",
        "for i in range(len(class_names)):\n",
        "    mask = (y_test_true == i)\n",
        "    acc = np.mean(y_test_pred[mask] == y_test_true[mask]) if np.sum(mask) > 0 else 0.0\n",
        "    per_class_acc.append(acc)\n",
        "bars = plt.bar(range(len(class_names)), per_class_acc)\n",
        "plt.xlabel('Class'); plt.ylabel('Accuracy'); plt.title('Per-Class Accuracy on Test Set')\n",
        "plt.xticks(range(len(class_names)), target_names, rotation=45, ha='right')\n",
        "plt.ylim([0, 1.1]); plt.grid(True, alpha=0.3, axis='y')\n",
        "for bar, acc in zip(bars, per_class_acc):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
        "             f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('rnn_pytorch_per_class_accuracy.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"[Output] Saved: rnn_pytorch_training_results.png, rnn_pytorch_per_class_accuracy.png\")\n",
        "\n",
        "# -----------------------------\n",
        "# Sample predictions\n",
        "# -----------------------------\n",
        "hr(\"SAMPLE PREDICTIONS\")\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "k = min(5, len(X_test))  # show up to 5 samples\n",
        "sample_indices = np.random.choice(len(X_test), size=k, replace=False)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for idx in sample_indices:\n",
        "        seq_tensor = torch.LongTensor(X_test[idx]).unsqueeze(0).to(device)  # (1, T)\n",
        "        logits = model(seq_tensor)\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "        pred_label = int(np.argmax(probs))\n",
        "        true_label = int(y_test[idx])\n",
        "\n",
        "        # reconstruct a short text preview (ignore PAD tokens)\n",
        "        tokens = [inverse_vocab.get(int(tok), \"<UNK>\") for tok in X_test[idx] if int(tok) != vocab['<PAD>']]\n",
        "        text_preview = \" \".join(tokens[:20]) if tokens else \"N/A\"\n",
        "\n",
        "        print(f\"Text preview: {text_preview}...\")\n",
        "        print(f\"True Label: {class_names[true_label]}\")\n",
        "        print(f\"Predicted:  {class_names[pred_label]}\")\n",
        "        print(f\"Confidence: {probs[pred_label]*100:.2f}%\")\n",
        "        print(\"Result: \" + (\"Correct\" if true_label == pred_label else \"Incorrect\"))\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "# -----------------------------\n",
        "# Save model\n",
        "# -----------------------------\n",
        "print(\"\\nSaving model and metadata ...\")\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),  # learned weights\n",
        "    'vocab': vocab,                          # token->id mapping\n",
        "    'class_names': class_names,              # id->class mapping\n",
        "    'hyperparameters': {                     # training configuration\n",
        "        'embedding_dim': embedding_dim,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'dropout': dropout,\n",
        "        'batch_size': batch_size,\n",
        "        'learning_rate': learning_rate,\n",
        "        'weight_decay': weight_decay,\n",
        "        'label_smoothing': label_smooth,\n",
        "        'seq_len': SEQ_LEN,\n",
        "        'vocab_size': len(vocab)\n",
        "    }\n",
        "}, 'rnn_pytorch_model.pt')\n",
        "print(\"Saved model as: rnn_pytorch_model.pt\")\n",
        "\n",
        "print(\"\\n\" + PRINT_LINE)\n",
        "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(PRINT_LINE)\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"Device used: {device}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2148560e571740eaafdd2300b9a69271": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_334ffff882564e87bd7f5a24e3985682",
            "placeholder": "​",
            "style": "IPY_MODEL_9cc0f24fc2f745ecadc1245bc10a0ee9",
            "value": " 20/20 [02:33&lt;00:00, 10.66s/it]"
          }
        },
        "334ffff882564e87bd7f5a24e3985682": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "367eb461c3074766982658d4effd638a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3681808c5fa4699a65a4e06d58df9e0",
            "placeholder": "​",
            "style": "IPY_MODEL_c4288fc6c8894db6a571137d5b79f6cc",
            "value": "Best trial: 16. Best value: 0.666667: 100%"
          }
        },
        "3ba50848270d4d9e86582fc3d7806b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57d6fd6c2b5d44feac56be0f9f158ead": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5adcdc9b64774579bac4f08d5e4b4eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57d6fd6c2b5d44feac56be0f9f158ead",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ba50848270d4d9e86582fc3d7806b4b",
            "value": 20
          }
        },
        "9cc0f24fc2f745ecadc1245bc10a0ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3681808c5fa4699a65a4e06d58df9e0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4288fc6c8894db6a571137d5b79f6cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f49dcd2ffb8442a08e4d99ce66566dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_367eb461c3074766982658d4effd638a",
              "IPY_MODEL_5adcdc9b64774579bac4f08d5e4b4eb3",
              "IPY_MODEL_2148560e571740eaafdd2300b9a69271"
            ],
            "layout": "IPY_MODEL_ff733b753e7c44ff8bea380d1c23198e"
          }
        },
        "ff733b753e7c44ff8bea380d1c23198e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
